"""
## Setup

"""
import os

os.environ["KERAS_BACKEND"] = "tensorflow"

import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras import regularizers
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_probability as tfp
import pandas as pd




# Specify the example you are dealing with
esempio = 'L_FRAME_DT'
# Training and Testing data
ID              = '7_1'
save_ID         = 'Classifier_total_7_1/'
path            = "../" + esempio + '/Dati/'
path_data_train   = path + 'istantrain_' + ID
path_data_test   = path + 'istantest_' + ID
# Saving model
path_save       = "results/"
# Prediction model
restore_ID= 'Classifier_total_7_1/'
path_restore = "results/"

# Which dof monitored
which_channels = [1,2,3,4,5,6,7,8]

n_channels = 8
seq_len = 200
# Specify if you are using accelerations =1 or displacements =0
accelerations = 0

# Questa funzione calcola il valore quadratico medio (RMS) di un vettore di segnale, utile per misurare l'energia del segnale e per 
# aggiungere rumore durante la corruzione dei segnali.
def RMS(vect):
    return np.sqrt(np.mean(np.square(vect)))


# ----------------------------------------------------------------------------

"""
## Importazione dati
"""

# Read data and create dataset with only desired classes
def read_data(path_data):    
    label_path_class = path_data + '/Damage_class.csv'                                
    labels_class     = np.genfromtxt(label_path_class)
    labels_class     = labels_class.astype('int')
    N_ist = len(labels_class)  

    if accelerations == 1:
        path_rec  = path_data + '/U2_concat_'
    elif accelerations == 0:
        path_rec  = path_data + '/U_concat_'

    if path_data == path_data_train:
        signals_means = np.zeros((n_channels))
        signals_stds = np.zeros((n_channels))
    else:
        signals_means = np.load(path_restore+'/Corrupted_signals_means.npy')
        signals_stds = np.load(path_restore+'/Corrupted_signals_stds.npy')
    # Create the dataset output structure
    X       = np.zeros((N_ist, seq_len, n_channels))   #(10000,200,8)
    # X_noise = np.zeros((N_ist, seq_len, n_channels))
    for i1 in range(n_channels):
        path_data_var  = path_rec + str(which_channels[i1]) + '.csv'
        X_singledof = pd.read_csv(path_data_var, header=None).to_numpy()[:,0]
        for i2 in range(N_ist):
            # RESHAPE THE SIGNALS
            X[i2,:,i1]= X_singledof[1 + i2*(1+seq_len) : (i2+1)*(1+seq_len)]
            # # CORRUPT THE SIGNALS
            # rms_signal = RMS(X[i2,:,i1])
            # dev_std    = rms_signal / np.sqrt(addedd_SNR)
            # sample = st.norm(0, dev_std)
            # X_noise[i2,:,i1] = X[i2,:,i1] + sample.rvs(size=seq_len)
        if path_data == path_data_train:
            # COMPUTE STATISTICS FOR EACH CHANNEL
            # signals_means[i1] = np.mean(np.reshape(X_noise[:,:,i1], (N_ist*seq_len)))
            # signals_stds[i1] = np.std(np.reshape(X_noise[:,:,i1], (N_ist*seq_len)))
            signals_means[i1] = np.mean(np.reshape(X[:,:,i1], (N_ist*seq_len)))
            signals_stds[i1] = np.std(np.reshape(X[:,:,i1], (N_ist*seq_len)))

        # NORMALIZE THE SIGNALS    
        # X_noise[:,:,i1] = (X_noise[:,:,i1] - signals_means[i1])/signals_stds[i1]
        X[:,:,i1] = (X[:,:,i1] - signals_means[i1])/signals_stds[i1]
   
    # if path_data == path_data_train:    
    #     np.save(path_save+'Corrupted_signals_means', signals_means)
    #     np.save(path_save+'Corrupted_signals_stds', signals_stds)
       
    # return X_noise, labels_class, N_ist
    return X, labels_class, N_ist


"""
## Affine coupling layer
"""

# Creating a custom layer with keras API.
output_dim = 256  # Opzioni 256, 512, 1024
reg = 0.01

# Number of classes in the dataset
n_class = 8

seq_len = 200

# Hyperparameters
validation_split = 0.20
batch_size = 32
n_epochs = 250
early_stop_epochs=15
initial_lr = 1e-3
decay_length = 0.8
ratio_to_stop = 0.05

filter_1      = 32;   filter_2      = 64;   filter_3      = 32
kernel_size_1 = 25;   kernel_size_2 =  13;   kernel_size_3 = 7
neurons_4 = 64
neurons_5 = 16
attivaz_conv = 'tanh'
attivaz_mlp = 'tanh'
k_reg = 1e-3
b_reg = 1e-3
rate_drop = 0.05


def Coupling(input_shape=(201, 8), num_classes=n_class, reg=0.01):
    # Input con shape (201, 8), ovvero 200 time steps con 8 feature per step
    input = layers.Input(shape=input_shape, name='Convolutional_inputs')
    
    # Input per le etichette di classe condizionate (one-hot encoding)
    class_input = layers.Input(shape=(num_classes,), name='Class_Input')

    # Blocchi convoluzionali per l'estrazione di caratteristiche temporali
    x = layers.Conv1D(filters=filter_1, kernel_size=kernel_size_1, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_1')(input)
    x = layers.MaxPooling1D()(x)
    x = layers.Conv1D(filters=filter_2, kernel_size=kernel_size_2, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_2')(x)
    x = layers.MaxPooling1D()(x)
    x = layers.Conv1D(filters=filter_3, kernel_size=kernel_size_3, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_3')(x)
    x = layers.MaxPooling1D()(x)
    x = layers.Flatten()(x)

    # Concatenazione con l'input della classe
    x = layers.Concatenate()([x, class_input])

    # Livelli densi per la componente t (traslazione)
    t_layer_1 = keras.layers.Dense(128, activation="relu", kernel_regularizer=regularizers.l2(reg))(x)
    t_layer_2 = keras.layers.Dense(input_shape[0] * input_shape[1], activation="linear", kernel_regularizer=regularizers.l2(reg))(t_layer_1)
    t_layer_2_reshaped = keras.layers.Reshape(input_shape)(t_layer_2)

    # Livelli densi per la componente s (scalatura)
    s_layer_1 = keras.layers.Dense(128, activation="relu", kernel_regularizer=regularizers.l2(reg))(x)
    s_layer_2 = keras.layers.Dense(input_shape[0] * input_shape[1], activation="tanh", kernel_regularizer=regularizers.l2(reg))(s_layer_1)
    s_layer_2_reshaped = keras.layers.Reshape(input_shape)(s_layer_2)

    return keras.Model(inputs=[input, class_input], outputs=[s_layer_2_reshaped, t_layer_2_reshaped])


"""
## Real NVP
"""


class RealNVP(keras.Model):
    def __init__(self, num_coupling_layers, num_classes):
        super().__init__()

        self.num_coupling_layers = num_coupling_layers
        self.num_classes = num_classes

        # Distribuzione dello spazio latente (201, 8)
        self.distribution = tfp.distributions.MultivariateNormalDiag(
            loc=[0.0] * 1608, scale_diag=[1.0] * 1608  # La distribuzione ha 1608 dimensioni (201 * 8)
        )

        # Maschere (alterniamo met√† delle dimensioni su entrambe le direzioni)
        self.masks = tf.convert_to_tensor(
            [[0] * 808 + [1] * 800, [1] * 808 + [0] * 800] * (num_coupling_layers // 2), dtype=tf.float32
        )

        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.layers_list = [Coupling(input_shape=(201, 8), num_classes=num_classes) for i in range(num_coupling_layers)]
    
    @property
    def metrics(self):
        return [self.loss_tracker]

    def call(self, x, y, training=True):
        log_det_inv = 0
        direction = 1
        if training:
            direction = -1

        # x ha la forma (None, 200, 8)
        # y ha la forma (None, 8)

        # Espandi y per avere una forma compatibile con x per la concatenazione
        y_expanded = tf.expand_dims(y, axis=1)  # Ora ha forma (None, 1, 8)

        # Verifica che le etichette abbiano la forma corretta
        print(f"y_expanded: {y_expanded.shape}")

        # Concatenazione lungo l'asse temporale (asse 1)
        x_expanded = tf.concat([x, y_expanded], axis=1)  # Ora ha forma (None, 201, 8)

        print(f"x_expanded: {x_expanded.shape}")

        for i in range(self.num_coupling_layers)[::direction]:

            mask = tf.reshape(self.masks[i], [1, 201, 8])  # Aggiungi una dimensione batch

            x_masked = x_expanded * mask  # Applica la maschera solo alle features
            reversed_mask = 1 - mask  # Maschera inversa
            
            # Ottieni s e t dai layer di coupling
            s, t = self.layers_list[i]([x_masked, y_expanded[:, 0, :]])  # Passa solo il primo passo temporale

            # Debug: Verifica le forme di s e t
            print(f"s shape: {s.shape}")
            print(f"t shape: {t.shape}")
            
            # Applica la maschera inversa alle trasformazioni
            s *= reversed_mask
            t *= reversed_mask

            gate = (direction - 1) / 2
            x = (
                reversed_mask * (x_expanded * tf.exp(direction * s) + direction * t * tf.exp(gate * s))  # Trasformazione delle features
                + x_masked  # Mantieni le features mascherate
            )

            # Accumula il determinante del jacobiano inverso
            log_det_inv += gate * tf.reduce_sum(s, axis=[1, 2])  # Log-determinante

            return x, log_det_inv
    

    def log_loss(self, x, y):
        y_pred, logdet = self(x, y)

        print(f"y_pred shape: {y_pred.shape}")

        # Rimodella y_pred in (None, 1608) per farlo corrispondere alla dimensione della distribuzione
        y_pred = tf.reshape(y_pred, [-1, 1608])

        # Calcola la log-likelihood
        log_likelihood = self.distribution.log_prob(y_pred) + logdet

        return -tf.reduce_mean(log_likelihood)
    

    def train_step(self, data):
        x, y = data  # Estraiamo i dati e le etichette
        with tf.GradientTape() as tape:
            loss = self.log_loss(x, y)

        g = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(g, self.trainable_variables))
        self.loss_tracker.update_state(loss)

        return {"loss": self.loss_tracker.result()}

    def test_step(self, data):
        x, y = data
        loss = self.log_loss(x, y)
        self.loss_tracker.update_state(loss)

        return {"loss": self.loss_tracker.result()}


"""
## Model training
"""

observations_normalized, damage_class_labels, N_ist_train = read_data(path_data_train)

print(f"Forma di observations_normalized: {observations_normalized.shape}")
print(f"Forma di damage_class_labels: {damage_class_labels.shape}")

model = RealNVP(num_coupling_layers=6, num_classes=n_class)

model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001))

# Crea un dataset tf.data.Dataset dai dati e dalle etichette
damage_class_labels = tf.cast(damage_class_labels, tf.int64)  # Converti in int64
print(f"Forma di damage_class_labels 1: {damage_class_labels.shape}")
damage_class_labels_one_hot = tf.one_hot(damage_class_labels, depth=n_class)
print(f"Forma di damage_class_labels_one_hot: {damage_class_labels_one_hot.shape}")

dataset = tf.data.Dataset.from_tensor_slices((observations_normalized, damage_class_labels_one_hot))

# Calcola il numero di batch di validazione
validation_split = 0.2
dataset_size = len(observations_normalized)
validation_size = int(dataset_size * validation_split)
train_size = dataset_size - validation_size

# Dividi il dataset in train e validation
train_dataset = dataset.take(train_size).batch(batch_size)
validation_dataset = dataset.skip(train_size).take(validation_size).batch(batch_size)


history = model.fit(
    train_dataset, batch_size=32, epochs=100, verbose=2, validation_data=validation_dataset
)




"""
## Performance evaluation
"""

# Questa porzione del codice genera un grafico dell'andamento della perdita (\textit{loss}) durante il processo di addestramento del modello, 
# per entrambi i set di dati di addestramento e di validazione.
plt.figure(figsize=(15, 10))   #
plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("model loss")
plt.legend(["train", "validation"], loc="upper right")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.show()
plt.close()



#Test del modello ottenuto

# Calcolo del log-likelihood su dati di test/validazione
# test_log_likelihood = -model.log_loss(normalized_data_2)
# print(f"Test log-likelihood: {test_log_likelihood.numpy()}")

# # Genera nuovi campioni dallo spazio latente
# z_samples = model.distribution.sample(1000)  # Esempio di 1000 campioni latenti
# generated_samples, _ = model.predict(z_samples)

# # Confronta i nuovi campioni con i dati originali
# plt.figure(figsize=(10, 6))
# plt.scatter(generated_samples[:, 0], generated_samples[:, 1], label="Generated Samples", color='b', alpha=0.5)
# plt.scatter(normalized_data_1[:, 0], normalized_data_1[:, 1], label="Training Data", color='r', alpha=0.5)
# plt.legend()
# plt.show()

# from scipy.stats import wasserstein_distance

# # Calcolo della distanza di Wasserstein tra due distribuzioni (ad esempio, prima e seconda dimensione)
# wd = wasserstein_distance(normalized_data_1[:, 0], generated_samples[:, 0])
# print(f"Wasserstein distance (dimension 1): {wd}")

# #Latent Space Analysis
# from sklearn.decomposition import PCA
# from sklearn.manifold import TSNE

# # PCA per ridurre lo spazio latente a 2D
# pca = PCA(n_components=2)
# z_pca = pca.fit_transform(z)

# plt.figure(figsize=(8, 6))
# plt.scatter(z_pca[:, 0], z_pca[:, 1], label="Latent Space (PCA)", color='b', alpha=0.5)
# plt.title("Latent space after PCA")
# plt.show()
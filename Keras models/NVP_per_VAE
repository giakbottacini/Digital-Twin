"""
## Setup

"""
import os

os.environ["KERAS_BACKEND"] = "tensorflow"

import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras import regularizers
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_probability as tfp
import pandas as pd

from keras import ops
from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_absolute_error
import seaborn as sns

import scipy.stats as st



# Specify the example you are dealing with
esempio = 'L_FRAME_DT'
# Training and Testing data
ID              = '7_1'
save_ID         = 'Classifier_total_7_1/'
path            = "../" + esempio + '/Dati/'
path_data_train   = path + 'istantrain_' + ID
path_data_test   = path + 'istantest_' + ID
# Saving model
path_save       = "models/"
# Prediction model
restore_ID= 'Classifier_total_7_1/'
path_restore = path + restore_ID

# Which dof monitored
which_channels = [1,2,3,4,5,6,7,8]

n_channels = 8
seq_len = 200
# Specify if you are using accelerations =1 or displacements =0
accelerations = 0

# Questa funzione calcola il valore quadratico medio (RMS) di un vettore di segnale, utile per misurare l'energia del segnale e per 
# aggiungere rumore durante la corruzione dei segnali.
def RMS(vect):
    return np.sqrt(np.mean(np.square(vect)))


# ----------------------------------------------------------------------------

"""
## Importazione dati
"""
# addedd_SNR = 100

# Read data and create dataset with only desired classes
def read_data(path_data, addedd_SNR):    
    label_path_class = path_data + '/Damage_class.csv'                                
    labels_class     = np.genfromtxt(label_path_class)
    labels_class     = labels_class.astype('int')
    N_ist = len(labels_class)  

    if accelerations == 1:
        path_rec  = path_data + '/U2_concat_'
    elif accelerations == 0:
        path_rec  = path_data + '/U_concat_'

    if path_data == path_data_train:
        signals_means = np.zeros((n_channels))
        signals_stds = np.zeros((n_channels))
    else:
        signals_means = np.load(path_restore+'/Corrupted_signals_means.npy')
        signals_stds = np.load(path_restore+'/Corrupted_signals_stds.npy')
    # Create the dataset output structure
    X       = np.zeros((N_ist, seq_len, n_channels))   #(10000,200,8)
    X_noise = np.zeros((N_ist, seq_len, n_channels))
    for i1 in range(n_channels):
        path_data_var  = path_rec + str(which_channels[i1]) + '.csv'
        X_singledof = pd.read_csv(path_data_var, header=None).to_numpy()[:,0]
        for i2 in range(N_ist):
            # RESHAPE THE SIGNALS
            X[i2,:,i1]= X_singledof[1 + i2*(1+seq_len) : (i2+1)*(1+seq_len)]
            # # CORRUPT THE SIGNALS
            rms_signal = RMS(X[i2,:,i1])
            dev_std    = rms_signal / np.sqrt(addedd_SNR)
            sample = st.norm(0, dev_std)
            X_noise[i2,:,i1] = X[i2,:,i1] + sample.rvs(size=seq_len)
        if path_data == path_data_train:
            # COMPUTE STATISTICS FOR EACH CHANNEL
            signals_means[i1] = np.mean(np.reshape(X_noise[:,:,i1], (N_ist*seq_len)))
            signals_stds[i1] = np.std(np.reshape(X_noise[:,:,i1], (N_ist*seq_len)))
            # signals_means[i1] = np.mean(np.reshape(X[:,:,i1], (N_ist*seq_len)))
            # signals_stds[i1] = np.std(np.reshape(X[:,:,i1], (N_ist*seq_len)))

        # NORMALIZE THE SIGNALS    
        X_noise[:,:,i1] = (X_noise[:,:,i1] - signals_means[i1])/signals_stds[i1]
        # X[:,:,i1] = (X[:,:,i1] - signals_means[i1])/signals_stds[i1]
   
    # if path_data == path_data_train:    
    #     np.save(path_save+'Corrupted_signals_means', signals_means)
    #     np.save(path_save+'Corrupted_signals_stds', signals_stds)
       
    # return X_noise, labels_class, N_ist
    return X_noise, labels_class, N_ist


# ----------------------------------------------------------------------------------------------------------------------

"""
## VAE
"""

"""
## Create a sampling layer
"""

class Sampling(layers.Layer):
    """Uses (z_mean, z_log_var) to sample z, the vector encoding a digit."""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.seed_generator = keras.random.SeedGenerator(1337)

    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = ops.shape(z_mean)[0]
        dim = ops.shape(z_mean)[1]
        epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)
        return z_mean + ops.exp(0.5 * z_log_var) * epsilon



"""
## Using Matteo's architecture
"""

n_class = 8
latent_dim = 4

# Hyperparameters
validation_split = 0.20
batch_size = 32
n_epochs = 150  #250
early_stop_epochs=15
initial_lr = 1e-3
decay_length = 0.8
ratio_to_stop = 0.05

filter_1      = 32;   filter_2      = 64;   filter_3      = 32
kernel_size_1 = 25;   kernel_size_2 =  13;   kernel_size_3 = 7
neurons_4 = 64
neurons_5 = 16
attivaz_conv = 'tanh'
attivaz_mlp = 'tanh'
k_reg = 1e-3
b_reg = 1e-3
rate_drop = 0.05

"""
## Build the encoder 
"""

encoder_inputs  = layers.Input(shape=(seq_len, n_channels), name='Convolutional_inputs')
x = layers.Conv1D(filters=filter_1, kernel_size=kernel_size_1, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_1')(encoder_inputs)
x = layers.MaxPooling1D()(x)
x = layers.Dropout(rate=rate_drop)(x)   #Gli strati Dropout vengono utilizzati per regolarizzare il modello durante l'addestramento, disattivando casualmente una frazione di neuroni. Questo aiuta a prevenire l'overfitting.

x = layers.Conv1D(filters=filter_2, kernel_size=kernel_size_2, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_2')(x)
x = layers.MaxPooling1D()(x)
x = layers.Dropout(rate=rate_drop)(x)

x = layers.Conv1D(filters=filter_3, kernel_size=kernel_size_3, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_3')(x)
x = layers.MaxPooling1D()(x)
x = layers.Dropout(rate=rate_drop)(x)

x = layers.Flatten()(x)

x = layers.Dense(units=neurons_4, activation=attivaz_mlp, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), name='Dense_1')(x)
x = layers.Dense(units=neurons_5, activation=attivaz_mlp, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), name='Dense_2')(x)

z_mean = layers.Dense(latent_dim, activation=None, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), name='z_mean')(x)
z_log_var = layers.Dense(latent_dim, activation=None, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), name='z_log_var')(x)
z = Sampling()([z_mean, z_log_var])

# Definisci il modello
encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name="encoder")
encoder.summary()

"""
## Build the decoder 
"""

latent_inputs_decoder = layers.Input(shape=(latent_dim,), name='Latent_inputs')

x = layers.Dense(units=neurons_5, activation=attivaz_mlp, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), name='Dense_1_decoder')(latent_inputs_decoder)
x = layers.Dense(units=neurons_4, activation=attivaz_mlp, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), name='Dense_2_decoder')(x)
x = layers.Dense(units=filter_3 * (seq_len // 8), activation=attivaz_mlp, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), name='Dense_3_decoder')(x)
x = layers.Reshape((seq_len // 8, filter_3))(x)

# Applica strati Conv1DTranspose (o Conv1D per upsampling)
x = layers.Conv1DTranspose(filters=filter_3, kernel_size=kernel_size_3, activation=attivaz_conv, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), name='ConvT_1')(x)
x = layers.UpSampling1D()(x)
x = layers.Dropout(rate=rate_drop)(x)

x = layers.Conv1DTranspose(filters=filter_2, kernel_size=kernel_size_2, activation=attivaz_conv, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), name='ConvT_2')(x)
x = layers.UpSampling1D()(x)
x = layers.Dropout(rate=rate_drop)(x)

x = layers.Conv1DTranspose(filters=filter_1, kernel_size=kernel_size_1, activation=attivaz_conv, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), name='ConvT_3')(x)
x = layers.UpSampling1D()(x)
x = layers.Dropout(rate=rate_drop)(x)

# Ultimo livello per tornare alla forma originale
decoder_outputs = layers.Conv1D(filters=n_channels, kernel_size=kernel_size_1, activation=None, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), name='Conv_1_decoder')(x)

# Definisci il modello
decoder = keras.Model(latent_inputs_decoder, decoder_outputs, name='decoder')
decoder.summary()




"""
## Build the Dense layer
"""

latent_inputs_classifier = layers.Input(shape=(latent_dim,), name='Latent_inputs')

x = layers.Dense(units=64, activation=attivaz_mlp, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), name='Dense_1')(latent_inputs_classifier)
x = layers.Dense(units=32, activation=attivaz_mlp, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), name='Dense_2')(x)
Dense_output = layers.Dense(n_class,activation=keras.activations.softmax,kernel_regularizer=regularizers.l2(k_reg),bias_regularizer=regularizers.l2(b_reg),name='Dense_3_with_Softmax')(x)

# Definisci il modello
classifier = keras.Model(latent_inputs_classifier, Dense_output, name='classifier')
classifier.summary()


"""
## Define the VAE class
"""

class VAE(keras.Model):
    def __init__(self, encoder, decoder, classifier,**kwargs):
        super().__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
        self.classifier = classifier
        self.total_loss_tracker = keras.metrics.Mean(name="total_loss")
        self.reconstruction_loss_tracker = keras.metrics.Mean(name="reconstruction_loss")
        self.kl_loss_tracker = keras.metrics.Mean(name="kl_loss")
        self.classification_loss_tracker = keras.metrics.Mean(name="classification_loss")

    @property
    def metrics(self):
        return [
            self.total_loss_tracker,
            self.reconstruction_loss_tracker,
            self.kl_loss_tracker,
        ]

    def train_step(self, data):

        observations, damage_class_labels = data


        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = self.encoder(observations)

            reconstruction = self.decoder(z)

            # Predizione per la classificazione
            classification_preds = self.classifier(z)

            reconstruction_loss = ops.mean(
                ops.sum(
                    # keras.losses.binary_crossentropy(observations, reconstruction), #La funzione keras.losses.binary_crossentropy(data, reconstruction) restituisce un tensore bidimensionale di forma (batch_size, time_steps). 
                                                                                      #Questo avviene perché binary_crossentropy calcola la perdita per ogni time step, non per ogni canale.
                    keras.losses.mean_squared_error(observations, reconstruction),  # Calcola la perdita MSE tra dati originali e ricostruiti
                    axis=1,   
                )
            )
            
            kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))
            kl_loss = ops.mean(ops.sum(kl_loss, axis=1))

            # Perdita di classificazione (categorical crossentropy)
            classification_loss = keras.losses.sparse_categorical_crossentropy(damage_class_labels, classification_preds)
            classification_loss = tf.reduce_mean(classification_loss)

            beta = 1.0
            gamma = 60.0
            total_loss = reconstruction_loss + beta * kl_loss + gamma * classification_loss

        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))

        self.total_loss_tracker.update_state(total_loss)
        self.reconstruction_loss_tracker.update_state(reconstruction_loss)
        self.kl_loss_tracker.update_state(kl_loss)
        self.classification_loss_tracker.update_state(classification_loss)
        return {
            "loss": self.total_loss_tracker.result(),
            "reconstruction_loss": self.reconstruction_loss_tracker.result(),
            "kl_loss": self.kl_loss_tracker.result(),
            "classification_loss": self.classification_loss_tracker.result(),
        }
    
    
    def test_step(self, data):
        observations, damage_class_labels = data
        z_mean, z_log_var, z = self.encoder(observations)
        reconstruction = self.decoder(z)
        classification_preds = self.classifier(z)

        # Calcolo della perdita per il test
        reconstruction_loss = ops.mean(
            ops.sum(
                # keras.losses.binary_crossentropy(data, reconstruction), #La funzione keras.losses.binary_crossentropy(data, reconstruction) restituisce un tensore bidimensionale di forma (batch_size, time_steps). 
                                                                          #Questo avviene perché binary_crossentropy calcola la perdita per ogni time step, non per ogni canale.
                keras.losses.mean_squared_error(observations, reconstruction),  # Calcola la perdita MSE tra dati originali e ricostruiti
                axis=1,   
                )
            )
        
        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)
        kl_loss = tf.reduce_mean(kl_loss)

        classification_loss = keras.losses.sparse_categorical_crossentropy(damage_class_labels, classification_preds)
        classification_loss = tf.reduce_mean(classification_loss)

        beta = 1.0
        gamma = 60.0
        total_loss = reconstruction_loss + beta * kl_loss + gamma * classification_loss

        return {
            "loss": total_loss,
            "reconstruction_loss": reconstruction_loss,
            "kl_loss": kl_loss,
            "classification_loss": classification_loss,
        }
    
    
    def call(self, inputs):
        # Passaggio attraverso l'encoder
        z_mean, z_log_var, z = self.encoder(inputs)
        
        # Decodifica del campione z
        reconstructed = self.decoder(z)

        # Decodifica del campione z
        classified = self.classifier(z)
        
        return reconstructed, classified



# -----------------------------------------------------------------------------------------------------------------------




"""
## Affine coupling layer
"""

# Creating a custom layer with keras API.
output_dim = 256  # Opzioni 256, 512, 1024
reg = 0.01

# Number of classes in the dataset
n_class = 8

seq_len = 200

# Hyperparameters
validation_split = 0.20
batch_size = 32
n_epochs = 150
early_stop_epochs=10
initial_lr = 1e-3
decay_length = 0.8
ratio_to_stop = 0.05

filter_1      = 32;   filter_2      = 64;   filter_3      = 32
kernel_size_1 = 25;   kernel_size_2 =  13;   kernel_size_3 = 7
neurons_4 = 64
neurons_5 = 16
attivaz_conv = 'tanh'
attivaz_mlp = 'tanh'
k_reg = 1e-3
b_reg = 1e-3
rate_drop = 0.05



# Mia struttura con primo Enhancement del layer embedding ma più fit   
def Coupling(input_shape=(200, 8), latent_dimension=latent_dim, reg=0.01):
    # Input con shape (200, 8), ovvero 200 time steps con 8 feature per step
    input_x = layers.Input(shape=input_shape, name='Input_X')
    input_z = layers.Input(shape=(latent_dimension,), name='Input_Z')
    
    # Network per condizionare s e t in funzione del label y
    # Uso un piccolo MLP per una rappresentazione più complessa dell'embedding delle classi
    label_condition = layers.Dense(16, activation='relu')(input_z)
    label_condition = layers.Dense(32, activation='relu')(label_condition)
    label_condition = layers.Dense(64, activation='relu')(label_condition)
    label_embedding = layers.Dense(128, activation='relu')(label_condition) 

    # Blocchi convoluzionali per l'estrazione di caratteristiche temporali
    x = layers.Conv1D(filters=filter_1, kernel_size=kernel_size_1, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_1')(input_x)
    x = layers.MaxPooling1D()(x)
    x = layers.Dropout(0.3)(x)   # Aggiungi Dropout
    x = layers.Conv1D(filters=filter_2, kernel_size=kernel_size_2, strides=2, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_2')(x)
    x = layers.Dropout(0.3)(x)  # Aggiungi Dropout
    x = layers.Conv1D(filters=filter_3, kernel_size=kernel_size_3, strides=2, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_3')(x)
    x = layers.Dropout(0.3)(x)  # Aggiungi Dropout

    x = layers.Flatten()(x)

    # Livelli densi per la componente t (traslazione) con condizionamento MLP
    t = keras.layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(k_reg))(x)
    t_embedding = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(k_reg))(label_embedding)
    t = layers.Concatenate()([t, t_embedding])  # Condizionamento
    t = keras.layers.Dense(input_shape[0] * input_shape[1], activation="linear", kernel_regularizer=regularizers.l2(k_reg))(t)
    t_reshaped = keras.layers.Reshape(input_shape)(t)

    # Livelli densi per la componente s (scalatura) con condizionamento MLP
    s = keras.layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(k_reg))(x)
    s_embedding = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(k_reg))(label_embedding)
    s = layers.Concatenate()([s, s_embedding])  # Condizionamento
    s = keras.layers.Dense(input_shape[0] * input_shape[1], activation="tanh", kernel_regularizer=regularizers.l2(k_reg))(s)
    s_reshaped = keras.layers.Reshape(input_shape)(s)

    return keras.Model(inputs=[input_x, input_z], outputs=[s_reshaped, t_reshaped])


    

"""
## Real NVP
"""


class RealNVP(keras.Model):
    def __init__(self, num_coupling_layers, num_classes):
        super().__init__()

        self.num_coupling_layers = num_coupling_layers
        self.num_classes = num_classes

        # Distribuzione dello spazio latente (200, 8)
        self.distribution = tfp.distributions.MultivariateNormalDiag(
            loc=[0.0] * 1600, scale_diag=[1.0] * 1600  # La distribuzione ha 1600 dimensioni (200 * 8)
        )

        # Maschere (alterniamo metà delle dimensioni su entrambe le direzioni)
        self.masks = tf.convert_to_tensor(
            [[0] * 800 + [1] * 800, [1] * 800 + [0] * 800] * (num_coupling_layers // 2), dtype=tf.float32
        )


        # # Creazione della maschera alternata sui canali
        # self.masks = tf.convert_to_tensor(
        #     [[1, 0] * 4] * num_coupling_layers,  # Alternanza sui canali (8 canali: [1, 0, 1, 0, 1, 0, 1, 0])
        #     dtype=tf.float32
        # )

        # # Ogni maschera è ora di dimensione (8,) quindi dobbiamo estendere la maschera in base ai time steps
        # self.masks = tf.reshape(self.masks, [num_coupling_layers, 1, 1, 8])  # Aggiunge dimensioni batch e time steps


        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.log_likelihood_tracker = keras.metrics.Mean(name="log_likelihood")
        self.log_det_inv_tracker = keras.metrics.Mean(name="log_det_inv")

        self.layers_list = [Coupling(input_shape=(200, 8), latent_dimension=latent_dim) for i in range(num_coupling_layers)]
    
    @property
    def metrics(self):
        return [self.loss_tracker,
                self.log_likelihood_tracker,
                self.log_det_inv_tracker]


    # call con struttura iniziale 
    def call(self, x, y, training=True):  

        log_det_inv = 0   

        for i in range(self.num_coupling_layers):
            # # Applica la maschera alternata sui canali
            # mask = self.masks[i]  # Ottieni la maschera per il layer i-esimo
            # mask = tf.tile(mask, [tf.shape(x)[0], tf.shape(x)[1], 1])  # Replica la maschera per batch e time steps

            mask = tf.reshape(self.masks[i], [1, 200, 8])  # Aggiungi una dimensione batch
            x_masked = x * mask  # Maschera l'input
            reversed_mask = 1 - mask
            s, t = self.layers_list[i]([x_masked, y])
            s *= reversed_mask
            t *= reversed_mask

            x = (
                reversed_mask * (x * tf.exp(s) +  t )
                + x_masked
            )
            log_det_inv += tf.reduce_sum(s, axis=[1, 2])

        return x, log_det_inv
    

    def compute_loss(self, x, y):

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y)

        # Rimodella z_pred in (None, 1600) per farlo corrispondere alla dimensione della distribuzione
        z_pred = tf.reshape(z_pred, [-1, 1600])

        # Calcola la log-likelihood condizionata
        log_likelihood_z = self.distribution.log_prob(z_pred) 

        return tf.reduce_mean(log_likelihood_z), tf.reduce_mean(log_det_inv)


    

    def train_step(self, data):
        x, y = data  # Estraiamo i dati e le etichette
        with tf.GradientTape() as tape:
            log_likelihood_z, log_det_inv = self.compute_loss(x, y)

            # Total loss is the sum of log-likelihood and the log-det term
            loss = -(log_likelihood_z + log_det_inv)

        # Calcola i gradienti
        grads = tape.gradient(loss, self.trainable_variables)
        # Applica i gradienti
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        # Aggiorna lo stato del tracker della perdita
        self.loss_tracker.update_state(loss)
        # Aggiorna i tracker delle componenti
        self.log_likelihood_tracker.update_state(log_likelihood_z)
        self.log_det_inv_tracker.update_state(log_det_inv)

        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }



    def test_step(self, data):
        x, y = data
        log_likelihood_z, log_det_inv = self.compute_loss(x, y)
        loss = -(log_likelihood_z + log_det_inv)

        self.loss_tracker.update_state(loss)
        self.log_likelihood_tracker.update_state(log_likelihood_z)
        self.log_det_inv_tracker.update_state(log_det_inv)
        
        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }


    def predict(self, x, y):
        # Assicurati che x e y siano TensorFlow tensors
        x = tf.convert_to_tensor(x, dtype=tf.float32)
        y = tf.convert_to_tensor(y, dtype=tf.float32)

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y, training=False)

        print(f"log_det_inv: {log_det_inv}")

        # Rimodella z_pred in (None, 1600) per farlo corrispondere alla dimensione della distribuzione
        z_pred = tf.reshape(z_pred, [-1, 1600])

        # Calcola la log-likelihood condizionata
        log_likelihood_z = self.distribution.log_prob(z_pred)

        log_likelihood = log_likelihood_z + log_det_inv

        return log_likelihood





"""
## Funzioni utili
"""



        

train_or_predict = 0   # 1: training, 0: predict                                       


if train_or_predict:
    """
    ## Model training
    """

    observations_normalized, damage_class_labels, N_ist_train = read_data(path_data_train, 100)

    print(f"Forma di observations_normalized: {observations_normalized.shape}")
    print(f"Forma di damage_class_labels: {damage_class_labels.shape}")


    NVP = RealNVP(num_coupling_layers=10, num_classes=n_class)

    NVP.compile(optimizer=keras.optimizers.Adam(learning_rate=keras.optimizers.schedules.CosineDecay(initial_learning_rate=initial_lr, decay_steps=int(decay_length*n_epochs*N_ist_train*(1-validation_split)/batch_size), alpha=ratio_to_stop)))
    NVP.summary()



    # Creazione dell'istanza VAE
    vae = VAE(encoder, decoder, classifier)

    # Caricamento dei pesi salvati
    vae.load_weights('./models/VAE_1D.weights.h5')

    _, _, latent_values = vae.encoder.predict(observations_normalized, verbose=0)

    print(f"latent_values generati")


    dataset = tf.data.Dataset.from_tensor_slices((observations_normalized, latent_values))


    # Calcola il numero di batch di validazione
    validation_split = 0.2
    dataset_size = len(observations_normalized)
    validation_size = int(dataset_size * validation_split)
    train_size = dataset_size - validation_size

    # Dividi il dataset in train e validation
    train_dataset = dataset.take(train_size).batch(batch_size)
    validation_dataset = dataset.skip(train_size).take(validation_size).batch(batch_size)


    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stop_epochs, restore_best_weights=True)

    history = NVP.fit(
        train_dataset, epochs=n_epochs, verbose=2, validation_data=validation_dataset, callbacks=[early_stop]
    )


    # Salva lo storico dell'addestramento in formato pickle
    hist = pd.DataFrame(history.history)  # Converti history.history (dati storici dell'addestramento) in un DataFrame di pandas
    hist['epoch'] = history.epoch  # Aggiungi una colonna per le epoche. history.epoch è una lista che contiene i numeri delle epoche per cui sono stati registrati i dati.

    # Salva il DataFrame
    try:
        hist.to_pickle(os.path.join(path_save, 'hist_NVP_per_VAE.pkl'))   #Salva il DataFrame hist in un file con formato pickle.
        print(f"Storico salvato in {os.path.join(path_save, 'hist_NVP_per_VAE.pkl')}")
    except Exception as e:
        print(f"Errore durante il salvataggio dello storico: {e}")


    # Salva i pesi
    NVP.save_weights('./models/NVP_per_VAE.weights.h5')


    """
    ## Performance evaluation
    """

    # Questa porzione del codice genera un grafico dell'andamento della perdita (\textit{loss}) durante il processo di addestramento del modello, 
    # per entrambi i set di dati di addestramento e di validazione.
    plt.figure(figsize=(15, 10))   #
    plt.plot(history.history["loss"])
    plt.plot(history.history["val_loss"])
    plt.title("model loss")
    plt.legend(["train", "validation"], loc="upper right")
    plt.ylabel("loss")
    plt.xlabel("epoch")
    plt.show()
    plt.close()

   
else:
    """
    ## Predict
    """
    output_dir = "output_NVP_per_VAE"

    # Caricamento dei dati di test
    X_test, labels_test, N_ist_test = read_data(path_data_test, 100)

    print(f"Forma di X_test: {X_test.shape}")

    X_train, labels_train, N_ist_train = read_data(path_data_train, 100)

    print(f"Forma di X_train: {X_train.shape}")


    # Creazione dell'istanza VAE
    vae = VAE(encoder, decoder, classifier)

    # Caricamento dei pesi salvati
    vae.load_weights('./models/VAE_1D.weights.h5')


    _, _, latent_values_test = vae.encoder.predict(X_test, verbose=0)

    _, _, latent_values_train = vae.encoder.predict(X_train, verbose=0)

    print(f"latent_values generati")



    # Creazione dell'istanza RealNVP
    NVP = RealNVP(num_coupling_layers=10, num_classes=n_class)

    # NVP.compile(optimizer = keras.optimizers.Adam(learning_rate=keras.optimizers.schedules.CosineDecay(initial_learning_rate=initial_lr, decay_steps=int(decay_length*n_epochs*N_ist_test*(1-validation_split)/batch_size), alpha=ratio_to_stop)))
    #Non serve compilare se non devo continuare il training

    # Caricamento dei pesi salvati
    NVP.load_weights('./models/NVP_per_VAE.weights.h5')

    # Caricamento della cronologia dell'addestramento
    hist = pd.read_pickle(path_save + 'hist_NVP_per_VAE.pkl')


    #Generazione loglikelihood per dati di test e train
    log_likelihood_test = NVP.predict(X_test, latent_values_test)  
    log_likelihood_train = NVP.predict(X_train, latent_values_train) 

    print(f"Forma di log_likelihood_test: {log_likelihood_test.shape}")
    print(f"Forma di log_likelihood_train: {log_likelihood_train.shape}")


    # Converti i tensori in array NumPy
    log_likelihood_train_np = log_likelihood_train.numpy()
    log_likelihood_test_np = log_likelihood_test.numpy()

   # Crea la figura
    plt.figure(figsize=(8, 6))

    # Istogramma del vettore log_likelihood_train
    plt.hist(log_likelihood_train_np, bins=100, range=(min(log_likelihood_train_np), max(log_likelihood_train_np)),
            alpha=0.9, color='b', label='Train', density=True)

    # Istogramma del vettore log_likelihood_test
    plt.hist(log_likelihood_test_np, bins=100, range=(min(log_likelihood_test_np), max(log_likelihood_test_np)),
            alpha=0.8, color='r', label='Test', density=True)

    # Titolo e etichette
    plt.title('Istogramma sovrapposto delle log-likelihood')
    plt.xlabel('log-likelihood')
    plt.ylabel('Frequenza normalizzata')
    plt.grid(True)

    # Aggiungi una legenda
    plt.legend()

    # Salva il grafico in un file immagine
    plt.savefig(os.path.join(output_dir, "hist_confronto.png"))

    # Mostra il grafico
    plt.show()





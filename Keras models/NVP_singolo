"""
Title: Density estimation using Real NVP
Authors: [Mandolini Giorgio Maria](https://www.linkedin.com/in/giorgio-maria-mandolini-a2a1b71b4/), [Sanna Daniele](https://www.linkedin.com/in/daniele-sanna-338629bb/), [Zannini Quirini Giorgio](https://www.linkedin.com/in/giorgio-zannini-quirini-16ab181a0/)
Date created: 2020/08/10
Last modified: 2020/08/10
Description: Estimating the density distribution of the "double moon" dataset.
Accelerator: GPU
"""

"""
## Introduction

The aim of this work is to map a simple distribution - which is easy to sample
and whose density is simple to estimate - to a more complex one learned from the data.
This kind of generative model is also known as "normalizing flow".

In order to do this, the model is trained via the maximum
likelihood principle, using the "change of variable" formula.

We will use an affine coupling function. We create it such that its inverse, as well as
the determinant of the Jacobian, are easy to obtain (more details in the referenced paper).

**Requirements:**

* Tensorflow 2.9.1
* Tensorflow probability 0.17.0

**Reference:**

[Density estimation using Real NVP](https://arxiv.org/abs/1605.08803)
"""

"""
## Setup

"""
import os

os.environ["KERAS_BACKEND"] = "tensorflow"

import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras import regularizers
from sklearn.datasets import make_moons
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_probability as tfp
import pandas as pd




"""
## Load the data
"""

# Path alla directory dove sono salvati i file CSV
csv_directory = "../L_FRAME_DT/Dati/istantrain_7_1/"
 
# Lista dei nomi dei file CSV
file_name = "U_concat_1.csv"

# Inizializza una lista per contenere i dati di ciascun sensore
sensori_data = []

# Carica il file CSV
file_path = os.path.join(csv_directory, file_name)
sensor_data = pd.read_csv(file_path, header=None).values.flatten()  # Converte in array numpy
sensori_data.append(sensor_data)

# Trasponi l'array in modo che le colonne siano i sensori e le righe le osservazioni
# Ogni colonna rappresenta un sensore, e ogni riga è una rilevazione.
dataset = np.stack(sensori_data, axis=1)

print(f"Forma di dataset: {dataset.shape}")

# Trova gli indici degli zeri che separano le osservazioni
split_indices = np.where(np.all(dataset == 0, axis=1))[0]

print(f"Forma di split_indices: {split_indices.shape}")

# Suddividi il dataset nelle osservazioni corrispoPrimindenti
observations = []
start_index = 0

for end_index in split_indices:
    if end_index > start_index:
        observation = dataset[start_index:end_index, :]
        if observation.shape[0] == 200:  # Assicurati che la lunghezza sia di 200 rilevazioni
            observations.append(observation)
    start_index = end_index + 1
#Con questo for manca l'ultima, ora la aggiungo
    observation = dataset[start_index:, :]
    if observation.shape[0] == 200:  
            observations.append(observation)

# Converti la lista in un array numpy con shape (10000, 200, 8)
observations = np.array(observations)

data = observations.squeeze()

print(f"Forma di observations: {observations.shape}")

for i in range(2):  
    print(f"Forma della {i+1}° osservazione: {observations[i].shape}")

# norm = layers.Normalization()  
# norm.adapt(data)  
# normalized_data = norm(data)

# print(f"Forma di normalized_data: {normalized_data.shape}")

# Dividiamo il dataset in dataset_1 (prime 8000 osservazioni) e dataset_2 (ultime 2000 osservazioni)
data_1 = data[:8000]
data_2 = data[8000:]

# Stampa le forme dei due sottoinsiemi
print(f"Forma di dataset_1: {data_1.shape}")  # Dovrebbe essere (8000, 200, 8)
print(f"Forma di dataset_2: {data_2.shape}")  # Dovrebbe essere (2000, 200, 8)

# Normalizzazione sui due dataset
norm_1 = layers.Normalization()
norm_2 = layers.Normalization()

# Adattamento dei layer di normalizzazione sui rispettivi dataset
norm_1.adapt(data_1)
norm_2.adapt(data_2)

# Normalizza i dataset
normalized_data_1 = norm_1(data_1)
normalized_data_2 = norm_2(data_2)

# Controllo delle forme
print(f"Forma di normalized_data_1: {normalized_data_1.shape}")
print(f"Forma di normalized_data_2: {normalized_data_2.shape}")


"""
## Affine coupling layer
"""

# Creating a custom layer with keras API.
output_dim = 256  # Opzioni 256, 512, 1024
reg = 0.01


def Coupling(input_shape):  #La funzione prende come parametro input_shape, che rappresenta la forma dell'input (cioè il numero di feature del dato in ingresso).
    
    #La funzione definisce due serie di strati densi (Dense layers), uno per la componente di traslazione (t_layers) e uno per la componente di scalatura (s_layers), 
    # che insieme costituiscono la parte chiave del modello.
    
    input = keras.layers.Input(shape=(input_shape,))

    #La rete t_layers rappresenta una funzione che trasforma i dati originali tramite una serie di strati densi (fully connected).
    #Ogni strato denso ha un numero di nodi pari a output_dim e utilizza l'attivazione ReLU.
    t_layer_1 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg) #Viene utilizzato un termine di regolarizzazione L2 (regularizers.l2(reg)) per limitare i pesi dei neuroni e prevenire l'overfitting.
    )(input)
    t_layer_2 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
    )(t_layer_1)
    t_layer_3 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
    )(t_layer_2)
    t_layer_4 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
    )(t_layer_3)
    t_layer_5 = keras.layers.Dense(
        input_shape, activation="linear", kernel_regularizer=regularizers.l2(reg) #L'ultimo strato denso ha come numero di nodi input_shape e utilizza l'attivazione lineare (cioè nessuna attivazione), 
    )(t_layer_4)                                                                  #poiché questo strato rappresenta la traslazione che non richiede una trasformazione non lineare finale.

    #La rete s_layers è molto simile a t_layers, con una serie di strati densi che rappresentano la funzione di scalatura dei dati.
    s_layer_1 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
    )(input)
    s_layer_2 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
    )(s_layer_1)
    s_layer_3 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
    )(s_layer_2)
    s_layer_4 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
    )(s_layer_3)
    s_layer_5 = keras.layers.Dense(
        input_shape, activation="tanh", kernel_regularizer=regularizers.l2(reg)  #L'ultimo strato denso ha una funzione di attivazione tanh, che limita i valori prodotti 
    )(s_layer_4)                                                                 #a un intervallo tra -1 e 1. Questo è comune per la componente di scalatura poiché mantiene i valori numerici stabili.

    return keras.Model(inputs=input, outputs=[s_layer_5, t_layer_5])  
    #Viene creato un modello Keras che prende l'input e produce due output: s_layer_5: la componente di scalatura e t_layer_5: la componente di traslazione.

# # Convolutional layer should be better for time series
# def Coupling(input_shape=200):
#     input = keras.layers.Input(shape=(input_shape, 1))  # Aggiungi un canale per la convoluzione

#     # Blocchi convoluzionali per l'estrazione di caratteristiche temporali
#     conv_layer_1 = keras.layers.Conv1D(64, kernel_size=3, activation="relu", padding="same")(input)
#     conv_layer_2 = keras.layers.Conv1D(128, kernel_size=3, activation="relu", padding="same")(conv_layer_1)
#     flatten = keras.layers.Flatten()(conv_layer_2)

#     # Livelli densi per la componente t (traslazione)
#     t_layer_1 = keras.layers.Dense(128, activation="relu", kernel_regularizer=regularizers.l2(reg))(flatten)
#     t_layer_2 = keras.layers.Dense(input_shape, activation="linear", kernel_regularizer=regularizers.l2(reg))(t_layer_1)

#     # Livelli densi per la componente s (scalatura)
#     s_layer_1 = keras.layers.Dense(128, activation="relu", kernel_regularizer=regularizers.l2(reg))(flatten)
#     s_layer_2 = keras.layers.Dense(input_shape, activation="tanh", kernel_regularizer=regularizers.l2(reg))(s_layer_1)

#     return keras.Model(inputs=input, outputs=[s_layer_2, t_layer_2])


"""
## Real NVP
"""


class RealNVP(keras.Model):
    def __init__(self, num_coupling_layers):
        super().__init__()

        self.num_coupling_layers = num_coupling_layers  #Memorizza il numero di strati di accoppiamento (coupling layers) che verranno utilizzati nel modello.

        # Distribution of the latent space.
        self.distribution = tfp.distributions.MultivariateNormalDiag(   #Viene definita una distribuzione normale multivariata (tfp.distributions.MultivariateNormalDiag) con media loc=[0.0, 0.0] e deviazione standard scale_diag=[1.0, 1.0]. Questa distribuzione rappresenta lo spazio latente in cui il modello proietta i dati dopo aver applicato le trasformazioni.
            loc=[0.0] * 200, scale_diag=[1.0] * 200    #Questa distribuzione rappresenta lo spazio latente in cui il modello proietta i dati dopo aver applicato le trasformazioni.
        )
        self.masks = np.array(   #self.masks è una matrice utilizzata per controllare quali componenti dell'input vengono trasformati a ciascuno strato di accoppiamento.
            [[0] * 100 + [1] * 100, [1] * 100 + [0] * 100] * (num_coupling_layers // 2), dtype="float32"   #La maschera alterna i valori prendendo o i primi 100 o i secondi 100, il che significa che i dati vengono suddivisi in due componenti. Ogni strato di accoppiamento trasforma una sola componente alla volta, lasciando l'altra invariata.
        )
        self.loss_tracker = keras.metrics.Mean(name="loss")    #Viene utilizzato un keras.metrics.Mean chiamato loss_tracker per tracciare il valore medio della perdita (loss) durante l'addestramento del modello. 
        self.layers_list = [Coupling(200) for i in range(num_coupling_layers)]  #Questa lista contiene num_coupling_layers strati di accoppiamento, ciascuno creato utilizzando la funzione Coupling(2). Ogni strato trasforma l'input tramite la combinazione di reti di traslazione e scalatura, come visto in precedenza. 
    
    # Ogni strato di accoppiamento nella lista self.layers_list trasforma l'input utilizzando le reti di scalatura e traslazione (s_layers e t_layers). 
    # Queste trasformazioni sono invertibili e consentono di "mappare" i dati verso una distribuzione più semplice, mantenendo la possibilità di invertire 
    # le trasformazioni per ricostruire i dati originali.

    @property
    def metrics(self):
        """List of the model's metrics.

        We make sure the loss tracker is listed as part of `model.metrics`
        so that `fit()` and `evaluate()` are able to `reset()` the loss tracker
        at the start of each epoch and at the start of an `evaluate()` call.
        """
        return [self.loss_tracker]

    def call(self, x, training=True):  #training: Un booleano che indica se il modello è in fase di addestramento (True) o inferenza (False). Questo influenza la direzione delle trasformazioni.
        log_det_inv = 0   #Questa variabile accumula il logaritmo del determinante jacobiano della trasformazione invertibile. Il determinante del Jacobiano tiene traccia della modifica di volume in ogni passaggio della trasformazione.
        direction = 1
        if training:
            direction = -1
        for i in range(self.num_coupling_layers)[::direction]:
            x_masked = x * self.masks[i]
            reversed_mask = 1 - self.masks[i]
            s, t = self.layers_list[i](x_masked)
            s *= reversed_mask
            t *= reversed_mask
            gate = (direction - 1) / 2
            x = (
                reversed_mask
                * (x * tf.exp(direction * s) + direction * t * tf.exp(gate * s))
                + x_masked
            )
            log_det_inv += gate * tf.reduce_sum(s, [1])

        return x, log_det_inv

    # Log likelihood of the normal distribution plus the log determinant of the jacobian.

    def log_loss(self, x):
        y, logdet = self(x)
        log_likelihood = self.distribution.log_prob(y) + logdet
        return -tf.reduce_mean(log_likelihood)

    def train_step(self, data):
        with tf.GradientTape() as tape:
            loss = self.log_loss(data)

        g = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(g, self.trainable_variables))
        self.loss_tracker.update_state(loss)

        return {"loss": self.loss_tracker.result()}

    def test_step(self, data):
        loss = self.log_loss(data)
        self.loss_tracker.update_state(loss)

        return {"loss": self.loss_tracker.result()}


"""
## Model training
"""

model = RealNVP(num_coupling_layers=6)

model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001))

history = model.fit(
    normalized_data_1, batch_size=500, epochs=100, verbose=2, validation_split=0.2
)

"""
## Performance evaluation
"""

# Questa porzione del codice genera un grafico dell'andamento della perdita (\textit{loss}) durante il processo di addestramento del modello, 
# per entrambi i set di dati di addestramento e di validazione.
plt.figure(figsize=(15, 10))   #
plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("model loss")
plt.legend(["train", "validation"], loc="upper right")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.show()
plt.close()

#Poco informativo con 200 componenti 

# From data to latent space.
z, _ = model(normalized_data_1)  #La variabile z rappresenta i dati trasformati nello spazio latente usando il modello. Lo spazio latente a la stessa dimensione di quello di partenza quindi 200 in questo caso.

print(f"Forma di z: {z.shape}") 

# From latent space to data.
x, _ = model.predict(normalized_data_2)  #Utilizzando model.predict(samples), i campioni dello spazio latente vengono trasformati indietro nello spazio dati originale, ottenendo x.

print(f"Forma di x: {x.shape}") 

fig, axes = plt.subplots(2, 2)
fig.set_size_inches(20, 15)

axes[0, 0].scatter(normalized_data_1[:, 0], normalized_data_1[:, 1], color="r")
axes[0, 0].set(title="Dati di training", xlabel="x", ylabel="y")
axes[0, 1].scatter(z[:, 0], z[:, 1], color="r")
axes[0, 1].set(title="Dati di training nel latent space", xlabel="x", ylabel="y")
axes[0, 1].set_xlim([-3.5, 4])
axes[0, 1].set_ylim([-4, 4])
axes[1, 0].scatter(normalized_data_2[:, 0], normalized_data_2[:, 1], color="g")
axes[1, 0].set(title="Nuovi dati", xlabel="x", ylabel="y")
axes[1, 1].scatter(x[:, 0], x[:, 1], color="g")
axes[1, 1].set(title="Dati nuovi nel latent space", label="x", ylabel="y")
axes[1, 1].set_xlim([-2, 2])
axes[1, 1].set_ylim([-2, 2])

plt.show()


#Test del modello ottenuto

# Calcolo del log-likelihood su dati di test/validazione
test_log_likelihood = -model.log_loss(normalized_data_2)
print(f"Test log-likelihood: {test_log_likelihood.numpy()}")

# Genera nuovi campioni dallo spazio latente
z_samples = model.distribution.sample(1000)  # Esempio di 1000 campioni latenti
generated_samples, _ = model.predict(z_samples)

# Confronta i nuovi campioni con i dati originali
plt.figure(figsize=(10, 6))
plt.scatter(generated_samples[:, 0], generated_samples[:, 1], label="Generated Samples", color='b', alpha=0.5)
plt.scatter(normalized_data_1[:, 0], normalized_data_1[:, 1], label="Training Data", color='r', alpha=0.5)
plt.legend()
plt.show()

from scipy.stats import wasserstein_distance

# Calcolo della distanza di Wasserstein tra due distribuzioni (ad esempio, prima e seconda dimensione)
wd = wasserstein_distance(normalized_data_1[:, 0], generated_samples[:, 0])
print(f"Wasserstein distance (dimension 1): {wd}")

#Latent Space Analysis
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# PCA per ridurre lo spazio latente a 2D
pca = PCA(n_components=2)
z_pca = pca.fit_transform(z)

plt.figure(figsize=(8, 6))
plt.scatter(z_pca[:, 0], z_pca[:, 1], label="Latent Space (PCA)", color='b', alpha=0.5)
plt.title("Latent space after PCA")
plt.show()
"""
Title: Density estimation using Real NVP
Authors: [Mandolini Giorgio Maria](https://www.linkedin.com/in/giorgio-maria-mandolini-a2a1b71b4/), [Sanna Daniele](https://www.linkedin.com/in/daniele-sanna-338629bb/), [Zannini Quirini Giorgio](https://www.linkedin.com/in/giorgio-zannini-quirini-16ab181a0/)
Date created: 2020/08/10
Last modified: 2020/08/10
Description: Estimating the density distribution of the "double moon" dataset.
Accelerator: GPU
"""

"""
## Introduction

The aim of this work is to map a simple distribution - which is easy to sample
and whose density is simple to estimate - to a more complex one learned from the data.
This kind of generative model is also known as "normalizing flow".

In order to do this, the model is trained via the maximum
likelihood principle, using the "change of variable" formula.

We will use an affine coupling function. We create it such that its inverse, as well as
the determinant of the Jacobian, are easy to obtain (more details in the referenced paper).

**Requirements:**

* Tensorflow 2.9.1
* Tensorflow probability 0.17.0

**Reference:**

[Density estimation using Real NVP](https://arxiv.org/abs/1605.08803)
"""

"""
## Setup

"""
import os

os.environ["KERAS_BACKEND"] = "tensorflow"

import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras import regularizers
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_probability as tfp
import pandas as pd




"""
## Load the data
"""

#DT_recordings dovrà avere dimensione (10000,200,8)

# Path alla directory dove sono salvati i file CSV
csv_directory = "../L_FRAME_DT/Dati/istantrain_7_1/"
 
# Lista dei nomi dei file CSV
file_names = ["U_concat_1.csv", "U_concat_2.csv", "U_concat_3.csv", 
              "U_concat_4.csv", "U_concat_5.csv", "U_concat_6.csv", 
              "U_concat_7.csv", "U_concat_8.csv"]

# Inizializza una lista per contenere i dati di ciascun sensore
sensori_data = []

# Carica ogni file CSV e aggiungi i dati alla lista
for file_name in file_names:
    file_path = os.path.join(csv_directory, file_name)
    sensor_data = pd.read_csv(file_path, header=None).values.flatten()  # Converte in array numpy
    sensori_data.append(sensor_data)

# Trasponi l'array in modo che le colonne siano i sensori e le righe le osservazioni
# Ogni colonna rappresenta un sensore, e ogni riga è una rilevazione.
dataset = np.stack(sensori_data, axis=1)

print(f"Forma di dataset: {dataset.shape}")

# Trova gli indici degli zeri che separano le osservazioni
split_indices = np.where(np.all(dataset == 0, axis=1))[0]

print(f"Forma di split_indices: {split_indices.shape}")

# Suddividi il dataset nelle osservazioni corrispondenti
observations = []
start_index = 0

for end_index in split_indices:
    if end_index > start_index:
        observation = dataset[start_index:end_index, :]
        if observation.shape[0] == 200:  # Assicurati che la lunghezza sia di 200 rilevazioni
            observations.append(observation)
    start_index = end_index + 1
#Con questo for manca l'ultima, ora la aggiungo
    observation = dataset[start_index:, :]
    if observation.shape[0] == 200:  
            observations.append(observation)

# Converti la lista in un array numpy con shape (10000, 200, 8)
observations = np.array(observations)

print(f"Forma di observations: {observations.shape}")

for i in range(2):  
    print(f"Forma della {i+1}° osservazione: {observations[i].shape}")


# Dividiamo il dataset in dataset_1 (prime 8000 osservazioni) e dataset_2 (ultime 2000 osservazioni)
data_1 = observations[:8000]
data_2 = observations[8000:]

# Stampa le forme dei due sottoinsiemi
print(f"Forma di dataset_1: {data_1.shape}")  # Dovrebbe essere (8000, 200, 8)
print(f"Forma di dataset_2: {data_2.shape}")  # Dovrebbe essere (2000, 200, 8)

# Normalizzazione sui due dataset
norm_1 = layers.Normalization()
norm_2 = layers.Normalization()

# Adattamento dei layer di normalizzazione sui rispettivi dataset    # Questa normalizzazione calcola la media e la deviazione standard in base al batch intero 
norm_1.adapt(data_1)                                                 # che gli viene fornito tramite adapt(). Normalizza i dati feature-wise, cioè per ogni feature 
norm_2.adapt(data_2)                                                 # (colonna o dimensione) nel dataset in base ai valori osservati durante l’adattamento.

# Normalizza i dataset
normalized_data_1 = norm_1(data_1)
normalized_data_2 = norm_2(data_2)

# Controllo delle forme
print(f"Forma di normalized_data_1: {normalized_data_1.shape}")  
print(f"Forma di normalized_data_2: {normalized_data_2.shape}")


"""
## Affine coupling layer
"""

# Creating a custom layer with keras API.
output_dim = 256  # Opzioni 256, 512, 1024
reg = 0.01


# def Coupling(input_shape):  # input_shape sarà (200, 8)
#     input = keras.layers.Input(shape=(input_shape,))

#     # La rete per la componente t (traslazione)
#     t_layer_1 = keras.layers.Dense(
#         output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
#     )(input)
#     t_layer_2 = keras.layers.Dense(
#         output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
#     )(t_layer_1)
#     t_layer_3 = keras.layers.Dense(
#         output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
#     )(t_layer_2)
#     t_layer_4 = keras.layers.Dense(
#         output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
#     )(t_layer_3)
#     t_layer_5 = keras.layers.Dense(
#         input_shape, activation="linear", kernel_regularizer=regularizers.l2(reg)
#     )(t_layer_4)

#     # La rete per la componente s (scalatura)
#     s_layer_1 = keras.layers.Dense(
#         output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
#     )(input)
#     s_layer_2 = keras.layers.Dense(
#         output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
#     )(s_layer_1)
#     s_layer_3 = keras.layers.Dense(
#         output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
#     )(s_layer_2)
#     s_layer_4 = keras.layers.Dense(
#         output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
#     )(s_layer_3)
#     s_layer_5 = keras.layers.Dense(
#         input_shape, activation="tanh", kernel_regularizer=regularizers.l2(reg)
#     )(s_layer_4)

#     return keras.Model(inputs=input, outputs=[s_layer_5, t_layer_5])
#     #Viene creato un modello Keras che prende l'input e produce due output: s_layer_5: la componente di scalatura e t_layer_5: la componente di traslazione.

filter_1      = 32;   filter_2      = 64;   filter_3      = 32
kernel_size_1 = 25;   kernel_size_2 =  13;   kernel_size_3 = 7
neurons_4 = 64
neurons_5 = 16
attivaz_conv = 'tanh'
attivaz_mlp = 'tanh'
k_reg = 1e-3
b_reg = 1e-3
rate_drop = 0.05


def Coupling(input_shape=(200, 8), reg=0.01):
    # Input con shape (200, 8), ovvero 200 time steps con 8 feature per step
    input  = layers.Input(shape=input_shape, name='Convolutional_inputs')

    # Blocchi convoluzionali per l'estrazione di caratteristiche temporali
    x = layers.Conv1D(filters=filter_1, kernel_size=kernel_size_1, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_1')(input)
    x = layers.MaxPooling1D()(x)
    # x = layers.Dropout(rate=rate_drop)(x)   #Gli strati Dropout vengono utilizzati per regolarizzare il modello durante l'addestramento, disattivando casualmente una frazione di neuroni. Questo aiuta a prevenire l'overfitting.
    x = layers.Conv1D(filters=filter_2, kernel_size=kernel_size_2, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_2')(x)
    x = layers.MaxPooling1D()(x)
    # x = layers.Dropout(rate=rate_drop)(x)
    x = layers.Conv1D(filters=filter_3, kernel_size=kernel_size_3, kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_3')(x)
    x = layers.MaxPooling1D()(x)
    # x = layers.Dropout(rate=rate_drop)(x)
    x = layers.Flatten()(x)

    # Livelli densi per la componente t (traslazione)
    t_layer_1 = keras.layers.Dense(128, activation="relu", kernel_regularizer=regularizers.l2(reg))(x)
    t_layer_2 = keras.layers.Dense(input_shape[0] * input_shape[1], activation="linear", kernel_regularizer=regularizers.l2(reg))(t_layer_1)
    t_layer_2_reshaped = keras.layers.Reshape(input_shape)(t_layer_2)

    # Livelli densi per la componente s (scalatura)
    s_layer_1 = keras.layers.Dense(128, activation="relu", kernel_regularizer=regularizers.l2(reg))(x)
    s_layer_2 = keras.layers.Dense(input_shape[0] * input_shape[1], activation="tanh", kernel_regularizer=regularizers.l2(reg))(s_layer_1)
    s_layer_2_reshaped = keras.layers.Reshape(input_shape)(s_layer_2)

    return keras.Model(inputs=input, outputs=[s_layer_2_reshaped, t_layer_2_reshaped])


"""
## Real NVP
"""


class RealNVP(keras.Model):
    def __init__(self, num_coupling_layers):
        super().__init__()

        self.num_coupling_layers = num_coupling_layers

        # Distribuzione dello spazio latente (200, 8)
        self.distribution = tfp.distributions.MultivariateNormalDiag(
            loc=[0.0] * 1600, scale_diag=[1.0] * 1600  # La distribuzione ha 1600 dimensioni (200 * 8)
        )

        # Maschere (alterniamo metà delle dimensioni su entrambe le direzioni)
        self.masks = tf.convert_to_tensor(
            [[0] * 800 + [1] * 800, [1] * 800 + [0] * 800] * (num_coupling_layers // 2), dtype=tf.float32
        )

        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.layers_list = [Coupling(input_shape=(200, 8)) for i in range(num_coupling_layers)]
    
    # Ogni strato di accoppiamento nella lista self.layers_list trasforma l'input utilizzando le reti di scalatura e traslazione (s_layers e t_layers). 
    # Queste trasformazioni sono invertibili e consentono di "mappare" i dati verso una distribuzione più semplice, mantenendo la possibilità di invertire 
    # le trasformazioni per ricostruire i dati originali.

    @property
    def metrics(self):
        """List of the model's metrics.

        We make sure the loss tracker is listed as part of `model.metrics`
        so that `fit()` and `evaluate()` are able to `reset()` the loss tracker
        at the start of each epoch and at the start of an `evaluate()` call.
        """
        return [self.loss_tracker]

    def call(self, x, training=True):
        log_det_inv = 0
        direction = 1
        if training:
            direction = -1
            
        for i in range(self.num_coupling_layers)[::direction]:
            # Reshape la maschera per corrispondere alla dimensione dell'input (None, 200, 8)
            mask = tf.reshape(self.masks[i], [1, 200, 8])  # Aggiungi una dimensione batch
            x_masked = x * mask  # Maschera l'input
            reversed_mask = 1 - mask
            s, t = self.layers_list[i](x_masked)
            s *= reversed_mask
            t *= reversed_mask
            gate = (direction - 1) / 2
            x = (
                reversed_mask
                * (x * tf.exp(direction * s) + direction * t * tf.exp(gate * s))
                + x_masked
            )
            log_det_inv += gate * tf.reduce_sum(s, axis=[1, 2])

        return x, log_det_inv

    # Log likelihood of the normal distribution plus the log determinant of the jacobian.

    def log_loss(self, x):
        y, logdet = self(x)

        # Rimodella y in (None, 1600) per farlo corrispondere alla dimensione della distribuzione
        y = tf.reshape(y, [-1, 1600])

        # Calcola la log-likelihood
        log_likelihood = self.distribution.log_prob(y) + logdet

        return -tf.reduce_mean(log_likelihood)

    def train_step(self, data):
        with tf.GradientTape() as tape:
            loss = self.log_loss(data)

        g = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(g, self.trainable_variables))
        self.loss_tracker.update_state(loss)

        return {"loss": self.loss_tracker.result()}

    def test_step(self, data):
        loss = self.log_loss(data)
        self.loss_tracker.update_state(loss)

        return {"loss": self.loss_tracker.result()}


"""
## Model training
"""

model = RealNVP(num_coupling_layers=6)

model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001))

history = model.fit(
    normalized_data_1, batch_size=32, epochs=100, verbose=2, validation_split=0.2
)

"""
## Performance evaluation
"""

# Questa porzione del codice genera un grafico dell'andamento della perdita (\textit{loss}) durante il processo di addestramento del modello, 
# per entrambi i set di dati di addestramento e di validazione.
plt.figure(figsize=(15, 10))   #
plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("model loss")
plt.legend(["train", "validation"], loc="upper right")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.show()
plt.close()

# #Poco informativo con 200 componenti 

# # From data to latent space.
# z, _ = model(normalized_data_1)  #La variabile z rappresenta i dati trasformati nello spazio latente usando il modello. Lo spazio latente a la stessa dimensione di quello di partenza quindi 200 in questo caso.

# print(f"Forma di z: {z.shape}") 

# # From latent space to data.
# x, _ = model.predict(normalized_data_2)  #Utilizzando model.predict(samples), i campioni dello spazio latente vengono trasformati indietro nello spazio dati originale, ottenendo x.

# print(f"Forma di x: {x.shape}") 

# fig, axes = plt.subplots(2, 2)
# fig.set_size_inches(20, 15)

# axes[0, 0].scatter(normalized_data_1[:, 0], normalized_data_1[:, 1], color="r")
# axes[0, 0].set(title="Dati di training", xlabel="x", ylabel="y")
# axes[0, 1].scatter(z[:, 0], z[:, 1], color="r")
# axes[0, 1].set(title="Dati di training nel latent space", xlabel="x", ylabel="y")
# axes[0, 1].set_xlim([-3.5, 4])
# axes[0, 1].set_ylim([-4, 4])
# axes[1, 0].scatter(normalized_data_2[:, 0], normalized_data_2[:, 1], color="g")
# axes[1, 0].set(title="Nuovi dati", xlabel="x", ylabel="y")
# axes[1, 1].scatter(x[:, 0], x[:, 1], color="g")
# axes[1, 1].set(title="Dati nuovi nel latent space", label="x", ylabel="y")
# axes[1, 1].set_xlim([-2, 2])
# axes[1, 1].set_ylim([-2, 2])

# plt.show()


#Test del modello ottenuto

# Calcolo del log-likelihood su dati di test/validazione
test_log_likelihood = -model.log_loss(normalized_data_2)
print(f"Test log-likelihood: {test_log_likelihood.numpy()}")

# # Genera nuovi campioni dallo spazio latente
# z_samples = model.distribution.sample(1000)  # Esempio di 1000 campioni latenti
# generated_samples, _ = model.predict(z_samples)

# # Confronta i nuovi campioni con i dati originali
# plt.figure(figsize=(10, 6))
# plt.scatter(generated_samples[:, 0], generated_samples[:, 1], label="Generated Samples", color='b', alpha=0.5)
# plt.scatter(normalized_data_1[:, 0], normalized_data_1[:, 1], label="Training Data", color='r', alpha=0.5)
# plt.legend()
# plt.show()

# from scipy.stats import wasserstein_distance

# # Calcolo della distanza di Wasserstein tra due distribuzioni (ad esempio, prima e seconda dimensione)
# wd = wasserstein_distance(normalized_data_1[:, 0], generated_samples[:, 0])
# print(f"Wasserstein distance (dimension 1): {wd}")

# #Latent Space Analysis
# from sklearn.decomposition import PCA
# from sklearn.manifold import TSNE

# # PCA per ridurre lo spazio latente a 2D
# pca = PCA(n_components=2)
# z_pca = pca.fit_transform(z)

# plt.figure(figsize=(8, 6))
# plt.scatter(z_pca[:, 0], z_pca[:, 1], label="Latent Space (PCA)", color='b', alpha=0.5)
# plt.title("Latent space after PCA")
# plt.show()
"""
Title: MCMC NVP 2
"""

import os

os.environ["KERAS_BACKEND"] = "tensorflow"

# Imposta la variabile d'ambiente
os.environ['OMP_NUM_THREADS'] = '1'

import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras import regularizers
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_probability as tfp
import pandas as pd
import pymc as pm

from keras import ops
from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_absolute_error
import seaborn as sns

import scipy.stats as st

import arviz as az



EXPECTED_SHAPE_X = [None, 4]  # La dimensione batch può variare, ma x ha sempre 4 elementi
EXPECTED_SHAPE_Y = [None, 7]  # La dimensione batch può variare, ma y ha sempre 7 elementi

import multiprocessing











# Specify the example you are dealing with
esempio = 'L_FRAME_DT'
# Training and Testing data
ID              = '7_1'
save_ID         = 'Classifier_total_7_1/'
path            = "../" + esempio + '/Dati/'
path_data_train   = path + 'istantrain_' + ID
path_data_test   = path + 'istantest_' + ID
# Saving model
path_save       = "models/"
# Prediction model
restore_ID= 'Classifier_total_7_1/'
path_restore = path + restore_ID

path_z = "output_VAE_for_MCMC"


# Which dof monitored
which_channels = [1,2,3,4,5,6,7,8]

n_channels = 8
seq_len = 200
# addedd_SNR = 100
# Specify if you are using accelerations =1 or displacements =0
accelerations = 0


latent_dim = 4




def RMS(signal):
    return np.sqrt(np.mean(signal**2))


# Read data and create dataset
def read_data(path_data, train):    
    label_path_class = path_data + '/Damage_class.csv'                                
    labels_class     = np.genfromtxt(label_path_class)
    labels_class     = labels_class.astype('int')
    label_path_level = path_data + '/Damage_level.csv'                                
    labels_level     = np.genfromtxt(label_path_level)
    
    # Filtra gli indici con damage_class diversa da 0
    valid_indices = np.where(labels_class != 0)[0]
    N_ist = len(valid_indices)  # Usa solo gli indici validi

    # Importiamo i valori generati dal VAE
    if train:
        path_data_z = os.path.join(path_z, "z_train2.csv")
    else:
        path_data_z = os.path.join(path_z, "z_test2.csv")
    
    Z = pd.read_csv(path_data_z, header=0)


    # Creazione dei vettori di label di 7 elementi
    labels_vector = np.zeros((N_ist, 7))  # Vettore di etichette a 7 classi
    
    for i, idx in enumerate(valid_indices):
        # Inserisci il damage_level nella posizione corretta in base alla classe di danno
        damage_class = labels_class[idx] - 1  # Mappiamo la classe 1 su 0, 2 su 1, ecc.
        damage_level = labels_level[idx]
        labels_vector[i][damage_class] = damage_level - 0.25

    return Z, labels_vector, N_ist


#---------------------------------------------------------------------------------------------------------------

"""
## Affine coupling layer
"""

# Creating a custom layer with keras API.
output_dim = 256  # Opzioni 256, 512, 1024
reg = 0.01

# Number of classes in the dataset
n_class = 7


# Hyperparameters
validation_split = 0.20
batch_size = 32
n_epochs = 250
early_stop_epochs=15
initial_lr = 1e-3
decay_length = 0.8
ratio_to_stop = 0.05

filter_1      = 32;   filter_2      = 64;   filter_3      = 32
kernel_size_1 = 25;   kernel_size_2 =  13;   kernel_size_3 = 7
neurons_4 = 64
neurons_5 = 16
attivaz_conv = 'tanh'
attivaz_mlp = 'tanh'
k_reg = 1e-3
b_reg = 1e-3
rate_drop = 0.05



# Struttura senza Dropout  
# def Coupling(input_shape=latent_dim, num_classes=n_class):
#     # Input per il vettore latente Z di dimensione 4
#     input_z = layers.Input(shape=(latent_dim,), name='Input_Z')
#     # Input per il vettore di label condizionale di dimensione num_classes
#     input_l = layers.Input(shape=(num_classes,), name='Input_Label')
    
#     # Network per condizionare s e t in funzione del label y
#     # Uso un piccolo MLP per una rappresentazione più complessa dell'embedding delle classi
#     label_condition = layers.Dense(32, activation='relu')(input_l)
#     label_condition = layers.Dense(64, activation='relu')(label_condition)
#     label_embedding = layers.Dense(128, activation='relu')(label_condition) 


#     # Livelli densi per la componente t (traslazione) con condizionamento MLP
#     t = layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(reg))(input_z)
#     t_embedding = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
#     t = layers.Concatenate()([t, t_embedding])  # Condizionamento con il label embedding
#     t = layers.Dense(latent_dim, activation="linear", kernel_regularizer=regularizers.l2(reg))(t)

#     # Livelli densi per la componente s (scalatura) con condizionamento MLP
#     s = layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(reg))(input_z)
#     s_embedding = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
#     s = layers.Concatenate()([s, s_embedding])  # Condizionamento con il label embedding
#     s = layers.Dense(latent_dim, activation="tanh", kernel_regularizer=regularizers.l2(reg))(s)

#     return keras.Model(inputs=[input_z, input_l], outputs=[s, t])


# def Coupling(input_shape=latent_dim, num_classes=n_class, reg=0.01, dropout_rate=0.3):
#     # Input per il vettore latente Z di dimensione 4
#     input_z = layers.Input(shape=(latent_dim,), name='Input_Z')
#     # Input per il vettore di label condizionale di dimensione num_classes
#     input_l = layers.Input(shape=(num_classes,), name='Input_Label')

#     # Network per condizionare s e t in funzione del label y
#     label_condition = layers.Dense(32, activation='relu')(input_l)
#     label_condition = layers.Dense(64, activation='relu')(label_condition)
#     label_embedding = layers.Dense(128, activation='relu')(label_condition) 

#     # Livelli densi per la componente t (traslazione) con condizionamento MLP
#     t = layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(reg))(input_z)
#     t = layers.Dropout(dropout_rate)(t)  # Aggiungi Dropout
#     t_embedding = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
#     t = layers.Concatenate()([t, t_embedding])  # Condizionamento con il label embedding
#     t = layers.Dense(latent_dim, activation="linear", kernel_regularizer=regularizers.l2(reg))(t)

#     # Livelli densi per la componente s (scalatura) con condizionamento MLP
#     s = layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(reg))(input_z)
#     s = layers.Dropout(dropout_rate)(s)  # Aggiungi Dropout
#     s_embedding = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
#     s = layers.Concatenate()([s, s_embedding])  # Condizionamento con il label embedding
#     s = layers.Dense(latent_dim, activation="tanh", kernel_regularizer=regularizers.l2(reg))(s)

#     return keras.Model(inputs=[input_z, input_l], outputs=[s, t])




def Coupling(input_shape=latent_dim, num_classes=n_class, reg=0.001, dropout_rate=0.1):
    # Input per il vettore latente Z di dimensione 4
    input_z = layers.Input(shape=(latent_dim,), name='Input_Z')
    # Input per il vettore di label condizionale di dimensione num_classes
    input_l = layers.Input(shape=(num_classes,), name='Input_Label')

    # Network per condizionare s e t in funzione del label y
    label_condition = layers.Dense(32, activation='relu')(input_l)
    label_condition = layers.Dense(64, activation='relu')(label_condition)
    label_embedding = layers.Dense(128, activation='relu')(label_condition) 

    # Livelli densi per la componente t (traslazione) con condizionamento MLP
    t = layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(reg))(input_z)
    t = layers.Dropout(dropout_rate)(t)  # Aggiungi Dropout
    t_embedding = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
    t = layers.Concatenate()([t, t_embedding])  # Condizionamento con il label embedding
    t = layers.Dense(32, activation="relu", kernel_regularizer=regularizers.l2(reg))(t)
    t = layers.Dense(latent_dim, activation="linear", kernel_regularizer=regularizers.l2(reg))(t)

    # Livelli densi per la componente s (scalatura) con condizionamento MLP
    s = layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(reg))(input_z)
    s = layers.Dropout(dropout_rate)(s)  # Aggiungi Dropout
    s_embedding = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
    s = layers.Concatenate()([s, s_embedding])  # Condizionamento con il label embedding
    s = layers.Dense(32, activation="relu", kernel_regularizer=regularizers.l2(reg))(s)
    s = layers.Dense(latent_dim, activation="tanh", kernel_regularizer=regularizers.l2(reg))(s)

    return keras.Model(inputs=[input_z, input_l], outputs=[s, t])





    

"""
## Real NVP
"""

class RealNVP(keras.Model):
    def __init__(self, num_coupling_layers, num_classes):
        super().__init__()

        self.num_coupling_layers = num_coupling_layers
        self.num_classes = num_classes

        # Distribuzione dello spazio latente di dim 4
        self.distribution = tfp.distributions.MultivariateNormalDiag(
            loc=[0.0] * 4, scale_diag=[1.0] * 4  # La distribuzione ha 4 dimensioni
        )

        # Maschere (alterniamo metà delle dimensioni: 2 su 4)
        self.masks = tf.convert_to_tensor(
            [[0] * 2 + [1] * 2, [1] * 2 + [0] * 2] * (self.num_coupling_layers // 2), dtype=tf.float32
        )


        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.log_likelihood_tracker = keras.metrics.Mean(name="log_likelihood")
        self.log_det_inv_tracker = keras.metrics.Mean(name="log_det_inv")

        self.layers_list = [Coupling(input_shape=latent_dim, num_classes=n_class) for i in range(num_coupling_layers)]
    
    @property
    def metrics(self):
        return [self.loss_tracker,
                self.log_likelihood_tracker,
                self.log_det_inv_tracker]


    # call con struttura iniziale 
    def call(self, x, y, training=True):  

        log_det_inv = 0   

        for i in range(self.num_coupling_layers):

            mask = tf.reshape(self.masks[i], [1, 4])  # Aggiungi una dimensione batch
            x_masked = x * mask  # Maschera l'input
            reversed_mask = 1 - mask
            s, t = self.layers_list[i]([x_masked, y])
            s *= reversed_mask
            t *= reversed_mask

            x = (
                reversed_mask * (x * tf.exp(s) +  t )
                + x_masked
            )
            log_det_inv += tf.reduce_sum(s, axis=1)

        return x, log_det_inv
    

    def compute_loss(self, x, y):

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y)

        # Calcola la log-likelihood condizionata
        log_likelihood_z = self.distribution.log_prob(z_pred) 

        return tf.reduce_mean(log_likelihood_z), tf.reduce_mean(log_det_inv)


    

    def train_step(self, data):
        x, y = data  # Estraiamo i dati e le etichette
        with tf.GradientTape() as tape:
            log_likelihood_z, log_det_inv = self.compute_loss(x, y)

            # Total loss is the sum of log-likelihood and the log-det term
            loss = -(log_likelihood_z + log_det_inv)

        # Calcola i gradienti
        grads = tape.gradient(loss, self.trainable_variables)
        # Applica i gradienti
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        # Aggiorna lo stato del tracker della perdita
        self.loss_tracker.update_state(loss)
        # Aggiorna i tracker delle componenti
        self.log_likelihood_tracker.update_state(log_likelihood_z)
        self.log_det_inv_tracker.update_state(log_det_inv)

        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }



    def test_step(self, data):
        x, y = data
        log_likelihood_z, log_det_inv = self.compute_loss(x, y)
        loss = -(log_likelihood_z + log_det_inv)

        self.loss_tracker.update_state(loss)
        self.log_likelihood_tracker.update_state(log_likelihood_z)
        self.log_det_inv_tracker.update_state(log_det_inv)
        
        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }


    def predict(self, x, y):
        # Assicurati che x e y siano TensorFlow tensors
        x = tf.convert_to_tensor(x, dtype=tf.float32)
        y = tf.convert_to_tensor(y, dtype=tf.float32)

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y, training=False)

        # Calcola la log-likelihood condizionata
        log_likelihood_z = self.distribution.log_prob(z_pred)

        log_likelihood = log_likelihood_z + log_det_inv

        return 8*log_likelihood


    @tf.function(jit_compile=True)  # Abilita XLA per la compilazione JIT
    def predict_per_uno(self, x, y):
        # # Evita la conversione ridondante se x e y sono già tensori
        # if not isinstance(x, tf.Tensor):
        #     x = tf.convert_to_tensor(x, dtype=tf.float32)
        #     x.set_shape([EXPECTED_SHAPE_X])  # Specifica la forma statica di x
        # if not isinstance(y, tf.Tensor):
        #     y = tf.convert_to_tensor(y, dtype=tf.float32)
        #     y.set_shape([EXPECTED_SHAPE_Y])  # Specifica la forma statica di y

        x = tf.convert_to_tensor(x, dtype=tf.float64)
        y = tf.convert_to_tensor(y, dtype=tf.float64)

        # Disabilita la tracciatura del gradiente per migliorare l'efficienza
        with tf.GradientTape(persistent=False, watch_accessed_variables=False) as tape:
            tape.stop_recording()  # Disabilita il calcolo dei gradienti

            # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
            z_pred, log_det_inv = self(x, y, training=False)

            # Calcola la log-likelihood condizionata
            log_likelihood_z = self.distribution.log_prob(z_pred)
            log_likelihood = log_likelihood_z + log_det_inv

        # Restituisci il risultato scalato come richiesto
        return 8 * log_likelihood


#---------------------------------------------------------------------------------------------------------------------------------

# Creazione dell'istanza RealNVP
NVP = RealNVP(num_coupling_layers=10, num_classes=n_class)

# Caricamento dei pesi salvati
NVP.load_weights('./models/NVP_for_MCMC.weights.h5')





import pytensor.tensor as pt
from pytensor.graph.op import Op
from pytensor.graph.basic import Apply
import time

# Definisci l'Op personalizzata che esegue la chiamata al modello RealNVP con TensorFlow
class RealNVPTF(Op):
    def __init__(self, nvp_model):
        self.nvp_model = nvp_model  # Passa il modello NVP come attributo

    #make_node consente di definire come i dati entrano ed escono dal grafo computazionale e garantendo la compatibilità con i requisiti di PyMC.
    def make_node(self, data, label_vector):   
        # Forza il tipo di dati float64 per assicurarsi che PyMC riceva il tipo corretto
        data = pt.as_tensor_variable(data.astype(np.float64))
        label_vector = pt.as_tensor_variable(label_vector.astype(np.float64))
        return Apply(self, [data, label_vector], [pt.dscalar()])  # Restituisci uno scalare come output

    def perform(self, node, inputs, outputs):
        # Ricevi i numpy array e forza float64 per ogni operazione
        data, label_vector = inputs

        # Imposta a 0 i valori tra 0 e 0.05 nel vettore delle label
        label_vector[label_vector < 0.05] = 0.0

        # Assicurati che anche l'input al modello NVP sia in formato float64
        label_vector_eval = np.expand_dims(label_vector, axis=0).astype(np.float64)  # (batch_size, n_class)

        # Esegui il modello NVP e ottieni la log-likelihood in float64
        # log_prob = self.nvp_model.predict(data.astype(np.float64), label_vector_eval.astype(np.float64))
        log_prob = self.nvp_model.predict_per_uno(data.astype(np.float64), label_vector_eval.astype(np.float64))

        # Assegna l'output con la giusta precisione
        outputs[0][0] = np.array(log_prob, dtype=np.float64)  # Forza l'output come float64


# Crea un'istanza dell'Op personalizzata con il modello NVP
real_nvp_tf_op = RealNVPTF(NVP)

# Definisci la funzione di log-likelihood che userai in PyMC
def realnvp_logp(data, label_vector):
    # Converte i dati e label_vector in float64 prima di passare all'Op personalizzato
    return real_nvp_tf_op(data.astype(np.float64), label_vector.astype(np.float64))


# Funzione che esegue l'inferenza MCMC usando Metropolis-Hastings
def mcmc_classification_pymc(z_mean, z_log_var, num_iterations):
    with pm.Model() as model:
        # Definiamo una distribuzione a priori per il vettore di label
        # label_vector = pm.Normal("label_vector", mu=0, sigma=1, shape=(n_class,))
        label_vector = pm.Uniform('label_vector', lower=0, upper=0.55, shape=(n_class,))
        # label_vector = pm.Beta('label_vector', alpha=2, beta=5, shape=(n_class,))
        # label_vector = pm.Exponential('label_vector', lam=1/0.7, shape=(n_class,))

        # Calcola la deviazione standard dalla log-varianza
        sigma = pm.math.exp(0.5 * z_log_var)  # Deviance standard
        # Genera i nuovi dati campionando dalla distribuzione normale
        sampled_data = pm.Normal('sampled_data', mu=z_mean, sigma=sigma, shape=(len(z_mean),))

        # Usa pm.Potential per integrare la log-likelihood calcolata esternamente
        loglikelihood = pm.Potential("loglikelihood", realnvp_logp(sampled_data, label_vector))

        # Eseguiamo l'inferenza MCMC usandoset Metropolis-Hastings
        # step = pm.Metropolis()  # Usa Metropolis-Hastings come algoritmo di campionamento
        # Usare DE-Metropolis come algoritmo di campionamento
        # step = pm.DEMetropolis()
        step = pm.DEMetropolisZ(scaling=0.0003, tune_interval=80)
        trace = pm.sample(num_iterations, cores=1, chains=2, step=step, tune=10000 , return_inferencedata=True)


        # Riepilogo del campionamento
        summary = az.summary(trace)

        # Mostra la sintesi
        print(summary)

    # Restituisci i risultati del vettore continuo campionato
    return trace




#---------------------------------------------------------------------------------------------------------------------------------






#Importo i dati 
Z_test, labels, N_ist = read_data(path_data_test, 0)

print(f"Forma di Z_test: {Z_test.shape}")

print(f"Forma di labels: {labels.shape}")

print("Prime 2 righe di labels:")
print(labels[:2])




# Funzione per stimare la moda utilizzando gli istogrammi
def estimate_mode_from_histogram(samples, bins=50):
    # Calcola la moda per ogni feature
    mode_per_feature = np.zeros(samples.shape[0])
    
    for i in range(samples.shape[0]):
        hist, bin_edges = np.histogram(samples[i, :], bins=bins)
        mode_bin_index = np.argmax(hist)  # Indice del bin con la frequenza massima
        mode_per_feature[i] = 0.5 * (bin_edges[mode_bin_index] + bin_edges[mode_bin_index + 1])  # Media del bin
    return mode_per_feature


# Funzione per calcolare la deviazione standard dei campioni
def calculate_std(samples):
    return np.std(samples, axis=-1)


import csv

# Funzione che salva i risultati in un file CSV
# def append_results_to_csv(file_name, idx, true_label, label_estimate_mean, label_estimate_mode):
#     # Se il file non esiste, crealo e aggiungi l'intestazione
#     file_exists = os.path.isfile(file_name)

#     with open(file_name, mode='a', newline='') as file:
#         writer = csv.writer(file)
        
#         # Aggiungi intestazione solo se il file non esiste
#         if not file_exists:
#             writer.writerow(['Index', 'True Label', 'Mean Label Estimate', 'Mode Label Estimate'])
        
#         # Converti i vettori in stringhe separate da virgole
#         true_label_str = ','.join(map(str, true_label))
#         mean_label_str = ','.join(map(str, label_estimate_mean))
#         mode_label_str = ','.join(map(str, label_estimate_mode))
        
#         # Scrivi la riga completa nel CSV
#         writer.writerow([idx, true_label_str, mean_label_str, mode_label_str])


# Funzione che salva i risultati in un file CSV
def append_results_to_csv(file_name, idx, true_label, samples_mcmc):
    # Se il file non esiste, crealo e aggiungi l'intestazione
    file_exists = os.path.isfile(file_name)

    with open(file_name, mode='a', newline='') as file:
        writer = csv.writer(file)
        
        # Aggiungi intestazione solo se il file non esiste
        if not file_exists:
            writer.writerow(['Index', 'True Label', 'Mean Estimate', 'Mode Estimate', 'Standard Deviation', 'HPDI Lower', 'HPDI Upper'])

        # Calcola le statistiche dai campioni
        label_estimate_mean = np.mean(samples_mcmc, axis=-1)
        label_estimate_mode = estimate_mode_from_histogram(samples_mcmc)
        label_std_dev = calculate_std(samples_mcmc)
        # Trasponi l'array per avere la forma (2800, 7)
        samples_mcmc = samples_mcmc.T
        hpdi = calculate_hpdi(samples_mcmc)

        # # Calcola le metriche di performance
        # metrics = calculate_performance_metrics(true_label, label_estimate_mean, label_estimate_mode)
        
        # Converti i vettori in stringhe separate da virgole
        true_label_str = ','.join(map(str, true_label))
        mean_label_str = ','.join(map(str, label_estimate_mean))
        mode_label_str = ','.join(map(str, label_estimate_mode))
        std_dev_str = ','.join(map(str, label_std_dev))
        hpdi_lower_str = ','.join(map(str, hpdi[0, :]))
        hpdi_upper_str = ','.join(map(str, hpdi[1, :]))

        # Scrivi la riga completa nel CSV
        writer.writerow([idx, true_label_str, mean_label_str, mode_label_str, std_dev_str,
                         hpdi_lower_str, hpdi_upper_str])



# def append_results_to_csv(file_name, idx, true_label, label_estimate_mean, label_estimate_mode, standard_deviation, metrics):
#     # Se il file non esiste, crealo e aggiungi l'intestazione
#     file_exists = os.path.isfile(file_name)

#     with open(file_name, mode='a', newline='') as file:
#         writer = csv.writer(file)

#         # Aggiungi intestazione solo se il file non esiste
#         if not file_exists:
#             writer.writerow(['Index', 'True Label', 'Mean Estimate', 'Mode Estimate', 'Standard Deviation'
#                              'MAE Mean', 'MSE Mean', 'RMSE Mean', 'R² Mean', 'SMAPE Mean',
#                              'NRMSE Mean', 'Cosine Sim. Mean', 'Explained Variance Mean',
#                              'MAE Mode', 'MSE Mode', 'RMSE Mode', 'R² Mode', 'SMAPE Mode',
#                              'NRMSE Mode', 'Cosine Sim. Mode', 'Explained Variance Mode'])

#         # Scrivi la riga completa nel CSV con i risultati e le metriche
#         writer.writerow([idx, 
#                          ','.join(map(str, true_label)), 
#                          ','.join(map(str, label_estimate_mean)), 
#                          ','.join(map(str, label_estimate_mode)),
#                          ','.join(map(str, standard_deviation)),
#                          metrics['mae_mean'], metrics['mse_mean'], metrics['rmse_mean'], 
#                          metrics['r_squared_mean'], metrics['smape_mean'], metrics['nrmse_mean'], 
#                          metrics['cosine_sim_mean'], metrics['explained_variance_mean'],
#                          metrics['mae_mode'], metrics['mse_mode'], metrics['rmse_mode'], 
#                          metrics['r_squared_mode'], metrics['smape_mode'], metrics['nrmse_mode'], 
#                          metrics['cosine_sim_mode'], metrics['explained_variance_mode']])

# def update_performance_metrics_from_file(input_csv, output_csv):
#     # Leggi i dati dal file di input
#     df = pd.read_csv(input_csv)

#     # Per ogni riga del CSV
#     for idx, row in df.iterrows():
#         # Estrai i vettori di True Label, Mean Estimate e Mode Estimate
#         true_label = np.array([float(x) for x in row['True Label'].split(',')])
#         label_estimate_mean = np.array([float(x) for x in row['Mean Estimate'].split(',')])
#         label_estimate_mode = np.array([float(x) for x in row['Mode Estimate'].split(',')])
#         standard_deviation = np.array([float(x) for x in row['Standard Deviation'].split(',')])

#         # Calcola le metriche di performance
#         metrics = calculate_performance_metrics(true_label, label_estimate_mean, label_estimate_mode)

#         # Appendi i risultati aggiornati nel file di output
#         append_results_to_csv(output_csv, idx, true_label, label_estimate_mean, label_estimate_mode, standard_deviation, metrics)










# Funzione che calcola la classe predetta in base al valore massimo del vettore stimato
def get_predicted_class(estimated_label):
    # Trova l'indice con il valore massimo (classe predetta)
    return np.argmax(estimated_label)

# Funzione per creare confusion matrix e metriche
def create_confusion_matrix(csv_file):
    # Leggi il file CSV
    df = pd.read_csv(csv_file)
    
    # Converti le colonne 'True Label', 'Mean Label Estimate' e 'Mode Label Estimate' da stringhe a array di float
    df['True Label'] = df['True Label'].apply(lambda x: np.array(list(map(float, x.split(',')))))
    df['Mean Estimate'] = df['Mean Estimate'].apply(lambda x: np.array(list(map(float, x.split(',')))))
    df['Mode Estimate'] = df['Mode Estimate'].apply(lambda x: np.array(list(map(float, x.split(',')))))

    # Ottieni le classi vere e quelle stimate (media e moda)
    true_classes = df['True Label'].apply(get_predicted_class).values +1 
    mean_predicted_classes = df['Mean Estimate'].apply(get_predicted_class).values +1
    mode_predicted_classes = df['Mode Estimate'].apply(get_predicted_class).values +1

    # Calcola metriche per predizioni con media
    accuracy_mean = accuracy_score(true_classes, mean_predicted_classes)
    precision_mean = precision_score(true_classes, mean_predicted_classes, average='weighted')
    recall_mean = recall_score(true_classes, mean_predicted_classes, average='weighted')
    f1_mean = f1_score(true_classes, mean_predicted_classes, average='weighted')
    conf_matrix_mean = confusion_matrix(true_classes, mean_predicted_classes)

    # Calcola metriche per predizioni con moda
    accuracy_mode = accuracy_score(true_classes, mode_predicted_classes)
    precision_mode = precision_score(true_classes, mode_predicted_classes, average='weighted')
    recall_mode = recall_score(true_classes, mode_predicted_classes, average='weighted')
    f1_mode = f1_score(true_classes, mode_predicted_classes, average='weighted')
    conf_matrix_mode = confusion_matrix(true_classes, mode_predicted_classes)

    # Crea una figura con due sotto-figure
    fig, axs = plt.subplots(2, 2, figsize=(16, 12))

    # Plot della matrice di confusione per la media
    sns.heatmap(conf_matrix_mean, annot=True, fmt='d', cmap='Blues', ax=axs[0, 0])
    axs[0, 0].set_title('Confusion Matrix (Mean)')
    axs[0, 0].set_xlabel('Predicted Label')
    axs[0, 0].set_ylabel('True Label')

    # Aggiungi le metriche di classificazione per la media
    metrics_mean = (
        f"Accuracy: {accuracy_mean:.4f}\n"
        f"Precision: {precision_mean:.4f}\n"
        f"Recall: {recall_mean:.4f}\n"
        f"F1-Score: {f1_mean:.4f}"
    )
    axs[1, 0].text(0.5, 0.5, metrics_mean, fontsize=12, ha='center', va='center')
    axs[1, 0].set_axis_off()

    # Plot della matrice di confusione per la moda
    sns.heatmap(conf_matrix_mode, annot=True, fmt='d', cmap='Blues', ax=axs[0, 1])
    axs[0, 1].set_title('Confusion Matrix (Mode)')
    axs[0, 1].set_xlabel('Predicted Label')
    axs[0, 1].set_ylabel('True Label')

    # Aggiungi le metriche di classificazione per la moda
    metrics_mode = (
        f"Accuracy: {accuracy_mode:.4f}\n"
        f"Precision: {precision_mode:.4f}\n"
        f"Recall: {recall_mode:.4f}\n"
        f"F1-Score: {f1_mode:.4f}"
    )
    axs[1, 1].text(0.5, 0.5, metrics_mode, fontsize=12, ha='center', va='center')
    axs[1, 1].set_axis_off()

    # Aggiungi spazi tra le figure
    plt.tight_layout()
    plt.show()



def analyze_damage_levels(csv_file_name):
    # Carica i dati dal file CSV
    df = pd.read_csv(csv_file_name)
    
    # Estrai le colonne necessarie
    true_labels = df['True Label'].apply(lambda x: np.fromstring(x, sep=',')).to_list()
    mean_labels = df['Mean Label Estimate'].apply(lambda x: np.fromstring(x, sep=',')).to_list()
    mode_labels = df['Mode Label Estimate'].apply(lambda x: np.fromstring(x, sep=',')).to_list()

    # Converti in array NumPy per un'elaborazione più semplice
    true_labels = np.array(true_labels)
    mean_labels = np.array(mean_labels)
    mode_labels = np.array(mode_labels)

    # Funzione per estrarre il livello di danno dalla classe giusta
    def get_damage_level(label):
        # Trova l'indice dove c'è il livello di danno non nullo (classe con danno massimo)
        damage_index = np.argmax(label)
        return label[damage_index], damage_index  # Restituisci il livello di danno e l'indice della classe

    # Funzione per trovare la seconda classe con il livello di danno più alto
    def get_second_highest_class(label):
        sorted_indices = np.argsort(label)  # Ordina gli indici in base ai valori crescenti
        second_highest_index = sorted_indices[-2]  # Il secondo più alto è il penultimo nell'array ordinato
        return second_highest_index

    # Analizza i livelli di danno
    true_damage_levels = np.array([get_damage_level(label)[0] for label in true_labels])  # Ottieni i livelli di danno reali
    true_classes = np.array([get_damage_level(label)[1] for label in true_labels])  # Ottieni le classi reali

    predicted_damage_levels_mean = np.array([mean_labels[i][true_classes[i]] for i in range(len(mean_labels))])
    predicted_damage_levels_mode = np.array([mode_labels[i][true_classes[i]] for i in range(len(mode_labels))])

    # Calcola MAE (Errore Assoluto Medio)
    mae_mean = np.mean(np.abs(predicted_damage_levels_mean - true_damage_levels))
    mae_mode = np.mean(np.abs(predicted_damage_levels_mode - true_damage_levels))

    # Calcola l'errore relativo rispetto al livello di danno reale (variazione proporzionale)
    relative_error_mean = np.mean(np.abs(predicted_damage_levels_mean - true_damage_levels) / (true_damage_levels+0.25))
    relative_error_mode = np.mean(np.abs(predicted_damage_levels_mode - true_damage_levels) / (true_damage_levels+0.25))

    # Calcola la distanza tra la classe con il danno massimo e la seconda classe più alta
    distances = []
    for label in true_labels:
        damage_index = get_damage_level(label)[1]
        second_highest_index = get_second_highest_class(label)
        distance = abs(damage_index - second_highest_index)  # Distanza tra la prima e la seconda classe
        distances.append(distance)
    
    mean_distance = np.mean(distances)  # Media delle distanze

    # Stampa dei risultati
    print(f"Errore Assoluto Medio (Media): {mae_mean:.4f}")
    print(f"Errore Assoluto Medio (Moda): {mae_mode:.4f}")
    print(f"Errore Relativo Medio (Media): {relative_error_mean:.4f}")
    print(f"Errore Relativo Medio (Moda): {relative_error_mode:.4f}")
    print(f"Media della distanza tra prima e seconda classe più alta: {mean_distance:.4f}")




def calculate_performance_metrics(true_label, label_estimate_mean, label_estimate_mode):

    # Metti a 0 se i valori nei vettori label_estimate_mean e label_estimate_mode sono tra 0 e 0.05
    label_estimate_mean = np.where((label_estimate_mean > 0) & (label_estimate_mean < 0.05), 0, label_estimate_mean)
    label_estimate_mode = np.where((label_estimate_mode > 0) & (label_estimate_mode < 0.05), 0, label_estimate_mode)

    # MAE e MSE
    mae_mean = np.mean(np.abs(label_estimate_mean - true_label))
    mse_mean = np.mean((label_estimate_mean - true_label) ** 2)
    mae_mode = np.mean(np.abs(label_estimate_mode - true_label))
    mse_mode = np.mean((label_estimate_mode - true_label) ** 2)

    # R²
    ss_total = np.sum((true_label - np.mean(true_label)) ** 2)
    ss_residual_mean = np.sum((true_label - label_estimate_mean) ** 2)
    ss_residual_mode = np.sum((true_label - label_estimate_mode) ** 2)
    r_squared_mean = 1 - (ss_residual_mean / ss_total)
    r_squared_mode = 1 - (ss_residual_mode / ss_total)

    # RMSE
    rmse_mean = np.sqrt(mse_mean)
    rmse_mode = np.sqrt(mse_mode)

    # SMAPE
    smape_mean = 100 * np.mean(2 * np.abs(label_estimate_mean - true_label) / (np.abs(label_estimate_mean) + np.abs(true_label)))
    smape_mode = 100 * np.mean(2 * np.abs(label_estimate_mode - true_label) / (np.abs(label_estimate_mode) + np.abs(true_label)))

    # NRMSE
    nrmse_mean = rmse_mean / (np.max(true_label) - np.min(true_label))
    nrmse_mode = rmse_mode / (np.max(true_label) - np.min(true_label))

    # Cosine Similarity
    def cosine_similarity(y_true, y_pred):
        dot_product = np.sum(y_true * y_pred)
        norm_true = np.linalg.norm(y_true)
        norm_pred = np.linalg.norm(y_pred)
        return dot_product / (norm_true * norm_pred)
    
    cosine_sim_mean = cosine_similarity(true_label, label_estimate_mean)
    cosine_sim_mode = cosine_similarity(true_label, label_estimate_mode)

    # Explained Variance
    def explained_variance(y_true, y_pred):
        var_residual = np.var(y_true - y_pred)
        var_true = np.var(y_true)
        return 1 - (var_residual / var_true)

    explained_variance_mean = explained_variance(true_label, label_estimate_mean)
    explained_variance_mode = explained_variance(true_label, label_estimate_mode)

    # Crea un dizionario con tutti i risultati
    metrics = {
        "mae_mean": mae_mean,
        "mse_mean": mse_mean,
        "rmse_mean": rmse_mean,
        "r_squared_mean": r_squared_mean,
        "smape_mean": smape_mean,
        "nrmse_mean": nrmse_mean,
        "cosine_sim_mean": cosine_sim_mean,
        "explained_variance_mean": explained_variance_mean,
        "mae_mode": mae_mode,
        "mse_mode": mse_mode,
        "rmse_mode": rmse_mode,
        "r_squared_mode": r_squared_mode,
        "smape_mode": smape_mode,
        "nrmse_mode": nrmse_mode,
        "cosine_sim_mode": cosine_sim_mode,
        "explained_variance_mode": explained_variance_mode,
    }

    return metrics



def calculate_hpdi(samples, alpha=0.05):

    # Numero di campioni
    n_samples = samples.shape[0]
    
    # Percentuale da includere nell'HPDI (es. 95% -> include 95% dei campioni)
    credible_mass = 1.0 - alpha
    
    # HPDI per ciascuna dimensione
    hpdi = np.zeros((2, samples.shape[1]))
    
    # Calcola l'HPDI per ogni dimensione del vettore di label
    for dim in range(samples.shape[1]):
        # Ordina i campioni per quella dimensione
        sorted_samples = np.sort(samples[:, dim])
        
        # Numero di campioni da includere nell'HPDI
        interval_idx_inc = int(np.floor(credible_mass * n_samples))
        
        # Trova l'ampiezza minima dell'intervallo
        interval_width = sorted_samples[interval_idx_inc:] - sorted_samples[:n_samples - interval_idx_inc]
        
        # Trova l'indice dell'intervallo più stretto
        min_idx = np.argmin(interval_width)
        
        # Assegna i limiti inferiore e superiore dell'HPDI
        hpdi[0, dim] = sorted_samples[min_idx]
        hpdi[1, dim] = sorted_samples[min_idx + interval_idx_inc]
    
    return hpdi






def calcola_media_parametri(csv_file_name):
    # Carica il file CSV in un DataFrame
    df = pd.read_csv(csv_file_name)
    
    def converti_vettore(valore):
        """Converte una stringa in un vettore (lista di float), se possibile."""
        try:
            # Rimuove gli spazi e converte la stringa in lista di float
            return [float(x) for x in valore.split(',')]
        except:
            # Se non è un vettore (stringa singola), ritorna il valore originale
            return valore
    
    # Applica la conversione dei vettori per tutte le colonne
    df = df.applymap(lambda x: converti_vettore(x) if isinstance(x, str) else x)
    
    # Identifica le colonne da escludere
    colonne_da_escludere = ['Index', 'True Label', 'Mean Estimate', 'Mode Estimate']
    
    # Filtra le colonne del DataFrame per escludere quelle specificate
    df_filtrato = df.drop(columns=colonne_da_escludere, errors='ignore')

    # Identifica le colonne che contengono vettori e non valori singoli
    colonne_vettoriali = df_filtrato.apply(lambda col: col.apply(lambda x: isinstance(x, list)).all())
    
    # Calcola la media dei vettori
    medie = {}
    for colonna, is_vettoriale in colonne_vettoriali.items():
        if is_vettoriale:
            # Se è una colonna di vettori, convertila in array numpy
            array_vettori = np.array(df_filtrato[colonna].tolist())
            # Calcola la media elemento per elemento
            medie[colonna] = np.mean(array_vettori, axis=0)
        else:
            # Se è una colonna di valori singoli, calcola la media normalmente
            medie[colonna] = df_filtrato[colonna].mean()

    # Convertire le medie in un DataFrame per una migliore visualizzazione
    df_medie = pd.DataFrame(medie.items(), columns=['Parametro', 'Media'])
    
    # Formattare i risultati per una visualizzazione più bella
    df_medie['Media'] = df_medie['Media'].apply(
        lambda x: f"{x:.4f}" if isinstance(x, float) else ', '.join(f"{v:.4f}" for v in x)
    )

    # Stampare i risultati
    print("\nRisultati delle Medie:")
    print(df_medie.to_string(index=False))

    return df_medie





















"""
## Main
"""

# Indice casuale per selezionare un campione dal dataset Z
idx = np.random.randint(0, Z_test.shape[0]) 

# Estrai le colonne di media e log-varianza per il campione selezionato
z_mean = Z_test[['mean_0', 'mean_1', 'mean_2', 'mean_3']].to_numpy()  # Estrai le colonne delle medie
z_log_var = Z_test[['var_0', 'var_1', 'var_2', 'var_3']].to_numpy()  # Estrai le colonne delle log-varianze

# Definisci il numero di iterazioni per MCMC
n_iter = 20000  

# Esegui la classificazione con PyMC usando il vettore continuo
# Assicurati di passare la riga completa di medie e varianze per il campione selezionato
# trace = mcmc_classification_pymc(z_mean[idx], z_log_var[idx], n_iter)


# # Questo ti restituirà un array con dimensioni (n_samples_totali, 7), dove n_samples_totali = n_chains * n_draws (n_iter, i tunes sono scartati)
# accepted_labels = trace.posterior["label_vector"].stack(samples=("chain", "draw")).values



# # Analizza i risultati: Calcola la media dei campioni per ottenere il label continuo stimato
# label_estimate_mean = np.mean(accepted_labels, axis=-1)
# print(f"Label stimato con media: {label_estimate_mean}")
# print(f"Forma di label_estimate_mean: {label_estimate_mean.shape}")

# # Calcola la moda basata sull'istogramma per ciascun componente del label continuo
# label_estimate_mode = estimate_mode_from_histogram(accepted_labels, bins=50)
# print(f"Label stimato con moda (basata su istogramma): {label_estimate_mode}")
# print(f"Forma di label_estimate_mode: {label_estimate_mode.shape}")

# # Ottieni il vero label per l'indice scelto
# true_label = labels[idx]
# print(f"Vero label: {true_label}")



# # Visualizzazione dei risultati con istogrammi
# num_labels = true_label.shape[0]
# plt.figure(figsize=(15, 10))


# for i in range(num_labels):
#     plt.subplot(2, 4, i + 1)  # Crea una griglia di 2 righe e 4 colonne
#     plt.hist(accepted_labels[i, :], bins=50, alpha=0.7, color='blue', edgecolor='black', range=(0,0.55))
#     plt.title(f'Componente {i + 1}\nVero label: {true_label[i]:.2f}')
#     plt.xlabel('Valore')
#     plt.ylabel('Frequenza')


# output_dir = "output_MCMC_NVP_2"
# if not os.path.exists(output_dir):
#     os.makedirs(output_dir)

# # Aggiungi un layout e salvataggio
# plt.tight_layout()
# plt.savefig(os.path.join(output_dir, f'accepted_labels_histogram_idx_{idx}.png'))
# plt.show()





"""
## Creazione .csv
"""

n_iter = 20000 

# Nome del file CSV dove salvare i risultati
csv_file_name = 'mcmc_results_sampled.csv'

# Numero di iterazioni
num_iterations = 30

for i in range(num_iterations):

    # Stampa l'iterazione corrente
    print(f"Inizio iterazione {i + 1} di {num_iterations}")

    # Seleziona un indice casuale dal dataset Z_test
    idx = np.random.randint(0, Z_test.shape[0])
    
    # Estrai le colonne di media e log-varianza per il campione selezionato
    z_mean = Z_test[['mean_0', 'mean_1', 'mean_2', 'mean_3']].to_numpy()  # Estrai le colonne delle medie
    z_log_var = Z_test[['var_0', 'var_1', 'var_2', 'var_3']].to_numpy()  # Estrai le colonne delle log-varianze

    # Esegui la classificazione con PyMC usando il vettore continuo
    # Assicurati di passare la riga completa di medie e varianze per il campione selezionato
    trace = mcmc_classification_pymc(z_mean[idx], z_log_var[idx], n_iter)

    # Accedi ai campioni accettati
    accepted_labels = trace.posterior["label_vector"].stack(samples=("chain", "draw")).values

    # Ottieni il vero label per l'indice scelto
    true_label = labels[idx]

    # Salva i risultati nel CSV
    append_results_to_csv(csv_file_name, idx, true_label, accepted_labels)

print(f"Risultati salvati nel file {csv_file_name} dopo {num_iterations} iterazioni.")



"""
## Analisi performance
"""

# metrics = calculate_performance_metrics(true_label, label_estimate_mean, label_estimate_mode)

# print("Metriche di performance:")

# # Per la media
# print(f"MAE (Media): {metrics['mae_mean']}")
# print(f"MSE (Media): {metrics['mse_mean']}")
# print(f"RMSE (Media): {metrics['rmse_mean']}")
# print(f"R² (Media): {metrics['r_squared_mean']}")
# print(f"SMAPE (Media): {metrics['smape_mean']}")
# print(f"NRMSE (Media): {metrics['nrmse_mean']}")
# print(f"Cosine Similarity (Media): {metrics['cosine_sim_mean']}")
# print(f"Explained Variance (Media): {metrics['explained_variance_mean']}")

# # Per la moda
# print(f"MAE (Moda): {metrics['mae_mode']}")
# print(f"MSE (Moda): {metrics['mse_mode']}")
# print(f"RMSE (Moda): {metrics['rmse_mode']}")
# print(f"R² (Moda): {metrics['r_squared_mode']}")
# print(f"SMAPE (Moda): {metrics['smape_mode']}")
# print(f"NRMSE (Moda): {metrics['nrmse_mode']}")
# print(f"Cosine Similarity (Moda): {metrics['cosine_sim_mode']}")
# print(f"Explained Variance (Moda): {metrics['explained_variance_mode']}")


# medie_parametri = calcola_media_parametri('mcmc_results_3.csv')


# Esegui la funzione con il file CSV contenente i risultati
# csv_file_name = 'mcmc_results_2.csv'
# create_confusion_matrix(csv_file_name)
# analyze_damage_levels(csv_file_name)

# update_performance_metrics_from_file('mcmc_results.csv', 'mcmc_results_1.csv')

# medie_parametri = calcola_media_parametri('mcmc_results_2.csv')


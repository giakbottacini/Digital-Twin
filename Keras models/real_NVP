"""
Title: Density estimation using Real NVP
Authors: [Mandolini Giorgio Maria](https://www.linkedin.com/in/giorgio-maria-mandolini-a2a1b71b4/), [Sanna Daniele](https://www.linkedin.com/in/daniele-sanna-338629bb/), [Zannini Quirini Giorgio](https://www.linkedin.com/in/giorgio-zannini-quirini-16ab181a0/)
Date created: 2020/08/10
Last modified: 2020/08/10
Description: Estimating the density distribution of the "double moon" dataset.
Accelerator: GPU
"""

"""
## Introduction

The aim of this work is to map a simple distribution - which is easy to sample
and whose density is simple to estimate - to a more complex one learned from the data.
This kind of generative model is also known as "normalizing flow".

In order to do this, the model is trained via the maximum
likelihood principle, using the "change of variable" formula.

We will use an affine coupling function. We create it such that its inverse, as well as
the determinant of the Jacobian, are easy to obtain (more details in the referenced paper).

**Requirements:**

* Tensorflow 2.9.1
* Tensorflow probability 0.17.0

**Reference:**

[Density estimation using Real NVP](https://arxiv.org/abs/1605.08803)
"""

"""
## Setup

"""
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from sklearn.datasets import make_moons
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_probability as tfp

"""
## Load the data
"""

data = make_moons(3000, noise=0.05)[0].astype("float32")
    #make_moons(3000, noise=0.05): genera un dataset di 3000 punti con due classi separate in una forma a "mezzaluna". Il parametro noise=0.05 aggiunge un po' di rumore ai dati per renderli meno lineari e più realistici.
    #[0]: questa parte del codice estrae solo le coordinate (feature) dei dati generati, ignorando le etichette delle classi.
    #astype("float32"): converte i dati in numeri in virgola mobile a 32 bit, un formato comune nelle operazioni di machine learning per ridurre l'occupazione di memoria.
    #Risultato: data è un array di punti (coordinate) bidimensionali, ciascuno rappresentato da due valori float32.

norm = layers.Normalization()  #Crea un layer di normalizzazione, utilizzato per scalare i dati in modo che abbiano una media vicina a 0 e una deviazione standard vicina a 1.
norm.adapt(data)  #Calcola le statistiche (media e deviazione standard) dei dati forniti (data) e le "adatta" nel layer di normalizzazione. Queste statistiche verranno utilizzate successivamente per normalizzare i dati.
normalized_data = norm(data)  #Applica il layer di normalizzazione ai dati originali, restituendo i dati normalizzati. Questo significa che i dati normalized_data avranno ora una distribuzione con media 0 e deviazione standard 1.

"""
## Affine coupling layer
"""

# Creating a custom layer with keras API.
output_dim = 256
reg = 0.01


def Coupling(input_shape):  #La funzione prende come parametro input_shape, che rappresenta la forma dell'input (cioè il numero di feature del dato in ingresso).
    
    #La funzione definisce due serie di strati densi (Dense layers), uno per la componente di traslazione (t_layers) e uno per la componente di scalatura (s_layers), 
    # che insieme costituiscono la parte chiave del modello.
    
    input = keras.layers.Input(shape=(input_shape,))

    #La rete t_layers rappresenta una funzione che trasforma i dati originali tramite una serie di strati densi (fully connected).
    #Ogni strato denso ha un numero di nodi pari a output_dim e utilizza l'attivazione ReLU.
    t_layer_1 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg) #Viene utilizzato un termine di regolarizzazione L2 (regularizers.l2(reg)) per limitare i pesi dei neuroni e prevenire l'overfitting.
    )(input)
    t_layer_2 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
    )(t_layer_1)
    t_layer_3 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
    )(t_layer_2)
    t_layer_4 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
    )(t_layer_3)
    t_layer_5 = keras.layers.Dense(
        input_shape, activation="linear", kernel_regularizer=regularizers.l2(reg) #L'ultimo strato denso ha come numero di nodi input_shape e utilizza l'attivazione lineare (cioè nessuna attivazione), 
    )(t_layer_4)                                                                  #poiché questo strato rappresenta la traslazione che non richiede una trasformazione non lineare finale.

    #La rete s_layers è molto simile a t_layers, con una serie di strati densi che rappresentano la funzione di scalatura dei dati.
    s_layer_1 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
    )(input)
    s_layer_2 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
    )(s_layer_1)
    s_layer_3 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
    )(s_layer_2)
    s_layer_4 = keras.layers.Dense(
        output_dim, activation="relu", kernel_regularizer=regularizers.l2(reg)
    )(s_layer_3)
    s_layer_5 = keras.layers.Dense(
        input_shape, activation="tanh", kernel_regularizer=regularizers.l2(reg)  #L'ultimo strato denso ha una funzione di attivazione tanh, che limita i valori prodotti 
    )(s_layer_4)                                                                 #a un intervallo tra -1 e 1. Questo è comune per la componente di scalatura poiché mantiene i valori numerici stabili.

    return keras.Model(inputs=input, outputs=[s_layer_5, t_layer_5])  
    #Viene creato un modello Keras che prende l'input e produce due output: s_layer_5: la componente di scalatura e t_layer_5: la componente di traslazione.




"""
## Real NVP
"""


class RealNVP(keras.Model):
    def __init__(self, num_coupling_layers):
        super().__init__()

        self.num_coupling_layers = num_coupling_layers  #Memorizza il numero di strati di accoppiamento (coupling layers) che verranno utilizzati nel modello.

        # Distribution of the latent space.
        self.distribution = tfp.distributions.MultivariateNormalDiag(   #Viene definita una distribuzione normale multivariata (tfp.distributions.MultivariateNormalDiag) con media loc=[0.0, 0.0] e deviazione standard scale_diag=[1.0, 1.0]. Questa distribuzione rappresenta lo spazio latente in cui il modello proietta i dati dopo aver applicato le trasformazioni.
            loc=[0.0, 0.0], scale_diag=[1.0, 1.0]     #Questa distribuzione rappresenta lo spazio latente in cui il modello proietta i dati dopo aver applicato le trasformazioni.
        )
        self.masks = np.array(   #self.masks è una matrice utilizzata per controllare quali componenti dell'input vengono trasformati a ciascuno strato di accoppiamento.
            [[0, 1], [1, 0]] * (num_coupling_layers // 2), dtype="float32"   #La maschera alterna i valori tra [0, 1] e [1, 0], il che significa che i dati vengono suddivisi in due componenti. Ogni strato di accoppiamento trasforma una sola componente alla volta, lasciando l'altra invariata.
        )
        self.loss_tracker = keras.metrics.Mean(name="loss")    #Viene utilizzato un keras.metrics.Mean chiamato loss_tracker per tracciare il valore medio della perdita (loss) durante l'addestramento del modello. 
        self.log_likelihood_tracker = keras.metrics.Mean(name="log_likelihood")
        self.log_det_tracker = keras.metrics.Mean(name="log_det")
        
        self.layers_list = [Coupling(2) for i in range(num_coupling_layers)]  #Questa lista contiene num_coupling_layers strati di accoppiamento, ciascuno creato utilizzando la funzione Coupling(2). Ogni strato trasforma l'input tramite la combinazione di reti di traslazione e scalatura, come visto in precedenza. 
    
    # Ogni strato di accoppiamento nella lista self.layers_list trasforma l'input utilizzando le reti di scalatura e traslazione (s_layers e t_layers). 
    # Queste trasformazioni sono invertibili e consentono di "mappare" i dati verso una distribuzione più semplice, mantenendo la possibilità di invertire 
    # le trasformazioni per ricostruire i dati originali.

    @property
    def metrics(self):
        """List of the model's metrics.

        We make sure the loss tracker is listed as part of `model.metrics`
        so that `fit()` and `evaluate()` are able to `reset()` the loss tracker
        at the start of each epoch and at the start of an `evaluate()` call.
        """
        return [self.loss_tracker, self.log_likelihood_tracker, self.log_det_tracker]

    def call(self, x, training=True):  #training: Un booleano che indica se il modello è in fase di addestramento (True) o inferenza (False). Questo influenza la direzione delle trasformazioni.
        log_det_inv = 0   #Questa variabile accumula il logaritmo del determinante jacobiano della trasformazione invertibile. Il determinante del Jacobiano tiene traccia della modifica di volume in ogni passaggio della trasformazione.
        direction = 1
        if training:
            direction = -1
        for i in range(self.num_coupling_layers)[::direction]:
            x_masked = x * self.masks[i]
            reversed_mask = 1 - self.masks[i]
            s, t = self.layers_list[i](x_masked)
            s *= reversed_mask
            t *= reversed_mask
            gate = (direction - 1) / 2
            x = (
                reversed_mask
                * (x * tf.exp(direction * s) + direction * t * tf.exp(gate * s))
                + x_masked
            )
            log_det_inv += gate * tf.reduce_sum(s, [1])

        return x, log_det_inv


    # Log likelihood of the normal distribution plus the log determinant of the jacobian.

    def log_loss(self, x):
        y, logdet = self(x)
        log_likelihood = self.distribution.log_prob(y) + logdet

        tf.print("log_prob:", self.distribution.log_prob(y))
    
        # Return separate components
        return -tf.reduce_mean(log_likelihood), -tf.reduce_mean(logdet)

    def train_step(self, data):
        with tf.GradientTape() as tape:
            log_likelihood, log_det = self.log_loss(data)

            # Total loss is the sum of log-likelihood and the log-det term
            loss = log_likelihood + log_det

        g = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(g, self.trainable_variables))
        
        # Update all trackers
        self.loss_tracker.update_state(loss)
        self.log_likelihood_tracker.update_state(log_likelihood)
        self.log_det_tracker.update_state(log_det)

        # Return all tracked metrics
        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det": self.log_det_tracker.result(),
        }

    def test_step(self, data):
        log_likelihood, log_det = self.log_loss(data)
        loss = log_likelihood + log_det

        # Update all trackers
        self.loss_tracker.update_state(loss)
        self.log_likelihood_tracker.update_state(log_likelihood)
        self.log_det_tracker.update_state(log_det)

        # Return all tracked metrics
        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det": self.log_det_tracker.result(),
        }

    


"""
## Model training
"""

model = RealNVP(num_coupling_layers=6)

model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001))

history = model.fit(
    normalized_data, batch_size=256, epochs=300, verbose=2, validation_split=0.2
)

"""
## Performance evaluation
"""

# Questa porzione del codice genera un grafico dell'andamento della perdita (\textit{loss}) durante il processo di addestramento del modello, 
# per entrambi i set di dati di addestramento e di validazione.
plt.figure(figsize=(15, 10))   #
plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("model loss")
plt.legend(["train", "validation"], loc="upper right")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.show()
plt.close()
# From data to latent space.
z, _ = model(normalized_data)  #La variabile z rappresenta i dati trasformati nello spazio latente usando il modello.

# From latent space to data.
samples = model.distribution.sample(3000)  #samples rappresenta campioni generati dallo spazio latente (distribuzione multivariata normale).
x, _ = model.predict(samples)  #Utilizzando model.predict(samples), i campioni dello spazio latente vengono trasformati indietro nello spazio dati originale, ottenendo x.

fig, axes = plt.subplots(2, 2)
fig.set_size_inches(20, 15)

axes[0, 0].scatter(normalized_data[:, 0], normalized_data[:, 1], color="r")
axes[0, 0].set(title="Inference data space X", xlabel="x", ylabel="y")
axes[0, 1].scatter(z[:, 0], z[:, 1], color="r")
axes[0, 1].set(title="Inference latent space Z", xlabel="x", ylabel="y")
axes[0, 1].set_xlim([-3.5, 4])
axes[0, 1].set_ylim([-4, 4])
axes[1, 0].scatter(samples[:, 0], samples[:, 1], color="g")
axes[1, 0].set(title="Generated latent space Z", xlabel="x", ylabel="y")
axes[1, 1].scatter(x[:, 0], x[:, 1], color="g")
axes[1, 1].set(title="Generated data space X", label="x", ylabel="y")
axes[1, 1].set_xlim([-2, 2])
axes[1, 1].set_ylim([-2, 2])

plt.show()
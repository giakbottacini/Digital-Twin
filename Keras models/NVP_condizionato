"""
## Setup

"""
import os

os.environ["KERAS_BACKEND"] = "tensorflow"

import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras import regularizers
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_probability as tfp
import pandas as pd




# Specify the example you are dealing with
esempio = 'L_FRAME_DT'
# Training and Testing data
ID              = '7_1'
save_ID         = 'Classifier_total_7_1/'
path            = "../" + esempio + '/Dati/'
path_data_train   = path + 'istantrain_' + ID
path_data_test   = path + 'istantest_' + ID
# Saving model
path_save       = "models/"
# Prediction model
restore_ID= 'Classifier_total_7_1/'
path_restore = "results/"

# Which dof monitored
which_channels = [1,2,3,4,5,6,7,8]

n_channels = 8
seq_len = 200
# Specify if you are using accelerations =1 or displacements =0
accelerations = 0

# Questa funzione calcola il valore quadratico medio (RMS) di un vettore di segnale, utile per misurare l'energia del segnale e per 
# aggiungere rumore durante la corruzione dei segnali.
def RMS(vect):
    return np.sqrt(np.mean(np.square(vect)))


# ----------------------------------------------------------------------------

"""
## Importazione dati
"""

# Read data and create dataset with only desired classes
def read_data(path_data):    
    label_path_class = path_data + '/Damage_class.csv'                                
    labels_class     = np.genfromtxt(label_path_class)
    labels_class     = labels_class.astype('int')
    N_ist = len(labels_class)  

    if accelerations == 1:
        path_rec  = path_data + '/U2_concat_'
    elif accelerations == 0:
        path_rec  = path_data + '/U_concat_'

    if path_data == path_data_train:
        signals_means = np.zeros((n_channels))
        signals_stds = np.zeros((n_channels))
    else:
        signals_means = np.load(path_restore+'/Corrupted_signals_means.npy')
        signals_stds = np.load(path_restore+'/Corrupted_signals_stds.npy')
    # Create the dataset output structure
    X       = np.zeros((N_ist, seq_len, n_channels))   #(10000,200,8)
    # X_noise = np.zeros((N_ist, seq_len, n_channels))
    for i1 in range(n_channels):
        path_data_var  = path_rec + str(which_channels[i1]) + '.csv'
        X_singledof = pd.read_csv(path_data_var, header=None).to_numpy()[:,0]
        for i2 in range(N_ist):
            # RESHAPE THE SIGNALS
            X[i2,:,i1]= X_singledof[1 + i2*(1+seq_len) : (i2+1)*(1+seq_len)]
            # # CORRUPT THE SIGNALS
            # rms_signal = RMS(X[i2,:,i1])
            # dev_std    = rms_signal / np.sqrt(addedd_SNR)
            # sample = st.norm(0, dev_std)
            # X_noise[i2,:,i1] = X[i2,:,i1] + sample.rvs(size=seq_len)
        if path_data == path_data_train:
            # COMPUTE STATISTICS FOR EACH CHANNEL
            # signals_means[i1] = np.mean(np.reshape(X_noise[:,:,i1], (N_ist*seq_len)))
            # signals_stds[i1] = np.std(np.reshape(X_noise[:,:,i1], (N_ist*seq_len)))
            signals_means[i1] = np.mean(np.reshape(X[:,:,i1], (N_ist*seq_len)))
            signals_stds[i1] = np.std(np.reshape(X[:,:,i1], (N_ist*seq_len)))

        # NORMALIZE THE SIGNALS    
        # X_noise[:,:,i1] = (X_noise[:,:,i1] - signals_means[i1])/signals_stds[i1]
        X[:,:,i1] = (X[:,:,i1] - signals_means[i1])/signals_stds[i1]
   
    # if path_data == path_data_train:    
    #     np.save(path_save+'Corrupted_signals_means', signals_means)
    #     np.save(path_save+'Corrupted_signals_stds', signals_stds)
       
    # return X_noise, labels_class, N_ist
    return X, labels_class, N_ist


"""
## Affine coupling layer
"""

# Creating a custom layer with keras API.
output_dim = 256  # Opzioni 256, 512, 1024
reg = 0.01

# Number of classes in the dataset
n_class = 8

seq_len = 200

# Hyperparameters
validation_split = 0.20
batch_size = 32
n_epochs = 250
early_stop_epochs=15
initial_lr = 1e-3
decay_length = 0.8
ratio_to_stop = 0.05

filter_1      = 32;   filter_2      = 64;   filter_3      = 32
kernel_size_1 = 25;   kernel_size_2 =  13;   kernel_size_3 = 7
neurons_4 = 64
neurons_5 = 16
attivaz_conv = 'tanh'
attivaz_mlp = 'tanh'
k_reg = 1e-3
b_reg = 1e-3
rate_drop = 0.05


def Coupling(input_shape=(200, 8), num_classes=n_class, reg=0.01):
    # Input con shape (200, 8), ovvero 200 time steps con 8 feature per step
    input_x = layers.Input(shape=input_shape, name='Input_X')
    input_y = layers.Input(shape=(num_classes,), name='Input_Label')
    
    # Network per condizionare s e t in funzione del label y
    # Uso un piccolo MLP per una rappresentazione più complessa dell'embedding delle classi
    label_condition = layers.Dense(64, activation='relu')(input_y)
    label_condition = layers.Dense(128, activation='relu')(label_condition)
    label_condition = layers.Dense(128, activation='relu')(label_condition)
    label_embedding = layers.Dense(256, activation='relu')(label_condition)  # Aggiungiamo un terzo livello per maggiore complessità

    # Blocchi convoluzionali per l'estrazione di caratteristiche temporali
    x = layers.Conv1D(filters=filter_1, kernel_size=kernel_size_1, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_1')(input_x)
    x = layers.MaxPooling1D()(x)
    x = layers.Conv1D(filters=filter_2, kernel_size=kernel_size_2, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_2')(x)
    x = layers.MaxPooling1D()(x)
    x = layers.Conv1D(filters=filter_3, kernel_size=kernel_size_3, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_3')(x)
    x = layers.MaxPooling1D()(x)
    x = layers.Flatten()(x)

    # Livelli densi per la componente t (traslazione) con condizionamento MLP
    t = keras.layers.Dense(128, activation="relu", kernel_regularizer=regularizers.l2(reg))(x)
    t_embedding = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
    t = layers.Concatenate()([t, t_embedding])  # Condizionamento
    t = keras.layers.Dense(input_shape[0] * input_shape[1], activation="linear", kernel_regularizer=regularizers.l2(reg))(t)
    t_reshaped = keras.layers.Reshape(input_shape)(t)

    # Livelli densi per la componente s (scalatura) con condizionamento MLP
    s = keras.layers.Dense(128, activation="relu", kernel_regularizer=regularizers.l2(reg))(x)
    s_embedding = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
    s = layers.Concatenate()([s, s_embedding])  # Condizionamento
    s = keras.layers.Dense(input_shape[0] * input_shape[1], activation="tanh", kernel_regularizer=regularizers.l2(reg))(s)
    s_reshaped = keras.layers.Reshape(input_shape)(s)

    return keras.Model(inputs=[input_x, input_y], outputs=[s_reshaped, t_reshaped])





# def Coupling(input_shape=(200, 8), num_classes=n_class, reg=0.01):
#     # Input con shape (200, 8), ovvero 200 time steps con 8 feature per step
#     input_x = layers.Input(shape=input_shape, name='Input_X')
#     input_y = layers.Input(shape=(num_classes,), name='Input_Label')
    
#     # Network per condizionare s e t in funzione del label y
#     # Utilizzo di una rete di embedding per ottenere una rappresentazione continua della classe
#     label_embedding = layers.Dense(64, activation='relu')(input_y)
#     label_embedding = layers.Dense(128, activation='relu')(label_embedding)
    
#     # Blocchi convoluzionali per l'estrazione di caratteristiche temporali
#     x = layers.Conv1D(filters=64, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(reg))(input_x)
#     x = layers.MaxPooling1D()(x)
#     x = layers.Conv1D(filters=128, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(reg))(x)
#     x = layers.MaxPooling1D()(x)
#     x = layers.Conv1D(filters=256, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(reg))(x)
#     x = layers.MaxPooling1D()(x)
#     x = layers.Flatten()(x)
    
#     # Livelli densi per la componente t (traslazione) con condizionamento moltiplicativo
#     t = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(reg))(x)
#     t_embedding = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
#     t = layers.Concatenate()([t, t_embedding])
#     t = layers.Dense(input_shape[0] * input_shape[1], activation='linear', kernel_regularizer=regularizers.l2(reg))(t)
#     t_reshaped = layers.Reshape(input_shape)(t)

#     # Livelli densi per la componente s (scalatura) con condizionamento moltiplicativo
#     s = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(reg))(x)
#     s_embedding = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
#     s = layers.Concatenate()([s, s_embedding])
#     s = layers.Dense(input_shape[0] * input_shape[1], activation='tanh', kernel_regularizer=regularizers.l2(reg))(s)
#     s_reshaped = layers.Reshape(input_shape)(s)

#     return keras.Model(inputs=[input_x, input_y], outputs=[s_reshaped, t_reshaped])


"""
## Real NVP
"""


class RealNVP(keras.Model):
    def __init__(self, num_coupling_layers, num_classes):
        super().__init__()

        self.num_coupling_layers = num_coupling_layers
        self.num_classes = num_classes

        # Distribuzione dello spazio latente (200, 8)
        self.distribution = tfp.distributions.MultivariateNormalDiag(
            loc=[0.0] * 1600, scale_diag=[1.0] * 1600  # La distribuzione ha 1600 dimensioni (200 * 8)
        )

        # Maschere (alterniamo metà delle dimensioni su entrambe le direzioni)
        self.masks = tf.convert_to_tensor(
            [[0] * 800 + [1] * 800, [1] * 800 + [0] * 800] * (num_coupling_layers // 2), dtype=tf.float32
        )

        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.log_likelihood_tracker = keras.metrics.Mean(name="log_likelihood")
        self.log_det_inv_tracker = keras.metrics.Mean(name="log_det_inv")

        self.layers_list = [Coupling(input_shape=(200, 8), num_classes=num_classes) for i in range(num_coupling_layers)]
    
    @property
    def metrics(self):
        return [self.loss_tracker,
                self.log_likelihood_tracker,
                self.log_det_inv_tracker]

    def call(self, x, y, training=True):

        log_det_inv = 0
        direction = -1 if training else 1  # Direzione della trasformazione (invertito in training)

        for i in range(self.num_coupling_layers)[::direction]:
            # Separiamo x_A (parte trasformata) e x_B (parte invariata) tramite le maschere
            mask = tf.reshape(self.masks[i], [1, 200, 8])  # Aggiungi una dimensione batch

            mask = tf.tile(mask, [tf.shape(x)[0], 1, 1])  # Replica la maschera per il batch size

            x_A = x * mask  # Parte trasformata
            x_B = x * (1 - mask)  # Parte invariata
            
            # Passa x_B mascherato e il label y per calcolare s(x_B) e t(x_B)
            s, t = self.layers_list[i]([x_B, y])  # Funzioni di scalatura e traslazione
            
            # Applica la trasformazione diretta o inversa in base alla direzione
            if direction == 1:
                # In avanti: y_A = x_A * e^{s(x_B)} + t(x_B)
                x_A = x_A * tf.exp(s) + t
            else:
                # In inverso: x_A = (y_A - t(x_B)) / e^{s(x_B)}
                x_A = (x_A - t) / tf.exp(s)

            # Ricombina x_A trasformato con x_B invariato
            x = x_A * mask + x_B * (1 - mask)

            
            log_det_inv += tf.reduce_sum(s * mask, axis=[1, 2])  # Log-determinante su s(x_B)

        return x, log_det_inv
    

    def compute_loss(self, x, y):

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y)

        # Rimodella z_pred in (None, 1600) per farlo corrispondere alla dimensione della distribuzione
        z_pred = tf.reshape(z_pred, [-1, 1600])

        # Calcola la log-likelihood condizionata
        log_likelihood = self.distribution.log_prob(z_pred)

        # La funzione di perdita è la negativa della log-likelihood più il log_det_inv
        loss = -tf.reduce_mean(log_likelihood - log_det_inv)

        
        return loss, tf.reduce_mean(log_likelihood), tf.reduce_mean(log_det_inv)

    

    def train_step(self, data):
        x, y = data  # Estraiamo i dati e le etichette
        with tf.GradientTape() as tape:
            loss, log_likelihood, log_det_inv = self.compute_loss(x, y)

        # Calcola i gradienti
        grads = tape.gradient(loss, self.trainable_variables)
        # Applica i gradienti
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        # Aggiorna lo stato del tracker della perdita
        self.loss_tracker.update_state(loss)
        # Aggiorna i tracker delle componenti
        self.log_likelihood_tracker.update_state(log_likelihood)
        self.log_det_inv_tracker.update_state(log_det_inv)

        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }



    def test_step(self, data):
        x, y = data
        loss, log_likelihood, log_det_inv = self.compute_loss(x, y)
        self.loss_tracker.update_state(loss)

        self.log_likelihood_tracker.update_state(log_likelihood)
        self.log_det_inv_tracker.update_state(log_det_inv)
        
        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }


    def predict(self, x, y):
        # Assicurati che x e y siano TensorFlow tensors
        x = tf.convert_to_tensor(x, dtype=tf.float32)
        y = tf.convert_to_tensor(y, dtype=tf.float32)

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y, training=False)

        # Rimodella z_pred in (None, 1600) per farlo corrispondere alla dimensione della distribuzione
        z_pred = tf.reshape(z_pred, [-1, 1600])

        # Calcola la log-likelihood condizionata
        log_likelihood = self.distribution.log_prob(z_pred) - log_det_inv

        return log_likelihood


    def predict_for_all_labels(self, x):
        # Assicurati che x sia un tensore
        x = tf.convert_to_tensor(x, dtype=tf.float32)
        
        # Creare un array one-hot per tutte le classi
        all_labels = np.eye(self.num_classes)  # (num_classes, num_classes)
        all_labels = tf.convert_to_tensor(all_labels, dtype=tf.float32)
        
        # Replica x per ogni label (batch_size, num_classes, features)
        x_expanded = tf.expand_dims(x, 1)  # Aggiungi una dimensione per i label: (batch_size, 1, features)
        x_tiled = tf.tile(x_expanded, [1, self.num_classes, 1, 1])  # (batch_size, num_classes, 200, 8)
        
        # Aggiungi una dimensione per espandere all_labels (num_classes -> batch_size, num_classes, num_classes)
        all_labels_tiled = tf.tile(tf.expand_dims(all_labels, 0), [x.shape[0], 1, 1])  # (batch_size, num_classes, num_classes)
        
        # Reshaping per poter passare al modello
        x_reshaped = tf.reshape(x_tiled, [-1, x.shape[1], x.shape[2]])  # (batch_size * num_classes, 200, 8)
        labels_reshaped = tf.reshape(all_labels_tiled, [-1, self.num_classes])  # (batch_size * num_classes, num_classes)

        # Applica il modello per ottenere z e il log-determinante del Jacobiano
        z, log_det_inv = self(x_reshaped, labels_reshaped, training=False)  # (batch_size * num_classes, 200, 8)
        
        # Rimodella z per adattarsi alla dimensione della distribuzione
        z = tf.reshape(z, [-1, 1600])  # (batch_size * num_classes, 1600)
        
        # Calcola la log-likelihood condizionata nello spazio latente
        log_likelihood = self.distribution.log_prob(z) - log_det_inv  # (batch_size * num_classes)
        
        # Rimodella log-likelihood per ottenere (batch_size, num_classes)
        log_likelihoods = tf.reshape(log_likelihood, [x.shape[0], self.num_classes])

        return log_likelihoods
        


"""
## Model training
"""

observations_normalized, damage_class_labels, N_ist_train = read_data(path_data_train)

print(f"Forma di observations_normalized: {observations_normalized.shape}")
print(f"Forma di damage_class_labels: {damage_class_labels.shape}")

model = RealNVP(num_coupling_layers=6, num_classes=n_class)

model.compile(optimizer=keras.optimizers.Adam(learning_rate=keras.optimizers.schedules.CosineDecay(initial_learning_rate=initial_lr, decay_steps=int(decay_length*n_epochs*N_ist_train*(1-validation_split)/batch_size), alpha=ratio_to_stop)))

# Crea un dataset tf.data.Dataset dai dati e dalle etichette
damage_class_labels = tf.cast(damage_class_labels, tf.int64)  # Converti in int64
print(f"Forma di damage_class_labels 1: {damage_class_labels.shape}")
damage_class_labels_one_hot = tf.one_hot(damage_class_labels, depth=n_class)
print(f"Forma di damage_class_labels_one_hot: {damage_class_labels_one_hot.shape}")

dataset = tf.data.Dataset.from_tensor_slices((observations_normalized, damage_class_labels_one_hot))

# Calcola il numero di batch di validazione
validation_split = 0.2
dataset_size = len(observations_normalized)
validation_size = int(dataset_size * validation_split)
train_size = dataset_size - validation_size

# Dividi il dataset in train e validation
train_dataset = dataset.take(train_size).batch(batch_size)
validation_dataset = dataset.skip(train_size).take(validation_size).batch(batch_size)


early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stop_epochs, restore_best_weights=True)

history = model.fit(
    train_dataset, epochs=100, verbose=2, validation_data=validation_dataset, callbacks=[early_stop]
)


# Salva lo storico dell'addestramento in formato pickle
hist = pd.DataFrame(history.history)  # Converti history.history (dati storici dell'addestramento) in un DataFrame di pandas
hist['epoch'] = history.epoch  # Aggiungi una colonna per le epoche. history.epoch è una lista che contiene i numeri delle epoche per cui sono stati registrati i dati.

# Salva il DataFrame
try:
    hist.to_pickle(os.path.join(path_save, 'hist_NVP_cond.pkl'))   #Salva il DataFrame hist in un file con formato pickle.
    print(f"Storico salvato in {os.path.join(path_save, 'hist_NVP_cond.pkl')}")
except Exception as e:
    print(f"Errore durante il salvataggio dello storico: {e}")


# Salva i pesi
model.save_weights('./models/NVP_cond.weights.h5')




"""
## Performance evaluation
"""

# Questa porzione del codice genera un grafico dell'andamento della perdita (\textit{loss}) durante il processo di addestramento del modello, 
# per entrambi i set di dati di addestramento e di validazione.
plt.figure(figsize=(15, 10))   #
plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("model loss")
plt.legend(["train", "validation"], loc="upper right")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.show()
plt.close()



#Test del modello ottenuto

# Calcolo del log-likelihood su dati di test/validazione
# test_log_likelihood = -model.log_loss(normalized_data_2)
# print(f"Test log-likelihood: {test_log_likelihood.numpy()}")

# # Genera nuovi campioni dallo spazio latente
# z_samples = model.distribution.sample(1000)  # Esempio di 1000 campioni latenti
# generated_samples, _ = model.predict(z_samples)

# # Confronta i nuovi campioni con i dati originali
# plt.figure(figsize=(10, 6))
# plt.scatter(generated_samples[:, 0], generated_samples[:, 1], label="Generated Samples", color='b', alpha=0.5)
# plt.scatter(normalized_data_1[:, 0], normalized_data_1[:, 1], label="Training Data", color='r', alpha=0.5)
# plt.legend()
# plt.show()

# from scipy.stats import wasserstein_distance

# # Calcolo della distanza di Wasserstein tra due distribuzioni (ad esempio, prima e seconda dimensione)
# wd = wasserstein_distance(normalized_data_1[:, 0], generated_samples[:, 0])
# print(f"Wasserstein distance (dimension 1): {wd}")

# #Latent Space Analysis
# from sklearn.decomposition import PCA
# from sklearn.manifold import TSNE

# # PCA per ridurre lo spazio latente a 2D
# pca = PCA(n_components=2)
# z_pca = pca.fit_transform(z)

# plt.figure(figsize=(8, 6))
# plt.scatter(z_pca[:, 0], z_pca[:, 1], label="Latent Space (PCA)", color='b', alpha=0.5)
# plt.title("Latent space after PCA")
# plt.show()
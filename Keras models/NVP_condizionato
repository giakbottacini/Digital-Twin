"""
## Setup

"""
import os

os.environ["KERAS_BACKEND"] = "tensorflow"

import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras import regularizers
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_probability as tfp
import pandas as pd

from keras import ops
from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_absolute_error
import seaborn as sns

import scipy.stats as st



# Specify the example you are dealing with
esempio = 'L_FRAME_DT'
# Training and Testing data
ID              = '7_1'
save_ID         = 'Classifier_total_7_1/'
path            = "../" + esempio + '/Dati/'
path_data_train   = path + 'istantrain_' + ID
path_data_test   = path + 'istantest_' + ID
# Saving model
path_save       = "models/"
# Prediction model
restore_ID= 'Classifier_total_7_1/'
path_restore = path + restore_ID

# Which dof monitored
which_channels = [1,2,3,4,5,6,7,8]

n_channels = 8
seq_len = 200
# Specify if you are using accelerations =1 or displacements =0
accelerations = 0

# Questa funzione calcola il valore quadratico medio (RMS) di un vettore di segnale, utile per misurare l'energia del segnale e per 
# aggiungere rumore durante la corruzione dei segnali.
def RMS(vect):
    return np.sqrt(np.mean(np.square(vect)))


# ----------------------------------------------------------------------------

"""
## Importazione dati
"""
# addedd_SNR = 100

# Read data and create dataset with only desired classes
def read_data(path_data, addedd_SNR):    
    label_path_class = path_data + '/Damage_class.csv'                                
    labels_class     = np.genfromtxt(label_path_class)
    labels_class     = labels_class.astype('int')
    N_ist = len(labels_class)  

    if accelerations == 1:
        path_rec  = path_data + '/U2_concat_'
    elif accelerations == 0:
        path_rec  = path_data + '/U_concat_'

    if path_data == path_data_train:
        signals_means = np.zeros((n_channels))
        signals_stds = np.zeros((n_channels))
    else:
        signals_means = np.load(path_restore+'/Corrupted_signals_means.npy')
        signals_stds = np.load(path_restore+'/Corrupted_signals_stds.npy')
    # Create the dataset output structure
    X       = np.zeros((N_ist, seq_len, n_channels))   #(10000,200,8)
    X_noise = np.zeros((N_ist, seq_len, n_channels))
    for i1 in range(n_channels):
        path_data_var  = path_rec + str(which_channels[i1]) + '.csv'
        X_singledof = pd.read_csv(path_data_var, header=None).to_numpy()[:,0]
        for i2 in range(N_ist):
            # RESHAPE THE SIGNALS
            X[i2,:,i1]= X_singledof[1 + i2*(1+seq_len) : (i2+1)*(1+seq_len)]
            # # CORRUPT THE SIGNALS
            rms_signal = RMS(X[i2,:,i1])
            dev_std    = rms_signal / np.sqrt(addedd_SNR)
            sample = st.norm(0, dev_std)
            X_noise[i2,:,i1] = X[i2,:,i1] + sample.rvs(size=seq_len)
        if path_data == path_data_train:
            # COMPUTE STATISTICS FOR EACH CHANNEL
            signals_means[i1] = np.mean(np.reshape(X_noise[:,:,i1], (N_ist*seq_len)))
            signals_stds[i1] = np.std(np.reshape(X_noise[:,:,i1], (N_ist*seq_len)))
            # signals_means[i1] = np.mean(np.reshape(X[:,:,i1], (N_ist*seq_len)))
            # signals_stds[i1] = np.std(np.reshape(X[:,:,i1], (N_ist*seq_len)))

        # NORMALIZE THE SIGNALS    
        X_noise[:,:,i1] = (X_noise[:,:,i1] - signals_means[i1])/signals_stds[i1]
        # X[:,:,i1] = (X[:,:,i1] - signals_means[i1])/signals_stds[i1]
   
    # if path_data == path_data_train:    
    #     np.save(path_save+'Corrupted_signals_means', signals_means)
    #     np.save(path_save+'Corrupted_signals_stds', signals_stds)
       
    # return X_noise, labels_class, N_ist
    return X_noise, labels_class, N_ist


"""
## Affine coupling layer
"""

# Creating a custom layer with keras API.
output_dim = 256  # Opzioni 256, 512, 1024
reg = 0.01

# Number of classes in the dataset
n_class = 8

seq_len = 200

# Hyperparameters
validation_split = 0.20
batch_size = 32
n_epochs = 150
early_stop_epochs=10
initial_lr = 1e-3
decay_length = 0.8
ratio_to_stop = 0.05

filter_1      = 32;   filter_2      = 64;   filter_3      = 32
kernel_size_1 = 25;   kernel_size_2 =  13;   kernel_size_3 = 7
neurons_4 = 64
neurons_5 = 16
attivaz_conv = 'tanh'
attivaz_mlp = 'tanh'
k_reg = 1e-3
b_reg = 1e-3
rate_drop = 0.05


# # Mia struttura con primo Enhancement del layer embedding   accuracy 0.8 (un po' di overfitting)
# def Coupling(input_shape=(200, 8), num_classes=n_class, reg=0.01):
#     # Input con shape (200, 8), ovvero 200 time steps con 8 feature per step
#     input_x = layers.Input(shape=input_shape, name='Input_X')
#     input_y = layers.Input(shape=(num_classes,), name='Input_Label')
    
#     # Network per condizionare s e t in funzione del label y
#     # Uso un piccolo MLP per una rappresentazione più complessa dell'embedding delle classi
#     label_condition = layers.Dense(32, activation='relu')(input_y)
#     label_condition = layers.Dense(64, activation='relu')(label_condition)
#     label_condition = layers.Dense(128, activation='relu')(label_condition)
#     label_embedding = layers.Dense(256, activation='relu')(label_condition) 

#     # Blocchi convoluzionali per l'estrazione di caratteristiche temporali
#     x = layers.Conv1D(filters=filter_1, kernel_size=kernel_size_1, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_1')(input_x)
#     x = layers.MaxPooling1D()(x)
#     x = layers.Conv1D(filters=filter_2, kernel_size=kernel_size_2, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_2')(x)
#     x = layers.MaxPooling1D()(x)
#     x = layers.Conv1D(filters=filter_3, kernel_size=kernel_size_3, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_3')(x)
#     x = layers.MaxPooling1D()(x)
#     x = layers.Flatten()(x)

#     # Livelli densi per la componente t (traslazione) con condizionamento MLP
#     t = keras.layers.Dense(128, activation="relu", kernel_regularizer=regularizers.l2(reg))(x)
#     t_embedding = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
#     t = layers.Concatenate()([t, t_embedding])  # Condizionamento
#     t = keras.layers.Dense(input_shape[0] * input_shape[1], activation="linear", kernel_regularizer=regularizers.l2(reg))(t)
#     t_reshaped = keras.layers.Reshape(input_shape)(t)

#     # Livelli densi per la componente s (scalatura) con condizionamento MLP
#     s = keras.layers.Dense(128, activation="relu", kernel_regularizer=regularizers.l2(reg))(x)
#     s_embedding = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
#     s = layers.Concatenate()([s, s_embedding])  # Condizionamento
#     s = keras.layers.Dense(input_shape[0] * input_shape[1], activation="tanh", kernel_regularizer=regularizers.l2(reg))(s)
#     s_reshaped = keras.layers.Reshape(input_shape)(s)

#     return keras.Model(inputs=[input_x, input_y], outputs=[s_reshaped, t_reshaped])



# Mia struttura con primo Enhancement del layer embedding ma più fit   
def Coupling(input_shape=(200, 8), num_classes=n_class, reg=0.01):
    # Input con shape (200, 8), ovvero 200 time steps con 8 feature per step
    input_x = layers.Input(shape=input_shape, name='Input_X')
    input_y = layers.Input(shape=(num_classes,), name='Input_Label')
    
    # Network per condizionare s e t in funzione del label y
    # Uso un piccolo MLP per una rappresentazione più complessa dell'embedding delle classi
    label_condition = layers.Dense(16, activation='relu')(input_y)
    label_condition = layers.Dense(32, activation='relu')(label_condition)
    label_condition = layers.Dense(64, activation='relu')(label_condition)
    label_embedding = layers.Dense(128, activation='relu')(label_condition) 

    # Blocchi convoluzionali per l'estrazione di caratteristiche temporali
    x = layers.Conv1D(filters=filter_1, kernel_size=kernel_size_1, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_1')(input_x)
    x = layers.MaxPooling1D()(x)
    x = layers.Dropout(0.3)(x)   # Aggiungi Dropout
    x = layers.Conv1D(filters=filter_2, kernel_size=kernel_size_2, strides=2, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_2')(x)
    x = layers.Dropout(0.3)(x)  # Aggiungi Dropout
    x = layers.Conv1D(filters=filter_3, kernel_size=kernel_size_3, strides=2, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_3')(x)
    x = layers.Dropout(0.3)(x)  # Aggiungi Dropout

    x = layers.Flatten()(x)

    # Livelli densi per la componente t (traslazione) con condizionamento MLP
    t = keras.layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(k_reg))(x)
    t_embedding = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(k_reg))(label_embedding)
    t = layers.Concatenate()([t, t_embedding])  # Condizionamento
    t = keras.layers.Dense(input_shape[0] * input_shape[1], activation="linear", kernel_regularizer=regularizers.l2(k_reg))(t)
    t_reshaped = keras.layers.Reshape(input_shape)(t)

    # Livelli densi per la componente s (scalatura) con condizionamento MLP
    s = keras.layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(k_reg))(x)
    s_embedding = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(k_reg))(label_embedding)
    s = layers.Concatenate()([s, s_embedding])  # Condizionamento
    s = keras.layers.Dense(input_shape[0] * input_shape[1], activation="tanh", kernel_regularizer=regularizers.l2(k_reg))(s)
    s_reshaped = keras.layers.Reshape(input_shape)(s)

    return keras.Model(inputs=[input_x, input_y], outputs=[s_reshaped, t_reshaped])


# # Mia struttura con primo Enhancement del layer embedding ma ancora più fit  (Non ancora provata)
# def Coupling(input_shape=(200, 8), num_classes=n_class, reg=0.01):
#     # Input con shape (200, 8), ovvero 200 time steps con 8 feature per step
#     input_x = layers.Input(shape=input_shape, name='Input_X')
#     input_y = layers.Input(shape=(num_classes,), name='Input_Label')
    
#     # Network per condizionare s e t in funzione del label y
#     # Uso un piccolo MLP per una rappresentazione più complessa dell'embedding delle classi
#     label_condition = layers.Dense(32, activation='relu')(input_y)
#     label_embedding = layers.Dense(64, activation='relu')(label_condition) 

#     # Blocchi convoluzionali per l'estrazione di caratteristiche temporali
#     x = layers.Conv1D(filters=filter_1, kernel_size=kernel_size_1, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_1')(input_x)
#     x = layers.MaxPooling1D()(x)
#     x = layers.Dropout(0.3)(x)   # Aggiungi Dropout
#     x = layers.Conv1D(filters=filter_2, kernel_size=kernel_size_2, strides=2, padding='same', kernel_regularizer=regularizers.l2(k_reg), bias_regularizer=regularizers.l2(b_reg), activation=attivaz_conv, name='Conv_2')(x)
#     x = layers.Dropout(0.3)(x)  # Aggiungi Dropout

#     x = layers.Flatten()(x)

#     # Livelli densi per la componente t (traslazione) con condizionamento MLP
#     t = keras.layers.Dense(32, activation="relu", kernel_regularizer=regularizers.l2(k_reg))(x)
#     t_embedding = layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(k_reg))(label_embedding)
#     t = layers.Concatenate()([t, t_embedding])  # Condizionamento
#     t = keras.layers.Dense(input_shape[0] * input_shape[1], activation="linear", kernel_regularizer=regularizers.l2(k_reg))(t)
#     t_reshaped = keras.layers.Reshape(input_shape)(t)

#     # Livelli densi per la componente s (scalatura) con condizionamento MLP
#     s = keras.layers.Dense(32, activation="relu", kernel_regularizer=regularizers.l2(k_reg))(x)
#     s_embedding = layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(k_reg))(label_embedding)
#     s = layers.Concatenate()([s, s_embedding])  # Condizionamento
#     s = keras.layers.Dense(input_shape[0] * input_shape[1], activation="tanh", kernel_regularizer=regularizers.l2(k_reg))(s)
#     s_reshaped = keras.layers.Reshape(input_shape)(s)

#     return keras.Model(inputs=[input_x, input_y], outputs=[s_reshaped, t_reshaped])

    

"""
## Real NVP
"""


class RealNVP(keras.Model):
    def __init__(self, num_coupling_layers, num_classes):
        super().__init__()

        self.num_coupling_layers = num_coupling_layers
        self.num_classes = num_classes

        # Distribuzione dello spazio latente (200, 8)
        self.distribution = tfp.distributions.MultivariateNormalDiag(
            loc=[0.0] * 1600, scale_diag=[1.0] * 1600  # La distribuzione ha 1600 dimensioni (200 * 8)
        )

        # Maschere (alterniamo metà delle dimensioni su entrambe le direzioni)
        self.masks = tf.convert_to_tensor(
            [[0] * 800 + [1] * 800, [1] * 800 + [0] * 800] * (num_coupling_layers // 2), dtype=tf.float32
        )


        # # Creazione della maschera alternata sui canali
        # self.masks = tf.convert_to_tensor(
        #     [[1, 0] * 4] * num_coupling_layers,  # Alternanza sui canali (8 canali: [1, 0, 1, 0, 1, 0, 1, 0])
        #     dtype=tf.float32
        # )

        # # Ogni maschera è ora di dimensione (8,) quindi dobbiamo estendere la maschera in base ai time steps
        # self.masks = tf.reshape(self.masks, [num_coupling_layers, 1, 1, 8])  # Aggiunge dimensioni batch e time steps


        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.log_likelihood_tracker = keras.metrics.Mean(name="log_likelihood")
        self.log_det_inv_tracker = keras.metrics.Mean(name="log_det_inv")

        self.layers_list = [Coupling(input_shape=(200, 8), num_classes=num_classes) for i in range(num_coupling_layers)]
    
    @property
    def metrics(self):
        return [self.loss_tracker,
                self.log_likelihood_tracker,
                self.log_det_inv_tracker]


    # # call modificato da me 
    # def call(self, x, y, training=True):

    #     log_det_inv = 0
    #     direction = -1 if training else 1  # Direzione della trasformazione (invertito in training)

    #     for i in range(self.num_coupling_layers)[::direction]:
    #         # Separiamo x_A (parte trasformata) e x_B (parte invariata) tramite le maschere
    #         mask = tf.reshape(self.masks[i], [1, 200, 8])  # Aggiungi una dimensione batch

    #         x_A = x * mask  # Parte trasformata
    #         x_B = x * (1 - mask)  # Parte invariata
            
    #         # Passa x_B mascherato e il label y per calcolare s(x_B) e t(x_B)
    #         s, t = self.layers_list[i]([x_B, y])  # Funzioni di scalatura e traslazione
            
    #         # Applica la trasformazione diretta o inversa in base alla direzione
    #         if direction == 1:
    #             # In avanti: y_A = x_A * e^{s(x_B)} + t(x_B)
    #             x_A = x_A * tf.exp(s) + t
    #         else:
    #             # In inverso: x_A = (y_A - t(x_B)) / e^{s(x_B)}
    #             x_A = (x_A - t) / tf.exp(s)

    #         # Ricombina x_A trasformato con x_B invariato
    #         x = x_A * mask + x_B * (1 - mask)

            
    #         log_det_inv += tf.reduce_sum(s * mask, axis=[1, 2])  # Log-determinante su s(x_B)

    #     return x, -log_det_inv


    # call con struttura iniziale 
    def call(self, x, y, training=True):  

        log_det_inv = 0   

        for i in range(self.num_coupling_layers):
            # # Applica la maschera alternata sui canali
            # mask = self.masks[i]  # Ottieni la maschera per il layer i-esimo
            # mask = tf.tile(mask, [tf.shape(x)[0], tf.shape(x)[1], 1])  # Replica la maschera per batch e time steps

            mask = tf.reshape(self.masks[i], [1, 200, 8])  # Aggiungi una dimensione batch
            x_masked = x * mask  # Maschera l'input
            reversed_mask = 1 - mask
            s, t = self.layers_list[i]([x_masked, y])
            s *= reversed_mask
            t *= reversed_mask

            x = (
                reversed_mask * (x * tf.exp(s) +  t )
                + x_masked
            )
            log_det_inv += tf.reduce_sum(s, axis=[1, 2])

        return x, log_det_inv
    

    def compute_loss(self, x, y):

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y)

        # Rimodella z_pred in (None, 1600) per farlo corrispondere alla dimensione della distribuzione
        z_pred = tf.reshape(z_pred, [-1, 1600])

        # Calcola la log-likelihood condizionata
        log_likelihood_z = self.distribution.log_prob(z_pred) 

        # tf.print("log_prob:", self.distribution.log_prob(z_pred))



        # # La funzione di perdita è la negativa della log-likelihood più il log_det_inv
        # loss = -tf.reduce_mean(log_likelihood - log_det_inv)

        
        return tf.reduce_mean(log_likelihood_z), tf.reduce_mean(log_det_inv)


    

    def train_step(self, data):
        x, y = data  # Estraiamo i dati e le etichette
        with tf.GradientTape() as tape:
            log_likelihood_z, log_det_inv = self.compute_loss(x, y)

            # Total loss is the sum of log-likelihood and the log-det term
            loss = -(log_likelihood_z + log_det_inv)

        # Calcola i gradienti
        grads = tape.gradient(loss, self.trainable_variables)
        # Applica i gradienti
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        # Aggiorna lo stato del tracker della perdita
        self.loss_tracker.update_state(loss)
        # Aggiorna i tracker delle componenti
        self.log_likelihood_tracker.update_state(log_likelihood_z)
        self.log_det_inv_tracker.update_state(log_det_inv)

        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }



    def test_step(self, data):
        x, y = data
        log_likelihood_z, log_det_inv = self.compute_loss(x, y)
        loss = -(log_likelihood_z + log_det_inv)

        self.loss_tracker.update_state(loss)
        self.log_likelihood_tracker.update_state(log_likelihood_z)
        self.log_det_inv_tracker.update_state(log_det_inv)
        
        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }


    def predict(self, x, y):
        # Assicurati che x e y siano TensorFlow tensors
        x = tf.convert_to_tensor(x, dtype=tf.float32)
        y = tf.convert_to_tensor(y, dtype=tf.float32)

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y, training=False)

        print(f"log_det_inv: {log_det_inv}")

        # Rimodella z_pred in (None, 1600) per farlo corrispondere alla dimensione della distribuzione
        z_pred = tf.reshape(z_pred, [-1, 1600])

        # Calcola la log-likelihood condizionata
        log_likelihood_z = self.distribution.log_prob(z_pred)

        log_likelihood = log_likelihood_z + log_det_inv

        return log_likelihood



    def predict_for_all_labels(self, x):
        # Assicurati che x sia un tensore
        x = tf.convert_to_tensor(x, dtype=tf.float32)

        all_log_likelihoods = []

        # Prevedi una classe alla volta
        for i in range(self.num_classes):
            # Crea un vettore one-hot per la classe corrente
            current_label = tf.one_hot([i], self.num_classes)

            # Ripeti current_label per l'intero batch
            current_label_repeated = tf.repeat(current_label, repeats=tf.shape(x)[0], axis=0)  # (batch_size, num_classes)

            # Prevedi per la classe corrente
            z_pred, log_det_inv = self(x, current_label_repeated, training=False)

            # Rimodella z per adattarsi alla dimensione della distribuzione
            z_pred = tf.reshape(z_pred, [-1, 1600])  # (batch_size, 1600)
            
            # Calcola la log-likelihood condizionata nello spazio latente
            log_likelihood_z = self.distribution.log_prob(z_pred)

            # Calcola la log-likelihood complessiva per questa classe
            log_likelihood = log_likelihood_z + log_det_inv
            
            # Aggiungi il risultato alla lista
            all_log_likelihoods.append(log_likelihood)

        # Concatena tutte le log-likelihoods per restituire un risultato finale
        return tf.stack(all_log_likelihoods, axis=1)




"""
## Funzioni utili
"""


def plot_class_transition_heatmap(NVP, data1, data2):

    log_likelihoods_1 = NVP.predict_for_all_labels(data1)
    log_likelihoods_2 = NVP.predict_for_all_labels(data2)

    # Predizione delle classi usando la log-likelihood massima
    predicted_classes_1 = tf.argmax(log_likelihoods_1, axis=1).numpy()
    predicted_classes_2 = tf.argmax(log_likelihoods_2, axis=1).numpy()

    # Seleziona un esempio casuale 
    idx = np.random.randint(0, data1.shape[0])
    original_signal = data1[idx]
    reconstructed_signal = data2[idx]

    # Plot dei segnali
    plt.plot(original_signal.flatten(), label='Meno rumore')
    plt.plot(reconstructed_signal.flatten(), label='Più rumore')
    plt.legend()
    plt.title(f"Segnale Meno Rumore vs Segnale Più Rumore - Esempio {idx}")
    plt.show()

    # Costruisci la matrice di transizione (matrice di confusione tra le due predizioni)
    transition_matrix = confusion_matrix(predicted_classes_1, predicted_classes_2, labels=np.arange(n_class))

    # Plotta la matrice di transizione come heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(transition_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(n_class), yticklabels=np.arange(n_class))
    
    # Etichette e titolo
    plt.title("Class Transition Matrix (Data1 vs Data2)", fontsize=16)
    plt.xlabel("Predicted Class (Data2)", fontsize=12)
    plt.ylabel("Predicted Class (Data1)", fontsize=12)

    # Salva la figura
    plt.savefig('output_NVP/class_transition_heatmap_NVP.png')

    # Mostra il grafico
    plt.show()


        

train_or_predict = 0   # 1: training, 0: predict                                       


if train_or_predict:
    """
    ## Model training
    """

    observations_normalized, damage_class_labels, N_ist_train = read_data(path_data_train, 100)

    print(f"Forma di observations_normalized: {observations_normalized.shape}")
    print(f"Forma di damage_class_labels: {damage_class_labels.shape}")

    model = RealNVP(num_coupling_layers=10, num_classes=n_class)

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=keras.optimizers.schedules.CosineDecay(initial_learning_rate=initial_lr, decay_steps=int(decay_length*n_epochs*N_ist_train*(1-validation_split)/batch_size), alpha=ratio_to_stop)))

    model.summary()

    # Crea un dataset tf.data.Dataset dai dati e dalle etichette
    damage_class_labels = tf.cast(damage_class_labels, tf.int64)  # Converti in int64
    print(f"Forma di damage_class_labels 1: {damage_class_labels.shape}")
    damage_class_labels_one_hot = tf.one_hot(damage_class_labels, depth=n_class)
    print(f"Forma di damage_class_labels_one_hot: {damage_class_labels_one_hot.shape}")

    dataset = tf.data.Dataset.from_tensor_slices((observations_normalized, damage_class_labels_one_hot))

    # Calcola il numero di batch di validazione
    validation_split = 0.2
    dataset_size = len(observations_normalized)
    validation_size = int(dataset_size * validation_split)
    train_size = dataset_size - validation_size

    # Dividi il dataset in train e validation
    train_dataset = dataset.take(train_size).batch(batch_size)
    validation_dataset = dataset.skip(train_size).take(validation_size).batch(batch_size)


    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stop_epochs, restore_best_weights=True)

    history = model.fit(
        train_dataset, epochs=n_epochs, verbose=2, validation_data=validation_dataset, callbacks=[early_stop]
    )


    # Salva lo storico dell'addestramento in formato pickle
    hist = pd.DataFrame(history.history)  # Converti history.history (dati storici dell'addestramento) in un DataFrame di pandas
    hist['epoch'] = history.epoch  # Aggiungi una colonna per le epoche. history.epoch è una lista che contiene i numeri delle epoche per cui sono stati registrati i dati.

    # Salva il DataFrame
    try:
        hist.to_pickle(os.path.join(path_save, 'hist_NVP_cond.pkl'))   #Salva il DataFrame hist in un file con formato pickle.
        print(f"Storico salvato in {os.path.join(path_save, 'hist_NVP_cond.pkl')}")
    except Exception as e:
        print(f"Errore durante il salvataggio dello storico: {e}")


    # Salva i pesi
    model.save_weights('./models/NVP_cond.weights.h5')


    """
    ## Performance evaluation
    """

    # Questa porzione del codice genera un grafico dell'andamento della perdita (\textit{loss}) durante il processo di addestramento del modello, 
    # per entrambi i set di dati di addestramento e di validazione.
    plt.figure(figsize=(15, 10))   #
    plt.plot(history.history["loss"])
    plt.plot(history.history["val_loss"])
    plt.title("model loss")
    plt.legend(["train", "validation"], loc="upper right")
    plt.ylabel("loss")
    plt.xlabel("epoch")
    plt.show()
    plt.close()

   
else:
    """
    ## Predict
    """
    output_dir = "output_NVP"

    # Caricamento dei dati di test
    X_test, labels_test, N_ist_test = read_data(path_data_test, 100)


    print(f"Forma di X_test: {X_test.shape}")

    # Crea una codifica one-hot delle etichette
    labels_test = tf.cast(labels_test, tf.int64)
    labels_test_one_hot = tf.one_hot(labels_test, depth=n_class)
    #print(f"Forma di labels test one hot encoding: {labels_test_one_hot.shape}")


    # test_dataset = tf.data.Dataset.from_tensor_slices((X_test, labels_test_one_hot))
    # test_dataset = test_dataset.batch(batch_size)

    # # Stampa la forma di un singolo batch
    # for x_batch, y_batch in test_dataset.take(1):  # Prendi un batch dal dataset per controllare le dimensioni
    #     print(f"Forma di un batch di dati: {x_batch.shape}")
    #     print(f"Forma di un batch di etichette: {y_batch.shape}")



    # Creazione dell'istanza RealNVP
    NVP = RealNVP(num_coupling_layers=8, num_classes=n_class)

    # NVP.compile(optimizer = keras.optimizers.Adam(learning_rate=keras.optimizers.schedules.CosineDecay(initial_learning_rate=initial_lr, decay_steps=int(decay_length*n_epochs*N_ist_test*(1-validation_split)/batch_size), alpha=ratio_to_stop)))
    #Non serve compilare se non devo continuare il training

    # Caricamento dei pesi salvati
    NVP.load_weights('./models/NVP_cond.weights.h5')



    # Ricostruzione dei dati di test
    log_likelihood_test = NVP.predict(X_test, labels_test_one_hot)  


    log_likelihoods = NVP.predict_for_all_labels(X_test)

    # print(f"Forma di log_likelihoods: {log_likelihoods.shape}")

    # Stampa delle prime 5 righe di log_likelihoods
    print("Prime 5 righe di log_likelihoods:")
    print(log_likelihoods.numpy()[:5])

    # Predizione delle classi usando la log-likelihood massima
    predicted_classes = tf.argmax(log_likelihoods, axis=1).numpy()

    print(f"Forma di predicted_classes: {predicted_classes.shape}")
    # Stampa delle prime 5 righe di predicted_classes
    print("Prime 5 righe di predicted_classes:")
    print(predicted_classes[:5])




    # Caricamento della cronologia dell'addestramento
    hist = pd.read_pickle(path_save + 'hist_NVP_cond.pkl')



    """
    ## Analisi della classificazione
    """ 

    # Calcola metriche di classificazione
    accuracy = accuracy_score(labels_test, predicted_classes)
    precision = precision_score(labels_test, predicted_classes, average='weighted')
    recall = recall_score(labels_test, predicted_classes, average='weighted')
    f1 = f1_score(labels_test, predicted_classes, average='weighted')

    # Calcola la matrice di confusione
    conf_matrix = confusion_matrix(labels_test, predicted_classes)

    # Crea una figura con due sotto-figure
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 14))

    # Plot della matrice di confusione
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=ax1, annot_kws={"size": 14})
    ax1.set_title('Confusion Matrix')
    ax1.set_xlabel('Predicted Label',fontsize=18)
    ax1.set_ylabel('True Label',fontsize=18)

    # Aggiungi le metriche di classificazione come testo nella seconda sotto-figura
    metrics_text = (
        f"Accuracy: {accuracy:.4f}\n"
        f"Precision: {precision:.4f}\n"
        f"Recall: {recall:.4f}\n"
        f"F1-Score: {f1:.4f}"
    )
    ax2.text(0.5, 0.5, metrics_text, fontsize=12, ha='center', va='center')
    ax2.set_axis_off()  # Nascondi gli assi

    # Salva il grafico
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "Confusion_Matrix_NVP.png"))

    # Mostra il grafico
    plt.show()




    """
    ## Analisi ulteriori
    """ 

    # X_test_2, labels_test_2, N_ist_test_2 = read_data(path_data_test, 20)
    # test_dataset_2 = tf.data.Dataset.from_tensor_slices((X_test_2, labels_test_2))
    # test_dataset_2 = test_dataset_2.batch(batch_size)

    # X_test_3, labels_test_3, N_ist_test_3 = read_data(path_data_test, 20)
    # test_dataset_3 = tf.data.Dataset.from_tensor_slices((X_test_3, labels_test_3))
    # test_dataset_3 = test_dataset_3.batch(batch_size)

    # plot_label_clusters_3d(NVP, X_test, predicted_classes)   #Plotta il clustering rispetto al label classificato dei dati test nello spazio latente

    # plot_variances_by_class(NVP, X_test, X_test_2, X_test_3)

    # plot_class_transition_heatmap(NVP, X_test, X_test_2)







"""
Title: MCMC NVP 2
"""

import os

os.environ["KERAS_BACKEND"] = "tensorflow"

# Imposta la variabile d'ambiente
os.environ['OMP_NUM_THREADS'] = '1'

import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras import regularizers
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_probability as tfp
import pandas as pd
import pymc as pm

from keras import ops
from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_absolute_error
import seaborn as sns

import scipy.stats as st

import arviz as az



# EXPECTED_SHAPE_X = [None, 4]  # La dimensione batch può variare, ma x ha sempre 4 elementi
# EXPECTED_SHAPE_Y = [None, 6]  # La dimensione batch può variare, ma y ha sempre 7 elementi

import multiprocessing





# Specify the example you are dealing with
esempio = 'DARCY_FRAME_DT'
# Training and Testing data
path_data_train   = '../DataGenerationDarcy/data/data_train_l25_kl16_7x7'
path_data_test   = '../DataGenerationDarcy/data/data_test_l25_kl16_7x7'
# Saving model
path_save       = "models/"
#Position of mean and std. deviation
path_restore  = '../DataGenerationDarcy/data/'

path_z = "output_VAE_for_MCMC"

seq_len = 49 #81
n_params = 16 #12 #10


latent_dim = 20 #16


# Read data and create dataset
def read_data(path_data, train):
    #Import labels (coeff of KL of the field)
    labels_path = path_data + '/samples_h1_60.csv'
    labels = np.genfromtxt(labels_path, delimiter=',', dtype=float)

    # Seleziona solo le prime 4 colonne
    # labels = labels[:, :n_params]
    labels = labels[:, :n_params]


    N_ist = len(labels) 

    # Importiamo i valori generati dal VAE
    if train:
        path_data_z = os.path.join(path_z, "z_train2.csv")
    else:
        path_data_z = os.path.join(path_z, "z_test2.csv")
    
    Z = pd.read_csv(path_data_z, header=0)

    return Z, labels, N_ist


#---------------------------------------------------------------------------------------------------------------

"""
## Affine coupling layer
"""

# Creating a custom layer with keras API.
output_dim = 256  # Opzioni 256, 512, 1024
reg = 0.01


# Hyperparameters
validation_split = 0.20
batch_size = 32
n_epochs = 350
early_stop_epochs=25
initial_lr = 1e-3
decay_length = 0.8
ratio_to_stop = 0.05

filter_1      = 32;   filter_2      = 64;   filter_3      = 32
kernel_size_1 = 25;   kernel_size_2 =  13;   kernel_size_3 = 7
neurons_4 = 64
neurons_5 = 16
attivaz_conv = 'tanh'
attivaz_mlp = 'tanh'
k_reg = 1e-3
b_reg = 1e-3
rate_drop = 0.05





# def Coupling(input_shape=latent_dim, num_params=n_params, reg=0.005, dropout_rate=0.1):
#     # Input per il vettore latente Z di dimensione 4
#     input_z = layers.Input(shape=(input_shape,), name='Input_Z')
#     # Input per il vettore di label condizionale di dimensione num_classes
#     input_l = layers.Input(shape=(num_params,), name='Input_Label')

#     # Network per condizionare s e t in funzione del label y
#     label_condition = layers.Dense(32, activation='elu')(input_l)
#     label_condition = layers.Dense(64, activation='elu')(label_condition)
#     label_embedding = layers.Dense(128, activation='elu')(label_condition) 

#     # Livelli densi per la componente t (traslazione) con condizionamento MLP
#     t = layers.Dense(64, activation="elu", kernel_regularizer=regularizers.l2(reg))(input_z)
#     t = layers.Dropout(dropout_rate)(t)  # Aggiungi Dropout
#     t_embedding = layers.Dense(64, activation='elu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
#     t = layers.Concatenate()([t, t_embedding])  # Condizionamento con il label embedding
#     t = layers.Dense(32, activation="elu", kernel_regularizer=regularizers.l2(reg))(t)
#     t = layers.Dense(latent_dim, activation="linear", kernel_regularizer=regularizers.l2(reg))(t)

#     # Livelli densi per la componente s (scalatura) con condizionamento MLP
#     s = layers.Dense(64, activation="elu", kernel_regularizer=regularizers.l2(reg))(input_z)
#     s = layers.Dropout(dropout_rate)(s)  # Aggiungi Dropout
#     s_embedding = layers.Dense(64, activation='elu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
#     s = layers.Concatenate()([s, s_embedding])  # Condizionamento con il label embedding
#     s = layers.Dense(32, activation="elu", kernel_regularizer=regularizers.l2(reg))(s)
#     s = layers.Dense(latent_dim, activation="tanh", kernel_regularizer=regularizers.l2(reg))(s)

#     return keras.Model(inputs=[input_z, input_l], outputs=[s, t])


def Coupling(input_shape=latent_dim, num_params=n_params, reg=0.01, dropout_rate=0.2):
    # Input per il vettore latente Z di dimensione 4
    input_z = layers.Input(shape=(input_shape,), name='Input_Z')
    # Input per il vettore di label condizionale di dimensione num_classes
    input_l = layers.Input(shape=(num_params,), name='Input_Label')

    # Network per condizionare s e t in funzione del label y
    label_condition = layers.Dense(32, activation='elu')(input_l)
    label_condition = layers.Dropout(dropout_rate)(label_condition)
    label_condition = layers.Dense(64, activation='elu')(label_condition)
    label_condition = layers.Dropout(dropout_rate)(label_condition)
    label_embedding = layers.Dense(128, activation='elu')(label_condition) 

    # Livelli densi per la componente t (traslazione) con condizionamento MLP
    t = layers.Dense(64, activation="elu", kernel_regularizer=regularizers.l2(reg))(input_z)
    t = layers.Dropout(dropout_rate)(t)  # Aggiungi Dropout
    t_embedding = layers.Dense(64, activation='elu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
    t = layers.Concatenate()([t, t_embedding])  # Condizionamento con il label embedding
    t = layers.Dense(32, activation="elu", kernel_regularizer=regularizers.l2(reg))(t)
    t = layers.Dense(latent_dim, activation="linear", kernel_regularizer=regularizers.l2(reg))(t)

    # Livelli densi per la componente s (scalatura) con condizionamento MLP
    s = layers.Dense(64, activation="elu", kernel_regularizer=regularizers.l2(reg))(input_z)
    s = layers.Dropout(dropout_rate)(s)  # Aggiungi Dropout
    s_embedding = layers.Dense(64, activation='elu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
    s = layers.Concatenate()([s, s_embedding])  # Condizionamento con il label embedding
    s = layers.Dense(32, activation="elu", kernel_regularizer=regularizers.l2(reg))(s)
    s = layers.Dense(latent_dim, activation="tanh", kernel_regularizer=regularizers.l2(reg))(s)

    return keras.Model(inputs=[input_z, input_l], outputs=[s, t])


    

"""
## Real NVP
"""


class RealNVP(keras.Model):
    def __init__(self, num_coupling_layers, num_params):
        super().__init__()

        self.num_coupling_layers = num_coupling_layers
        self.num_parameters = num_params

        # Distribuzione dello spazio latente di dim 4
        self.distribution = tfp.distributions.MultivariateNormalDiag(
            loc=[0.0] * latent_dim, scale_diag=[1.0] * latent_dim  # La distribuzione ha 4 dimensioni
        )

        # Maschere (alterniamo metà delle dimensioni: 2 su 4)
        self.masks = tf.convert_to_tensor(
            [[0] * 10 + [1] * 10, [1] * 10 + [0] * 10] * (self.num_coupling_layers // 2), dtype=tf.float32
        )


        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.log_likelihood_tracker = keras.metrics.Mean(name="log_likelihood")
        self.log_det_inv_tracker = keras.metrics.Mean(name="log_det_inv")

        self.layers_list = [Coupling(input_shape=latent_dim, num_params=n_params) for i in range(num_coupling_layers)]
    
    @property
    def metrics(self):
        return [self.loss_tracker,
                self.log_likelihood_tracker,
                self.log_det_inv_tracker]


    # call con struttura iniziale 
    # def call(self, x, y, training=True):  

    #     log_det_inv = 0   

    #     for i in range(self.num_coupling_layers):

    #         mask = tf.reshape(self.masks[i], [1, latent_dim])  # Aggiungi una dimensione batch
    #         x_masked = x * mask  # Maschera l'input
    #         reversed_mask = 1 - mask
    #         s, t = self.layers_list[i]([x_masked, y])
    #         s *= reversed_mask
    #         t *= reversed_mask

    #         x = (
    #             reversed_mask * (x * tf.exp(s) +  t )
    #             + x_masked
    #         )
    #         log_det_inv += tf.reduce_sum(s, axis=1)

    #     return x, log_det_inv

    def call(self, x, y, training=True):  
        log_det_inv = 0   

        for i in range(self.num_coupling_layers):

            mask = tf.reshape(self.masks[i], [1, latent_dim])  # Shape: [1, dim]
            reversed_mask = 1 - mask

            # Primo passo: trasforma la parte non mascherata
            x_masked = x * mask
            s1, t1 = self.layers_list[i]([x_masked, y])
            s1 *= reversed_mask
            t1 *= reversed_mask

            x = reversed_mask * (x * tf.exp(s1) + t1) + x_masked
            log_det_inv += tf.reduce_sum(s1, axis=1)

            # Secondo passo: trasforma la parte mascherata, ora che x è aggiornato
            x_masked = x * reversed_mask
            s2, t2 = self.layers_list[i]([x_masked, y])
            s2 *= mask
            t2 *= mask

            x = mask * (x * tf.exp(s2) + t2) + x_masked
            log_det_inv += tf.reduce_sum(s2, axis=1)

        return x, log_det_inv
    

    def compute_loss(self, x, y):

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y)

        # Calcola la log-likelihood condizionata
        log_likelihood_z = self.distribution.log_prob(z_pred) 

        return tf.reduce_mean(log_likelihood_z), tf.reduce_mean(log_det_inv)


    

    def train_step(self, data):
        x, y = data  # Estraiamo i dati e le etichette
        with tf.GradientTape() as tape:
            log_likelihood_z, log_det_inv = self.compute_loss(x, y)

            # Total loss is the sum of log-likelihood and the log-det term
            loss = -(log_likelihood_z + log_det_inv)

        # Calcola i gradienti
        grads = tape.gradient(loss, self.trainable_variables)
        # Applica i gradienti
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        # Aggiorna lo stato del tracker della perdita
        self.loss_tracker.update_state(loss)
        # Aggiorna i tracker delle componenti
        self.log_likelihood_tracker.update_state(log_likelihood_z)
        self.log_det_inv_tracker.update_state(log_det_inv)

        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }



    def test_step(self, data):
        x, y = data
        log_likelihood_z, log_det_inv = self.compute_loss(x, y)
        loss = -(log_likelihood_z + log_det_inv)

        self.loss_tracker.update_state(loss)
        self.log_likelihood_tracker.update_state(log_likelihood_z)
        self.log_det_inv_tracker.update_state(log_det_inv)
        
        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }


    def predict(self, x, y):
        
        # Assicurati che x e y siano TensorFlow tensors
        x = tf.convert_to_tensor(x, dtype=tf.float32)
        y = tf.convert_to_tensor(y, dtype=tf.float32)

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y, training=False)

        # Calcola la log-likelihood condizionata
        log_likelihood_z = self.distribution.log_prob(z_pred)

        log_likelihood = log_likelihood_z + log_det_inv

        return log_likelihood

    @tf.function(jit_compile=True)  # Abilita XLA per la compilazione JIT
    def predict_per_uno(self, x, y):
        # Evita la conversione ridondante se x e y sono già tensori
        if not isinstance(x, tf.Tensor):
            x = tf.convert_to_tensor(x, dtype=tf.float32)
            x.set_shape([EXPECTED_SHAPE_X])  # Specifica la forma statica di x
        if not isinstance(y, tf.Tensor):
            y = tf.convert_to_tensor(y, dtype=tf.float32)
            y.set_shape([EXPECTED_SHAPE_Y])  # Specifica la forma statica di y

        # Disabilita la tracciatura del gradiente per migliorare l'efficienza
        with tf.GradientTape(persistent=False, watch_accessed_variables=False) as tape:
            tape.stop_recording()  # Disabilita il calcolo dei gradienti

            # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
            z_pred, log_det_inv = self(x, y, training=False)

            # Calcola la log-likelihood condizionata
            log_likelihood_z = self.distribution.log_prob(z_pred)
            log_likelihood = log_likelihood_z + log_det_inv

        # Restituisci il risultato scalato come richiesto
        return log_likelihood



#---------------------------------------------------------------------------------------------------------------------------------

n_coupling_layers = 20

# Creazione dell'istanza RealNVP
NVP = RealNVP(num_coupling_layers=n_coupling_layers, num_params=n_params)

NVP.build([(None, latent_dim), (None, n_params)])

# Caricamento dei pesi salvati
NVP.load_weights('./models/NVP_for_MCMC.weights.h5')





import pytensor.tensor as pt
from pytensor.graph.op import Op
from pytensor.graph.basic import Apply
import time

# Definisci l'Op personalizzata che esegue la chiamata al modello RealNVP con TensorFlow
class RealNVPTF(Op):
    def __init__(self, nvp_model):
        self.nvp_model = nvp_model  # Passa il modello NVP come attributo

    #make_node consente di definire come i dati entrano ed escono dal grafo computazionale e garantendo la compatibilità con i requisiti di PyMC.
    def make_node(self, data, label_vector):   
        # Forza il tipo di dati float64 per assicurarsi che PyMC riceva il tipo corretto
        data = pt.as_tensor_variable(data.astype(np.float64))
        label_vector = pt.as_tensor_variable(label_vector.astype(np.float64))
        return Apply(self, [data, label_vector], [pt.dscalar()])  # Restituisci uno scalare come output

    def perform(self, node, inputs, outputs):
        # Ricevi i numpy array e forza float64 per ogni operazione
        data, label_vector = inputs

        # Assicurati che anche l'input al modello NVP sia in formato float64
        label_vector_eval = np.expand_dims(label_vector, axis=0).astype(np.float64)  # (batch_size, n_class)

        # Esegui il modello NVP e ottieni la log-likelihood in float64
        # log_prob = self.nvp_model.predict(data.astype(np.float64), label_vector_eval.astype(np.float64))
        log_prob = self.nvp_model.predict_per_uno(data.astype(np.float64), label_vector_eval.astype(np.float64))

        # Assegna l'output con la giusta precisione
        outputs[0][0] = np.array(log_prob, dtype=np.float64)  # Forza l'output come float64


# Crea un'istanza dell'Op personalizzata con il modello NVP
real_nvp_tf_op = RealNVPTF(NVP)

# Definisci la funzione di log-likelihood che userai in PyMC
# def realnvp_logp(data, label_vector):
#     # Converte i dati e label_vector in float64 prima di passare all'Op personalizzato
#     return real_nvp_tf_op(data.astype(np.float64), label_vector.astype(np.float64))

def realnvp_logp(label_vector, z_mean, sigma):

    # Conversione esplicita in TensorVariable
    z_mean = pt.as_tensor_variable(z_mean, dtype='float64')
    sigma = pt.as_tensor_variable(sigma, dtype='float64')

    # Genera un array di numeri casuali da una distribuzione normale
    sampled_data = pt.random.normal(loc=z_mean, scale=sigma, size=(z_mean.shape[0],))
    # sampled_data = z_mean
       
    # Converte i dati e label_vector in float64 prima di passare all'Op personalizzato
    return real_nvp_tf_op(sampled_data.astype(np.float64), label_vector.astype(np.float64))


# Funzione che esegue l'inferenza MCMC usando Metropolis-Hastings
def mcmc_classification_pymc(z_mean, z_log_var, num_iterations):
    with pm.Model() as model:
        # Definiamo una distribuzione a priori per il vettore di label
        label_vector = pm.Normal('label_vector', mu=0, sigma=1, shape=(n_params,))

        # Calcola la deviazione standard dalla log-varianza
        sigma = pm.math.exp(0.5 * z_log_var)  # Deviance standard
        # Genera i nuovi dati campionando dalla distribuzione normale
        # sampled_data = pm.Normal('sampled_data', mu=z_mean, sigma=sigma, shape=(len(z_mean),))
        # sampled_data = pm.Deterministic('sampled_data', z_mean + sigma * np.random.randn(len(z_mean)))

        # Usa pm.Potential per integrare la log-likelihood calcolata esternamente
        # pm.Potential("realnvp_loglikelihood", realnvp_logp(sampled_data, label_vector))
        pm.Potential("realnvp_loglikelihood", realnvp_logp(label_vector, z_mean, sigma))


        # Eseguiamo l'inferenza MCMC usandoset Metropolis-Hastings
        step = pm.DEMetropolisZ(scaling=0.1, tune_interval=100)
        trace = pm.sample(num_iterations, cores=1, chains=2, step=step, tune=12000 , return_inferencedata=True)

        # Riepilogo del campionamento
        summary = az.summary(trace)

        # Mostra la sintesi
        print(summary)

    # Restituisci i risultati del vettore continuo campionato
    return trace




#---------------------------------------------------------------------------------------------------------------------------------






#Importo i dati 
Z_test, labels, N_ist = read_data(path_data_test, 0)

print(f"Forma di Z_test: {Z_test.shape}")

print(f"Forma di labels: {labels.shape}")

print("Prime 2 righe di labels:")
print(labels[:2])




# Funzione per stimare la moda utilizzando gli istogrammi
def estimate_mode_from_histogram(samples, bins=50):
    # Calcola la moda per ogni feature
    mode_per_feature = np.zeros(samples.shape[0])
    
    for i in range(samples.shape[0]):
        hist, bin_edges = np.histogram(samples[i, :], bins=bins)
        mode_bin_index = np.argmax(hist)  # Indice del bin con la frequenza massima
        mode_per_feature[i] = 0.5 * (bin_edges[mode_bin_index] + bin_edges[mode_bin_index + 1])  # Media del bin
    return mode_per_feature


# Funzione per calcolare la deviazione standard dei campioni
def calculate_std(samples):
    return np.std(samples, axis=-1)


import csv

def calculate_hpdi(samples, alpha=0.05):

    # Numero di campioni
    n_samples = samples.shape[0]
    
    # Percentuale da includere nell'HPDI (es. 95% -> include 95% dei campioni)
    credible_mass = 1.0 - alpha
    
    # HPDI per ciascuna dimensione
    hpdi = np.zeros((2, samples.shape[1]))
    
    # Calcola l'HPDI per ogni dimensione del vettore di label
    for dim in range(samples.shape[1]):
        # Ordina i campioni per quella dimensione
        sorted_samples = np.sort(samples[:, dim])
        
        # Numero di campioni da includere nell'HPDI
        interval_idx_inc = int(np.floor(credible_mass * n_samples))
        
        # Trova l'ampiezza minima dell'intervallo
        interval_width = sorted_samples[interval_idx_inc:] - sorted_samples[:n_samples - interval_idx_inc]
        
        # Trova l'indice dell'intervallo più stretto
        min_idx = np.argmin(interval_width)
        
        # Assegna i limiti inferiore e superiore dell'HPDI
        hpdi[0, dim] = sorted_samples[min_idx]
        hpdi[1, dim] = sorted_samples[min_idx + interval_idx_inc]
    
    return hpdi


# Funzione che salva i risultati in un file CSV
def append_results_to_csv(file_name, idx, true_label, samples_mcmc):
    # Se il file non esiste, crealo e aggiungi l'intestazione
    file_exists = os.path.isfile(file_name)

    with open(file_name, mode='a', newline='') as file:
        writer = csv.writer(file)
        
        # Aggiungi intestazione solo se il file non esiste
        if not file_exists:
            writer.writerow(['Index', 'True Label', 'Mean Estimate', 'Mode Estimate', 'Standard Deviation', 'HPDI Lower', 'HPDI Upper'])

        # Calcola le statistiche dai campioni
        label_estimate_mean = np.mean(samples_mcmc, axis=-1)
        label_estimate_mode = estimate_mode_from_histogram(samples_mcmc)
        label_std_dev = calculate_std(samples_mcmc)
        # Trasponi l'array per avere la forma (2800, 7)
        samples_mcmc = samples_mcmc.T
        hpdi = calculate_hpdi(samples_mcmc)

        # Imposta a 0 i valori inferiori a 0.05 per mean e mode
        label_estimate_mean[label_estimate_mean < 0.05] = 0.0
        label_estimate_mode[label_estimate_mode < 0.05] = 0.0

        # # Calcola le metriche di performance
        # metrics = calculate_performance_metrics(true_label, label_estimate_mean, label_estimate_mode)
        
        # Converti i vettori in stringhe separate da virgole
        true_label_str = ','.join(map(str, true_label))
        mean_label_str = ','.join(map(str, label_estimate_mean))
        mode_label_str = ','.join(map(str, label_estimate_mode))
        std_dev_str = ','.join(map(str, label_std_dev))
        hpdi_lower_str = ','.join(map(str, hpdi[0, :]))
        hpdi_upper_str = ','.join(map(str, hpdi[1, :]))

        # Scrivi la riga completa nel CSV
        writer.writerow([idx, true_label_str, mean_label_str, mode_label_str, std_dev_str,
                         hpdi_lower_str, hpdi_upper_str])




"""
## Conductivity prediction analysis
"""

def load_eigenpairs(filename):
    data = np.load(filename)
    return data["eigenvalues"], data["eigenvectors"]


import matplotlib.tri as tri

def compute_field_difference(true_coeffs, estimated_coeffs_mean, estimated_coeffs_mode, filename, field_mean=1.0, field_stdev=1.0, mesh_size=60):
    # Carica autovalori e autovettori
    eigenvalues, eigenvectors = load_eigenpairs(filename)

    eigenvalues_tot = eigenvalues[:16]
    eigenvectors_tot = eigenvectors[:, :16]
    true_coeffs_tot = true_coeffs[:16]
    
    # Usa solo i primi n_params autovalori e autovettori
    true_coeffs = true_coeffs[:n_params]
    eigenvalues = eigenvalues[:n_params]
    eigenvectors = eigenvectors[:, :n_params]
    
    # Calcola i tre campi
    def compute_field(coeffs):
        return np.exp(field_mean + field_stdev * eigenvectors @ (np.sqrt(eigenvalues) * coeffs))
    
    def compute_field_tot(coeffs):
        return np.exp(field_mean + field_stdev * eigenvectors_tot @ (np.sqrt(eigenvalues_tot) * coeffs))
    
    field_true = compute_field(true_coeffs)
    field_est_mean = compute_field(estimated_coeffs_mean)
    field_est_mode = compute_field(estimated_coeffs_mode)

    field_tot = compute_field_tot(true_coeffs_tot)
    
    # Calcola l'integrale del campo su tutto il dominio unitario
    def compute_integral(field, mesh_size=60):
        dx = 1.0 / mesh_size
        field_matrix = field.reshape((mesh_size + 1, mesh_size + 1))  # Reshape ai nodi
        field_avg = 0.25 * (field_matrix[:-1, :-1] + field_matrix[1:, :-1] + 
                            field_matrix[:-1, 1:] + field_matrix[1:, 1:])  # Media per elemento
        return np.sum(field_avg) * dx**2  # Integrazione sugli elementi

    
    integral_true = compute_integral(field_true)
    integral_est_mean = compute_integral(field_est_mean)
    integral_est_mode = compute_integral(field_est_mode)

    integral_tot = compute_integral(field_tot)
    
    # Calcola la norma relativa della differenza
    def compute_relative_norm(field1, field2, integral_ref):
        return compute_integral(np.abs(field1 - field2)) / integral_ref
    
    relative_norm_mean = compute_relative_norm(field_true, field_est_mean, integral_true)
    relative_norm_mode = compute_relative_norm(field_true, field_est_mode, integral_true)

    relative_norm_mean_tot = compute_relative_norm(field_tot, field_est_mean, integral_tot)
    relative_norm_mode_tot = compute_relative_norm(field_tot, field_est_mode, integral_tot)

    relative_norm_true_tot = compute_relative_norm(field_tot, field_true, integral_tot)
    
    # return {
    #     "integral_true": integral_true,
    #     "integral_est_mean": integral_est_mean,
    #     "integral_est_mode": integral_est_mode,
    #     "relative_norm_mean": relative_norm_mean,
    #     "relative_norm_mode": relative_norm_mode
    # }

    print(f"integral_true: {integral_true}")
    print(f"integral_est_mean: {integral_est_mean}")
    print(f"integral_est_mode: {integral_est_mode}")
    print(f"relative_norm_mean: {relative_norm_mean}")
    print(f"relative_norm_mode: {relative_norm_mode}")
    print(f"relative_norm_mean_tot: {relative_norm_mean_tot}")
    print(f"relative_norm_mode_tot: {relative_norm_mode_tot}")
    print(f"relative_norm_true_tot: {relative_norm_true_tot}")

    # Genera la griglia del dominio unitario
    x = np.linspace(0, 1, mesh_size + 1)
    y = np.linspace(0, 1, mesh_size + 1)
    X, Y = np.meshgrid(x, y)
    points = np.column_stack([X.ravel(), Y.ravel()])
    triang = tri.Triangulation(points[:, 0], points[:, 1])
    
    # Creazione del plot con subplot
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    titles = ["True Field", "Estimated Field (Mean)", "Estimated Field (Mode)",
              "Difference: True - Estimated (Mean)", "Difference: True - Estimated (Mode)", "Field Tot"]
    fields = [field_true, field_est_mean, field_est_mode,
              abs(field_true - field_est_mean), abs(field_true - field_est_mode), field_tot]
    
    for ax, field, title in zip(axes.flat, fields, titles):
        contour = ax.tricontourf(triang, field.ravel(), cmap='viridis')
        fig.colorbar(contour, ax=ax)
        ax.set_title(title)
        ax.set_xlabel("x")
        ax.set_ylabel("y")
    
    plt.tight_layout()
    plt.show()







"""
## Main
"""

eigenpairs = "eigenpairs_l25.npz"


eigenvalues, eigenvectors = load_eigenpairs(eigenpairs)

print("Shape autovalori:", eigenvalues.shape)       # Es. (10,)
print("Shape autovettori:", eigenvectors.shape)     # Es. (n_points, 10)
print("Primi 16 autovalori:", eigenvalues[:16])



# Indice casuale per selezionare un campione dal dataset Z
idx = np.random.randint(0, Z_test.shape[0]) 

# Estrai le colonne di media e log-varianza per il campione selezionato
z_mean = Z_test[['mean_0','mean_1','mean_2','mean_3','mean_4','mean_5','mean_6','mean_7','mean_8','mean_9','mean_10','mean_11','mean_12','mean_13','mean_14','mean_15', 'mean_16','mean_17','mean_18','mean_19']].to_numpy() #,'mean_20','mean_21','mean_22','mean_23']].to_numpy() #,'mean_24','mean_25','mean_26','mean_27', 'mean_28','mean_29']].to_numpy()  # Estrai le colonne delle medie
z_log_var = Z_test[['var_0','var_1','var_2','var_3','var_4','var_5','var_6','var_7','var_8','var_9','var_10','var_11','var_12','var_13','var_14','var_15','var_16','var_17','var_18','var_19']].to_numpy() #,'var_20','var_21','var_22','var_23','var_24','var_25','var_26','var_27','var_28','var_29']].to_numpy()  # Estrai le colonne delle log-varianze
# z_mean = Z_test[['mean_0','mean_1','mean_2','mean_3']].to_numpy()  # Estrai le colonne delle medie
# z_log_var = Z_test[['var_0','var_1','var_2','var_3']].to_numpy()


# Definisci il numero di iterazioni per MCMC
n_iter = 40000  

# Esegui la classificazione con PyMC usando il vettore continuo
# Assicurati di passare la riga completa di medie e varianze per il campione selezionato
trace = mcmc_classification_pymc(z_mean[idx], z_log_var[idx], n_iter)


# Questo ti restituirà un array con dimensioni (n_samples_totali, 7), dove n_samples_totali = n_chains * n_draws (n_iter, i tunes sono scartati)
accepted_labels = trace.posterior["label_vector"].stack(samples=("chain", "draw")).values

print(accepted_labels.shape)

# Fattori di divisione per ciascun elemento del vettore
# scaling_factors = np.array([5.93, 4.32, 4.32, 3.15]).reshape(4, 1)

# Divisione elemento per elemento
# accepted_labels = accepted_labels / scaling_factors


# Analizza i risultati: Calcola la media dei campioni per ottenere il label continuo stimato
label_estimate_mean = np.mean(accepted_labels, axis=-1)
print(f"Label stimato con media: {label_estimate_mean}")
print(f"Forma di label_estimate_mean: {label_estimate_mean.shape}")

# Calcola la moda basata sull'istogramma per ciascun componente del label continuo
label_estimate_mode = estimate_mode_from_histogram(accepted_labels, bins=50)
print(f"Label stimato con moda (basata su istogramma): {label_estimate_mode}")
print(f"Forma di label_estimate_mode: {label_estimate_mode.shape}")

# Ottieni il vero label per l'indice scelto
true_label = labels[idx]
print(f"Vero label: {true_label}")



# Visualizzazione dei risultati con istogrammi
num_labels = n_params
plt.figure(figsize=(15, 10))


for i in range(num_labels):
    plt.subplot(4, 4, i + 1)  # Crea una griglia 
    plt.hist(accepted_labels[i, :], bins=100, alpha=0.7, color='blue', edgecolor='black', range=(-5,5))
    plt.title(f'Class {i + 1}\nTrue label: {true_label[i]:.2f}',fontsize=16)
    plt.xlabel('Valore',fontsize=14)
    plt.ylabel('Frequenza',fontsize=14)
    plt.tick_params(axis='both', which='major', labelsize=10)


output_dir = "output_MCMC_NVP_2"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Aggiungi un layout e salvataggio
plt.tight_layout()
plt.savefig(os.path.join(output_dir, f'accepted_labels_histogram_idx_{idx}.png'))
plt.show()


# Seleziona la componente da tracciare
components_to_plot = [2]

plt.figure(figsize=(10, 6))

for i in components_to_plot:
    plt.plot(accepted_labels[i, 0:20000], label=f'Component {i}')

plt.xlabel('Iteration')
plt.ylabel('Parameter Value')
plt.title('MCMC Trace for Selected Label Vector Components')
plt.legend()
plt.show()


compute_field_difference(true_label, label_estimate_mean, label_estimate_mode, eigenpairs)






"""
## Creazione .csv
"""

n_iter = 40000 

# Nome del file CSV dove salvare i risultati
csv_file_name = 'mcmc_results_sampled.csv'

# Numero di iterazioni
num_iterations = 0

for i in range(num_iterations):

    # Stampa l'iterazione corrente
    print(f"Inizio iterazione {i + 1} di {num_iterations}")

    # Seleziona un indice casuale dal dataset Z_test
    idx = np.random.randint(0, Z_test.shape[0])
    
    # Estrai le colonne di media e log-varianza per il campione selezionato
    z_mean = Z_test[['mean_0', 'mean_1', 'mean_2', 'mean_3']].to_numpy()  # Estrai le colonne delle medie
    z_log_var = Z_test[['var_0', 'var_1', 'var_2', 'var_3']].to_numpy()  # Estrai le colonne delle log-varianze

    # Esegui la classificazione con PyMC usando il vettore continuo
    # Assicurati di passare la riga completa di medie e varianze per il campione selezionato
    trace = mcmc_classification_pymc(z_mean[idx], z_log_var[idx], n_iter)

    # Accedi ai campioni accettati
    accepted_labels = trace.posterior["label_vector"].stack(samples=("chain", "draw")).values

    # Ottieni il vero label per l'indice scelto
    true_label = labels[idx]

    # Salva i risultati nel CSV
    append_results_to_csv(csv_file_name, idx, true_label, accepted_labels)

print(f"Risultati salvati nel file {csv_file_name} dopo {num_iterations} iterazioni.")



"""
## Analisi performance
"""

# metrics = calculate_performance_metrics(true_label, label_estimate_mean, label_estimate_mode)

# print("Metriche di performance:")

# # Per la media
# print(f"MAE (Media): {metrics['mae_mean']}")
# print(f"MSE (Media): {metrics['mse_mean']}")
# print(f"RMSE (Media): {metrics['rmse_mean']}")
# print(f"R² (Media): {metrics['r_squared_mean']}")
# print(f"SMAPE (Media): {metrics['smape_mean']}")
# print(f"NRMSE (Media): {metrics['nrmse_mean']}")
# print(f"Cosine Similarity (Media): {metrics['cosine_sim_mean']}")
# print(f"Explained Variance (Media): {metrics['explained_variance_mean']}")

# # Per la moda
# print(f"MAE (Moda): {metrics['mae_mode']}")
# print(f"MSE (Moda): {metrics['mse_mode']}")
# print(f"RMSE (Moda): {metrics['rmse_mode']}")
# print(f"R² (Moda): {metrics['r_squared_mode']}")
# print(f"SMAPE (Moda): {metrics['smape_mode']}")
# print(f"NRMSE (Moda): {metrics['nrmse_mode']}")
# print(f"Cosine Similarity (Moda): {metrics['cosine_sim_mode']}")
# print(f"Explained Variance (Moda): {metrics['explained_variance_mode']}")


# medie_parametri = calcola_media_parametri('mcmc_results_3.csv')


# # Esegui la funzione con il file CSV contenente i risultati
# csv_file_name = 'mcmc_results_sampled_5.csv'
# # create_confusion_matrix(csv_file_name)
# analyze_damage_levels(csv_file_name)

# update_performance_metrics_from_file('mcmc_results.csv', 'mcmc_results_1.csv')

# medie_parametri = calcola_media_parametri('mcmc_results_2.csv')


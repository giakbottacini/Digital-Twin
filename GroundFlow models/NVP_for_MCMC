import os

os.environ["KERAS_BACKEND"] = "tensorflow"

import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras import regularizers
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_probability as tfp
import pandas as pd

from keras import ops
from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_absolute_error
import seaborn as sns

import scipy.stats as st






# Specify the example you are dealing with
esempio = 'DARCY_FRAME_DT'
# Training and Testing data
path_data_train   = '../DataGenerationDarcy/data/data_train_l25_kl16_7x7'
path_data_test   = '../DataGenerationDarcy/data/data_test_l25_kl16_7x7'
# Saving model
path_save       = "models/"
#Position of mean and std. deviation
path_restore  = '../DataGenerationDarcy/data/'

path_z = "output_VAE_for_MCMC"

seq_len = 49 #81 #49 #36 #25
n_params = 16 #12 #8



latent_dim = 20 #16


# ----------------------------------------------------------------------------

"""
## Importazione dati
"""
# addedd_SNR = 100

# Read data and create dataset
def read_data(path_data, train):    
    #Import labels (coeff of KL of the field)
    labels_path = path_data + '/samples_h1_60.csv'
    labels = np.genfromtxt(labels_path, delimiter=',', dtype=float)

    # Seleziona solo le prime n_params colonne
    labels = labels[:, :n_params]

    # scaling_factors = np.array([5.93, 4.32, 4.32, 3.15])
    # scaled_labels = labels * scaling_factors
    
    N_ist = len(labels)  

    # Importiamo i valori generati dal VAE
    if train:
        path_data_z = os.path.join(path_z, "z_train.csv")
    else:
        path_data_z = os.path.join(path_z, "z_test.csv")
    
    Z = pd.read_csv(path_data_z, header=0).to_numpy()

    return Z, labels, N_ist


# ----------------------------------------------------------------------------------------------------------------------


"""
## Affine coupling layer
"""

# Creating a custom layer with keras API.
output_dim = 256  # Opzioni 256, 512, 1024
reg = 0.01


# Hyperparameters
validation_split = 0.20
batch_size = 32
n_epochs = 200
early_stop_epochs=20
initial_lr = 1e-3
decay_length = 0.8
ratio_to_stop = 0.05




def Coupling(input_shape=latent_dim, num_params=n_params, reg=0.001, dropout_rate=0.2):
    # Input per il vettore latente Z di dimensione 4
    input_z = layers.Input(shape=(input_shape,), name='Input_Z')
    # Input per il vettore di label condizionale di dimensione num_classes
    input_l = layers.Input(shape=(num_params,), name='Input_Label')

    # Network per condizionare s e t in funzione del label y
    label_condition = layers.Dense(32, activation='elu')(input_l)
    label_condition = layers.Dropout(dropout_rate)(label_condition)
    label_condition = layers.Dense(64, activation='elu')(label_condition)
    label_condition = layers.Dropout(dropout_rate)(label_condition)
    label_embedding = layers.Dense(128, activation='elu')(label_condition) 

    # Livelli densi per la componente t (traslazione) con condizionamento MLP
    t = layers.Dense(64, activation="elu", kernel_regularizer=regularizers.l2(reg))(input_z)
    t = layers.Dropout(dropout_rate)(t)  # Aggiungi Dropout
    t_embedding = layers.Dense(64, activation='elu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
    t = layers.Concatenate()([t, t_embedding])  # Condizionamento con il label embedding
    t = layers.Dense(32, activation="elu", kernel_regularizer=regularizers.l2(reg))(t)
    t = layers.Dense(latent_dim, activation="linear", kernel_regularizer=regularizers.l2(reg))(t)

    # Livelli densi per la componente s (scalatura) con condizionamento MLP
    s = layers.Dense(64, activation="elu", kernel_regularizer=regularizers.l2(reg))(input_z)
    s = layers.Dropout(dropout_rate)(s)  # Aggiungi Dropout
    s_embedding = layers.Dense(64, activation='elu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
    s = layers.Concatenate()([s, s_embedding])  # Condizionamento con il label embedding
    s = layers.Dense(32, activation="elu", kernel_regularizer=regularizers.l2(reg))(s)
    s = layers.Dense(latent_dim, activation="tanh", kernel_regularizer=regularizers.l2(reg))(s)

    return keras.Model(inputs=[input_z, input_l], outputs=[s, t])





    

"""
## Real NVP
"""


class RealNVP(keras.Model):
    def __init__(self, num_coupling_layers, num_params):
        super().__init__()

        self.num_coupling_layers = num_coupling_layers
        self.num_parameters = num_params

        # Distribuzione dello spazio latente di dim 4
        self.distribution = tfp.distributions.MultivariateNormalDiag(
            loc=[0.0] * latent_dim, scale_diag=[1.0] * latent_dim  # La distribuzione ha 4 dimensioni
        )

        # Maschere (alterniamo metà delle dimensioni: 2 su 4)
        self.masks = tf.convert_to_tensor(
            [[0] * 10 + [1] * 10, [1] * 10 + [0] * 10] * (self.num_coupling_layers // 2), dtype=tf.float32
        )


        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.log_likelihood_tracker = keras.metrics.Mean(name="log_likelihood")
        self.log_det_inv_tracker = keras.metrics.Mean(name="log_det_inv")

        self.layers_list = [Coupling(input_shape=latent_dim, num_params=n_params) for i in range(num_coupling_layers)]
    
    @property
    def metrics(self):
        return [self.loss_tracker,
                self.log_likelihood_tracker,
                self.log_det_inv_tracker]


    # call con struttura iniziale 
    # def call(self, x, y, training=True):  

    #     log_det_inv = 0   

    #     for i in range(self.num_coupling_layers):

    #         mask = tf.reshape(self.masks[i], [1, latent_dim])  # Aggiungi una dimensione batch
    #         x_masked = x * mask  # Maschera l'input
    #         reversed_mask = 1 - mask
    #         s, t = self.layers_list[i]([x_masked, y])
    #         s *= reversed_mask
    #         t *= reversed_mask

    #         x = (
    #             reversed_mask * (x * tf.exp(s) +  t )
    #             + x_masked
    #         )
    #         log_det_inv += tf.reduce_sum(s, axis=1)

    #     return x, log_det_inv

    def call(self, x, y, training=True):  
        log_det_inv = 0   

        for i in range(self.num_coupling_layers):

            mask = tf.reshape(self.masks[i], [1, latent_dim])  # Shape: [1, dim]
            reversed_mask = 1 - mask

            # Primo passo: trasforma la parte non mascherata
            x_masked = x * mask
            s1, t1 = self.layers_list[i]([x_masked, y])
            s1 *= reversed_mask
            t1 *= reversed_mask

            x = reversed_mask * (x * tf.exp(s1) + t1) + x_masked
            log_det_inv += tf.reduce_sum(s1, axis=1)

            # Secondo passo: trasforma la parte mascherata, ora che x è aggiornato
            x_masked = x * reversed_mask
            s2, t2 = self.layers_list[i]([x_masked, y])
            s2 *= mask
            t2 *= mask

            x = mask * (x * tf.exp(s2) + t2) + x_masked
            log_det_inv += tf.reduce_sum(s2, axis=1)

        return x, log_det_inv

    

    def compute_loss(self, x, y):

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y)

        # Calcola la log-likelihood condizionata
        log_likelihood_z = self.distribution.log_prob(z_pred) 

        return tf.reduce_mean(log_likelihood_z), tf.reduce_mean(log_det_inv)


    

    def train_step(self, data):
        x, y = data  # Estraiamo i dati e le etichette
        with tf.GradientTape() as tape:
            log_likelihood_z, log_det_inv = self.compute_loss(x, y)

            # Total loss is the sum of log-likelihood and the log-det term
            loss = -(log_likelihood_z + log_det_inv)

        # Calcola i gradienti
        grads = tape.gradient(loss, self.trainable_variables)
        # Applica i gradienti
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        # Aggiorna lo stato del tracker della perdita
        self.loss_tracker.update_state(loss)
        # Aggiorna i tracker delle componenti
        self.log_likelihood_tracker.update_state(log_likelihood_z)
        self.log_det_inv_tracker.update_state(log_det_inv)

        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }



    def test_step(self, data):
        x, y = data
        log_likelihood_z, log_det_inv = self.compute_loss(x, y)
        loss = -(log_likelihood_z + log_det_inv)

        self.loss_tracker.update_state(loss)
        self.log_likelihood_tracker.update_state(log_likelihood_z)
        self.log_det_inv_tracker.update_state(log_det_inv)
        
        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }


    def predict(self, x, y):
        
        # Assicurati che x e y siano TensorFlow tensors
        x = tf.convert_to_tensor(x, dtype=tf.float32)
        y = tf.convert_to_tensor(y, dtype=tf.float32)

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y, training=False)

        # Calcola la log-likelihood condizionata
        log_likelihood_z = self.distribution.log_prob(z_pred)

        log_likelihood = log_likelihood_z + log_det_inv

        return log_likelihood

    @tf.function(jit_compile=True)  # Abilita XLA per la compilazione JIT
    def predict_per_uno(self, x, y):
        # Evita la conversione ridondante se x e y sono già tensori
        if not isinstance(x, tf.Tensor):
            x = tf.convert_to_tensor(x, dtype=tf.float32)
            x.set_shape([EXPECTED_SHAPE_X])  # Specifica la forma statica di x
        if not isinstance(y, tf.Tensor):
            y = tf.convert_to_tensor(y, dtype=tf.float32)
            y.set_shape([EXPECTED_SHAPE_Y])  # Specifica la forma statica di y

        # Disabilita la tracciatura del gradiente per migliorare l'efficienza
        with tf.GradientTape(persistent=False, watch_accessed_variables=False) as tape:
            tape.stop_recording()  # Disabilita il calcolo dei gradienti

            # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
            z_pred, log_det_inv = self(x, y, training=False)

            # Calcola la log-likelihood condizionata
            log_likelihood_z = self.distribution.log_prob(z_pred)
            log_likelihood = log_likelihood_z + log_det_inv

        # Restituisci il risultato scalato come richiesto
        return log_likelihood




"""
## Funzioni utili
"""



n_coupling_layers = 20       

train_or_predict = 0   # 1: training, 0: predict                                       


if train_or_predict:
    """
    ## Model training
    """

    Z_train, labels_train, N_ist_train = read_data(path_data_train, 1)

    print(f"Forma di Z_train: {Z_train.shape}")
    print(f"Forma di labels_train: {labels_train.shape}")

    print("Prime 2 righe di Z_train:")
    print(Z_train[:2])


    NVP = RealNVP(num_coupling_layers=n_coupling_layers, num_params=n_params)

    NVP.compile(optimizer=keras.optimizers.Adam(learning_rate=keras.optimizers.schedules.CosineDecay(initial_learning_rate=initial_lr, decay_steps=int(decay_length*n_epochs*N_ist_train*(1-validation_split)/batch_size), alpha=ratio_to_stop)))
    NVP.summary()


    dataset = tf.data.Dataset.from_tensor_slices((Z_train, labels_train))
  
    buffer_size = len(Z_train)  # Grandezza del buffer per lo shuffle

    # Crea il dataset da TensorFlow
    dataset = tf.data.Dataset.from_tensor_slices((Z_train, labels_train))

    # Mescola i dati PRIMA della suddivisione
    dataset = dataset.shuffle(buffer_size, seed=42)  # Usa un seed per riproducibilità

    # Calcola le dimensioni train e validation
    dataset_size = len(Z_train)
    validation_size = int(dataset_size * validation_split)
    train_size = dataset_size - validation_size

    # Suddivisione in train e validation set
    train_dataset = dataset.take(train_size).batch(batch_size)
    validation_dataset = dataset.skip(train_size).take(validation_size).batch(batch_size)



    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stop_epochs, restore_best_weights=True)

    history = NVP.fit(
        train_dataset, epochs=n_epochs, verbose=2, validation_data=validation_dataset, callbacks=[early_stop]
    )


    # Salva lo storico dell'addestramento in formato pickle
    hist = pd.DataFrame(history.history)  # Converti history.history (dati storici dell'addestramento) in un DataFrame di pandas
    hist['epoch'] = history.epoch  # Aggiungi una colonna per le epoche. history.epoch è una lista che contiene i numeri delle epoche per cui sono stati registrati i dati.

    # Salva il DataFrame
    try:
        hist.to_pickle(os.path.join(path_save, 'hist_NVP_for_MCMC.pkl'))   #Salva il DataFrame hist in un file con formato pickle.
        print(f"Storico salvato in {os.path.join(path_save, 'hist_NVP_for_MCMC.pkl')}")
    except Exception as e:
        print(f"Errore durante il salvataggio dello storico: {e}")


    # Salva i pesi
    NVP.save_weights('./models/NVP_for_MCMC.weights.h5')


    """
    ## Performance evaluation
    """
    # Questa porzione del codice genera un grafico dell'andamento della perdita (\textit{loss}) durante il processo di addestramento del modello, 
    # per entrambi i set di dati di addestramento e di validazione.

    # Crea la cartella se non esiste
    output_dir = "output_NVP_for_MCMC"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Plot combinato per la training loss e la validation loss
    plt.figure(figsize=(15, 5))  # Dimensioni del plot
    plt.plot(history.history["loss"], label="Training Loss", color='blue')
    plt.plot(history.history["val_loss"], label="Validation Loss", color='orange')
    plt.title("Training and Validation Loss")
    plt.ylabel("Loss")
    plt.xlabel("Epoch")
    plt.legend(loc="upper right")
    plt.grid(True)
    plt.savefig(os.path.join(output_dir, "losses.png"))
    plt.show()
    plt.close()



    #Plot True/Wrong loglikelihood
    labels_train_wrong = np.random.permutation(labels_train)

    log_likelihood_test = NVP.predict(Z_train, labels_train)
    log_likelihood_wrong = NVP.predict(Z_train, labels_train_wrong)

    print(f"Forma di log_likelihood_test: {log_likelihood_test.shape}")
    print(f"Forma di log_likelihood_wrong: {log_likelihood_wrong.shape}")
    print(f"Forma di labels_train_truncated: {labels_train_wrong.shape}")


    # Converti i tensori in array NumPy
    log_likelihood_wrong_np = log_likelihood_wrong.numpy()
    log_likelihood_test_np = log_likelihood_test.numpy()


    # Crea la figura
    plt.figure(figsize=(8, 6))

    # Istogramma del vettore log_likelihood_train
    plt.hist(log_likelihood_wrong_np, bins=100, range=(2*min(log_likelihood_test_np), 2*max(log_likelihood_test_np)),
            alpha=0.9, color='r', label='Wrong')

    # Istogramma del vettore log_likelihood_test
    plt.hist(log_likelihood_test_np, bins=100, range=(2*min(log_likelihood_test_np), 2*max(log_likelihood_test_np)),
            alpha=0.8, color='b', label='Right')

    # Titolo e etichette
    plt.xlabel('Log-likelihood', fontsize=20)
    plt.ylabel('Frequency', fontsize=20)
    plt.grid(True)

    # Aggiungi una legenda
    plt.legend(fontsize=18)

    # Salva il grafico in un file immagine
    plt.savefig(os.path.join(output_dir, "hist_confronto_2.png"))

    # Mostra il grafico
    plt.show()

   
else:
    """
    ## Predict
    """
    output_dir = "output_NVP_for_MCMC"

    # Caricamento dei dati di test
    Z_test, labels_test, N_ist_test = read_data(path_data_test, 0)

    print(f"Forma di Z_test: {Z_test.shape}")

    Z_train, labels_train, N_ist_train = read_data(path_data_train, 1)

    print(f"Forma di Z_train: {Z_train.shape}")


    # Creazione dell'istanza RealNVP
    NVP = RealNVP(num_coupling_layers=n_coupling_layers, num_params=n_params)

    # NVP.compile(optimizer = keras.optimizers.Adam(learning_rate=keras.optimizers.schedules.CosineDecay(initial_learning_rate=initial_lr, decay_steps=int(decay_length*n_epochs*N_ist_test*(1-validation_split)/batch_size), alpha=ratio_to_stop)))
    #Non serve compilare se non devo continuare il training

    NVP.build([(None, latent_dim), (None, n_params)])
 
    # Caricamento dei pesi salvati
    NVP.load_weights('./models/NVP_for_MCMC.weights.h5')

    # Caricamento della cronologia dell'addestramento
    hist = pd.read_pickle(path_save + 'hist_NVP_for_MCMC.pkl')


    # import time

    # # Selezione casuale di 3 indici da Z_test
    # random_indices = np.random.choice(Z_test.shape[0], size=2, replace=False)

    # # Inizio del timer per i 3 campioni
    # start_time = time.time()

    # # Predizione per i 3 campioni selezionati
    # log_likelihood_random = NVP.predict(Z_test[random_indices], labels_test[random_indices])

    # # Fine del timer e calcolo del tempo
    # end_time = time.time()
    # random_time = end_time - start_time
    # print(f"Tempo di esecuzione per 3 campioni casuali: {random_time} secondi")



    # # Selezione casuale di 3 indici da Z_test
    # random_indices = np.random.choice(Z_test.shape[0], size=2, replace=False)

    # # Inizio del timer per i 3 campioni
    # start_time = time.time()

    # # Predizione per i 3 campioni selezionati
    # log_likelihood_random = NVP.predict(Z_test[random_indices], labels_test[random_indices])

    # # Fine del timer e calcolo del tempo
    # end_time = time.time()
    # random_time = end_time - start_time
    # print(f"Tempo di esecuzione per 3 campioni casuali: {random_time} secondi")



    # # Inizio del timer per l'intero dataset
    # start_time = time.time()

    # # Esecuzione del modello NVP sul dataset intero
    # log_likelihood_test = NVP.predict(Z_test, labels_test)

    # # Fine del timer e calcolo del tempo
    # end_time = time.time()
    # total_time = end_time - start_time
    # print(f"Tempo di esecuzione per l'intero dataset: {total_time} secondi")




    # # Selezione casuale di 3 indici da Z_test
    # random_indices = np.random.choice(Z_test.shape[0], size=1, replace=False)

    # # Inizio del timer per i 3 campioni
    # start_time = time.time()

    # # Predizione per i 3 campioni selezionati
    # log_likelihood_random = NVP.predict_per_uno(Z_test[random_indices], labels_test[random_indices])

    # # Fine del timer e calcolo del tempo
    # end_time = time.time()
    # random_time = end_time - start_time
    # print(f"Tempo di esecuzione per 3 campioni casuali: {random_time} secondi")




    # # Selezione casuale di 3 indici da Z_test
    # random_indices = np.random.choice(Z_test.shape[0], size=1, replace=False)

    # # Inizio del timer per i 3 campioni
    # start_time = time.time()

    # # Predizione per i 3 campioni selezionati
    # log_likelihood_random = NVP.predict_per_uno(Z_test[random_indices], labels_test[random_indices])

    # # Fine del timer e calcolo del tempo
    # end_time = time.time()
    # random_time = end_time - start_time
    # print(f"Tempo di esecuzione per 3 campioni casuali: {random_time} secondi")






    log_likelihood_test = NVP.predict(Z_test, labels_test)
    log_likelihood_train = NVP.predict(Z_train, labels_train)

    print(f"Forma di log_likelihood_test: {log_likelihood_test.shape}")
    print(f"Forma di log_likelihood_train: {log_likelihood_train.shape}")


    # Converti i tensori in array NumPy
    log_likelihood_train_np = log_likelihood_train.numpy()
    log_likelihood_test_np = log_likelihood_test.numpy()


    # Crea la figura
    plt.figure(figsize=(8, 6))

    # Istogramma del vettore log_likelihood_train
    plt.hist(log_likelihood_train_np, bins=100, range=(min(log_likelihood_train_np), max(log_likelihood_train_np)),
            alpha=0.9, color='b', label='Train')

    # Istogramma del vettore log_likelihood_test
    plt.hist(log_likelihood_test_np, bins=100, range=(min(log_likelihood_test_np), max(log_likelihood_test_np)),
            alpha=0.8, color='r', label='Test')


    # Titolo e etichette
    plt.title('Istogramma sovrapposto delle log-likelihood')
    plt.xlabel('log-likelihood')
    plt.ylabel('Frequenza normalizzata')
    plt.grid(True)

    # Aggiungi una legenda
    plt.legend()

    # Salva il grafico in un file immagine
    plt.savefig(os.path.join(output_dir, "hist_confronto.png"))

    # Mostra il grafico
    plt.show()




    # np.random.seed(23)  
    # indices = np.random.permutation(len(X_train))

    # # Applica lo shuffle mantenendo la corrispondenza tra dati e label
    # X_train = X_train[indices]

    # labels_test_wrong = labels_train[:4000,:]
    labels_test_wrong = np.random.permutation(labels_test)

    log_likelihood_test = NVP.predict(Z_test, labels_test)
    log_likelihood_wrong = NVP.predict(Z_test, labels_test_wrong)

    print(f"Forma di log_likelihood_test: {log_likelihood_test.shape}")
    print(f"Forma di log_likelihood_wrong: {log_likelihood_wrong.shape}")
    print(f"Forma di labels_test_truncated: {labels_test_wrong.shape}")


    # Converti i tensori in array NumPy
    log_likelihood_wrong_np = log_likelihood_wrong.numpy()
    log_likelihood_test_np = log_likelihood_test.numpy()


    # Crea la figura
    plt.figure(figsize=(8, 6))

    # Istogramma del vettore log_likelihood_train
    plt.hist(log_likelihood_wrong_np, bins=100, range=(1.5*min(log_likelihood_test_np), max(log_likelihood_test_np)),
            alpha=0.9, color='r', label='Wrong')

    # Istogramma del vettore log_likelihood_test
    plt.hist(log_likelihood_test_np, bins=100, range=(1.5*min(log_likelihood_test_np), max(log_likelihood_test_np)),
            alpha=0.8, color='b', label='Right')

    # Titolo e etichette
    plt.xlabel('Log-likelihood', fontsize=20)
    plt.ylabel('Frequency', fontsize=20)
    plt.grid(True)

    # Aggiungi una legenda
    plt.legend(fontsize=18)

    # Salva il grafico in un file immagine
    plt.savefig(os.path.join(output_dir, "hist_confronto_2.png"))

    # Mostra il grafico
    plt.show()






    



    





    



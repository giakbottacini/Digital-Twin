import os

os.environ["KERAS_BACKEND"] = "tensorflow"

import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras import regularizers
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_probability as tfp
import pandas as pd

from keras import ops
from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_absolute_error
import seaborn as sns

import scipy.stats as st

EXPECTED_SHAPE_X = [None, 4]  # La dimensione batch può variare, ma x ha sempre 4 elementi
EXPECTED_SHAPE_Y = [None, 6]  # La dimensione batch può variare, ma y ha sempre 7 elementi



# Specify the example you are dealing with
esempio = 'TRAIN_FRAME_DT'
# Training and Testing data
ID              = '2_1'
save_ID         = '/Classifier_total_2_1/'
path            = "../" + esempio 
path_data_train   = path + '/istantrain_' + ID
path_data_test   = path + '/istantest_' + ID
# Saving model
path_save       = "models/"
# Prediction model
restore_ID = '/Classifier_total_2_1/'
path_restore = path + restore_ID

path_z = "output_VAE_for_MCMC"

# Which dof monitored
which_channels = [1,2,3,4,5,6,7,8,9,10]

n_channels = 10
seq_len = 600
# Specify if you are using accelerations =1 or displacements =0
accelerations = 0


latent_dim = 4

# Questa funzione calcola il valore quadratico medio (RMS) di un vettore di segnale, utile per misurare l'energia del segnale e per 
# aggiungere rumore durante la corruzione dei segnali.
def RMS(vect):
    return np.sqrt(np.mean(np.square(vect)))


# ----------------------------------------------------------------------------

"""
## Importazione dati
"""
# addedd_SNR = 100

# Read data and create dataset
def read_data(path_data, train):    
    label_path_class = path_data + '/Damage_class.csv'                                
    labels_class     = np.genfromtxt(label_path_class)
    labels_class     = labels_class.astype('int')
    label_path_level = path_data + '/Damage_level.csv'                                
    labels_level     = np.genfromtxt(label_path_level)
    
    # Filtra gli indici con damage_class diversa da 0
    valid_indices = np.arange(len(labels_class))
    N_ist = len(valid_indices)  # Usa solo gli indici validi

    # Importiamo i valori generati dal VAE
    if train:
        path_data_z = os.path.join(path_z, "z_train.csv")
    else:
        path_data_z = os.path.join(path_z, "z_test.csv")
    
    Z = pd.read_csv(path_data_z, header=0).to_numpy()


    # Creazione dei vettori di label di 7 elementi
    labels_vector = np.zeros((N_ist, n_class))  # Vettore di etichette a 7 classi
    
    for i, val in enumerate(labels_class):
        damage_class = val - 1  # Mappiamo la classe 1 su 0, 2 su 1, ecc.
        damage_level = labels_level[i]
        
        if damage_class == -1:  # Se damage_class è 0 (mappato a -1)
            labels_vector[i] = np.zeros(n_class)  # Imposta il vettore con tutti gli zeri
        else:
            labels_vector[i][damage_class] = damage_level #- 0.2  # Inserisci il damage_level nella posizione corretta

    return Z, labels_vector, N_ist


# ----------------------------------------------------------------------------------------------------------------------


"""
## Affine coupling layer
"""

# Creating a custom layer with keras API.
output_dim = 256  # Opzioni 256, 512, 1024
reg = 0.01

# Number of classes in the dataset
n_class = 6   #Ho solo dati di damage_class > 0


# Hyperparameters
validation_split = 0.20
batch_size = 32
n_epochs = 250
early_stop_epochs=15
initial_lr = 1e-3
decay_length = 0.8
ratio_to_stop = 0.05

filter_1      = 32;   filter_2      = 64;   filter_3      = 32
kernel_size_1 = 25;   kernel_size_2 =  13;   kernel_size_3 = 7
neurons_4 = 64
neurons_5 = 16
attivaz_conv = 'tanh'
attivaz_mlp = 'tanh'
k_reg = 1e-3
b_reg = 1e-3
rate_drop = 0.05





def Coupling(input_shape=latent_dim, num_classes=n_class, reg=0.005, dropout_rate=0.1):
    # Input per il vettore latente Z di dimensione 4
    input_z = layers.Input(shape=(input_shape,), name='Input_Z')
    # Input per il vettore di label condizionale di dimensione num_classes
    input_l = layers.Input(shape=(num_classes,), name='Input_Label')

    # Network per condizionare s e t in funzione del label y
    label_condition = layers.Dense(32, activation='relu')(input_l)
    label_condition = layers.Dense(64, activation='relu')(label_condition)
    label_embedding = layers.Dense(128, activation='relu')(label_condition) 

    # Livelli densi per la componente t (traslazione) con condizionamento MLP
    t = layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(reg))(input_z)
    t = layers.Dropout(dropout_rate)(t)  # Aggiungi Dropout
    t_embedding = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
    t = layers.Concatenate()([t, t_embedding])  # Condizionamento con il label embedding
    t = layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(reg))(t)
    t = layers.Dense(latent_dim, activation="linear", kernel_regularizer=regularizers.l2(reg))(t)

    # Livelli densi per la componente s (scalatura) con condizionamento MLP
    s = layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(reg))(input_z)
    s = layers.Dropout(dropout_rate)(s)  # Aggiungi Dropout
    s_embedding = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
    s = layers.Concatenate()([s, s_embedding])  # Condizionamento con il label embedding
    s = layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(reg))(s)
    s = layers.Dense(latent_dim, activation="tanh", kernel_regularizer=regularizers.l2(reg))(s)

    return keras.Model(inputs=[input_z, input_l], outputs=[s, t])





    

"""
## Real NVP
"""


class RealNVP(keras.Model):
    def __init__(self, num_coupling_layers, num_classes):
        super().__init__()

        self.num_coupling_layers = num_coupling_layers
        self.num_classes = num_classes

        # Distribuzione dello spazio latente di dim 4
        self.distribution = tfp.distributions.MultivariateNormalDiag(
            loc=[0.0] * 4, scale_diag=[1.0] * 4  # La distribuzione ha 4 dimensioni
        )

        # Maschere (alterniamo metà delle dimensioni: 2 su 4)
        self.masks = tf.convert_to_tensor(
            [[0] * 2 + [1] * 2, [1] * 2 + [0] * 2] * (self.num_coupling_layers // 2), dtype=tf.float32
        )


        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.log_likelihood_tracker = keras.metrics.Mean(name="log_likelihood")
        self.log_det_inv_tracker = keras.metrics.Mean(name="log_det_inv")

        self.layers_list = [Coupling(input_shape=latent_dim, num_classes=n_class) for i in range(num_coupling_layers)]
    
    @property
    def metrics(self):
        return [self.loss_tracker,
                self.log_likelihood_tracker,
                self.log_det_inv_tracker]


    # call con struttura iniziale 
    def call(self, x, y, training=True):  

        log_det_inv = 0   

        for i in range(self.num_coupling_layers):

            mask = tf.reshape(self.masks[i], [1, 4])  # Aggiungi una dimensione batch
            x_masked = x * mask  # Maschera l'input
            reversed_mask = 1 - mask
            s, t = self.layers_list[i]([x_masked, y])
            s *= reversed_mask
            t *= reversed_mask

            x = (
                reversed_mask * (x * tf.exp(s) +  t )
                + x_masked
            )
            log_det_inv += tf.reduce_sum(s, axis=1)

        return x, log_det_inv
    

    def compute_loss(self, x, y):

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y)

        # Calcola la log-likelihood condizionata
        log_likelihood_z = self.distribution.log_prob(z_pred) 

        return tf.reduce_mean(log_likelihood_z), tf.reduce_mean(log_det_inv)


    

    def train_step(self, data):
        x, y = data  # Estraiamo i dati e le etichette
        with tf.GradientTape() as tape:
            log_likelihood_z, log_det_inv = self.compute_loss(x, y)

            # Total loss is the sum of log-likelihood and the log-det term
            loss = -(log_likelihood_z + log_det_inv)

        # Calcola i gradienti
        grads = tape.gradient(loss, self.trainable_variables)
        # Applica i gradienti
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        # Aggiorna lo stato del tracker della perdita
        self.loss_tracker.update_state(loss)
        # Aggiorna i tracker delle componenti
        self.log_likelihood_tracker.update_state(log_likelihood_z)
        self.log_det_inv_tracker.update_state(log_det_inv)

        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }



    def test_step(self, data):
        x, y = data
        log_likelihood_z, log_det_inv = self.compute_loss(x, y)
        loss = -(log_likelihood_z + log_det_inv)

        self.loss_tracker.update_state(loss)
        self.log_likelihood_tracker.update_state(log_likelihood_z)
        self.log_det_inv_tracker.update_state(log_det_inv)
        
        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }


    def predict(self, x, y):
        
        # Assicurati che x e y siano TensorFlow tensors
        x = tf.convert_to_tensor(x, dtype=tf.float32)
        y = tf.convert_to_tensor(y, dtype=tf.float32)

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y, training=False)

        # Calcola la log-likelihood condizionata
        log_likelihood_z = self.distribution.log_prob(z_pred)

        log_likelihood = log_likelihood_z + log_det_inv

        return log_likelihood

    @tf.function(jit_compile=True)  # Abilita XLA per la compilazione JIT
    def predict_per_uno(self, x, y):
        # Evita la conversione ridondante se x e y sono già tensori
        if not isinstance(x, tf.Tensor):
            x = tf.convert_to_tensor(x, dtype=tf.float32)
            x.set_shape([EXPECTED_SHAPE_X])  # Specifica la forma statica di x
        if not isinstance(y, tf.Tensor):
            y = tf.convert_to_tensor(y, dtype=tf.float32)
            y.set_shape([EXPECTED_SHAPE_Y])  # Specifica la forma statica di y

        # Disabilita la tracciatura del gradiente per migliorare l'efficienza
        with tf.GradientTape(persistent=False, watch_accessed_variables=False) as tape:
            tape.stop_recording()  # Disabilita il calcolo dei gradienti

            # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
            z_pred, log_det_inv = self(x, y, training=False)

            # Calcola la log-likelihood condizionata
            log_likelihood_z = self.distribution.log_prob(z_pred)
            log_likelihood = log_likelihood_z + log_det_inv

        # Restituisci il risultato scalato come richiesto
        return 8 * log_likelihood




"""
## Funzioni utili
"""



        

train_or_predict = 0   # 1: training, 0: predict                                       


if train_or_predict:
    """
    ## Model training
    """

    Z_train, labels_train, N_ist_train = read_data(path_data_train, 1)

    print(f"Forma di Z_train: {Z_train.shape}")
    print(f"Forma di labels_train: {labels_train.shape}")

    print("Prime 2 righe di Z_train:")
    print(Z_train[:2])


    NVP = RealNVP(num_coupling_layers=16, num_classes=n_class)

    NVP.compile(optimizer=keras.optimizers.Adam(learning_rate=keras.optimizers.schedules.CosineDecay(initial_learning_rate=initial_lr, decay_steps=int(decay_length*n_epochs*N_ist_train*(1-validation_split)/batch_size), alpha=ratio_to_stop)))
    NVP.summary()


    dataset = tf.data.Dataset.from_tensor_slices((Z_train, labels_train))


    # Calcola il numero di batch di validazione
    validation_split = 0.2
    dataset_size = len(Z_train)
    validation_size = int(dataset_size * validation_split)
    train_size = dataset_size - validation_size

    # Dividi il dataset in train e validation
    train_dataset = dataset.take(train_size).batch(batch_size)
    validation_dataset = dataset.skip(train_size).take(validation_size).batch(batch_size)


    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stop_epochs, restore_best_weights=True)

    history = NVP.fit(
        train_dataset, epochs=n_epochs, verbose=2, validation_data=validation_dataset, callbacks=[early_stop]
    )


    # Salva lo storico dell'addestramento in formato pickle
    hist = pd.DataFrame(history.history)  # Converti history.history (dati storici dell'addestramento) in un DataFrame di pandas
    hist['epoch'] = history.epoch  # Aggiungi una colonna per le epoche. history.epoch è una lista che contiene i numeri delle epoche per cui sono stati registrati i dati.

    # Salva il DataFrame
    try:
        hist.to_pickle(os.path.join(path_save, 'hist_NVP_for_MCMC.pkl'))   #Salva il DataFrame hist in un file con formato pickle.
        print(f"Storico salvato in {os.path.join(path_save, 'hist_NVP_for_MCMC.pkl')}")
    except Exception as e:
        print(f"Errore durante il salvataggio dello storico: {e}")


    # Salva i pesi
    NVP.save_weights('./models/NVP_for_MCMC.weights.h5')


    """
    ## Performance evaluation
    """
    # Questa porzione del codice genera un grafico dell'andamento della perdita (\textit{loss}) durante il processo di addestramento del modello, 
    # per entrambi i set di dati di addestramento e di validazione.

    # Crea la cartella se non esiste
    output_dir = "output_NVP_for_MCMC"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Plot combinato per la training loss e la validation loss
    plt.figure(figsize=(15, 5))  # Dimensioni del plot
    plt.plot(history.history["loss"], label="Training Loss", color='blue')
    plt.plot(history.history["val_loss"], label="Validation Loss", color='orange')
    plt.title("Training and Validation Loss")
    plt.ylabel("Loss")
    plt.xlabel("Epoch")
    plt.legend(loc="upper right")
    plt.grid(True)
    plt.savefig(os.path.join(output_dir, "losses.png"))
    plt.show()
    plt.close()


    #Plot True/Wrong loglikelihood
    labels_train_wrong = np.random.permutation(labels_train)

    log_likelihood_test = NVP.predict(Z_train, labels_train)
    log_likelihood_wrong = NVP.predict(Z_train, labels_train_wrong)

    print(f"Forma di log_likelihood_test: {log_likelihood_test.shape}")
    print(f"Forma di log_likelihood_wrong: {log_likelihood_wrong.shape}")
    print(f"Forma di labels_train_truncated: {labels_train_wrong.shape}")


    # Converti i tensori in array NumPy
    log_likelihood_wrong_np = log_likelihood_wrong.numpy()
    log_likelihood_test_np = log_likelihood_test.numpy()


    # Crea la figura
    plt.figure(figsize=(8, 6))

    # Istogramma del vettore log_likelihood_train
    plt.hist(log_likelihood_wrong_np, bins=100, range=(2*min(log_likelihood_test_np), 2*max(log_likelihood_test_np)),
            alpha=0.9, color='r', label='Wrong')

    # Istogramma del vettore log_likelihood_test
    plt.hist(log_likelihood_test_np, bins=100, range=(2*min(log_likelihood_test_np), 2*max(log_likelihood_test_np)),
            alpha=0.8, color='b', label='Right')

    # Titolo e etichette
    plt.xlabel('Log-likelihood', fontsize=20)
    plt.ylabel('Frequency', fontsize=20)
    plt.grid(True)

    # Aggiungi una legenda
    plt.legend(fontsize=18)

    # Salva il grafico in un file immagine
    plt.savefig(os.path.join(output_dir, "hist_confronto_2.png"))

    # Mostra il grafico
    plt.show()



   
else:
    """
    ## Predict
    """
    output_dir = "output_NVP_for_MCMC"

    # Caricamento dei dati di test
    Z_test, labels_test, N_ist_test = read_data(path_data_test, 0)

    print(f"Forma di Z_test: {Z_test.shape}")

    Z_train, labels_train, N_ist_train = read_data(path_data_train, 1)

    print(f"Forma di Z_train: {Z_train.shape}")


    # Creazione dell'istanza RealNVP
    NVP = RealNVP(num_coupling_layers=16, num_classes=n_class)

    # NVP.compile(optimizer = keras.optimizers.Adam(learning_rate=keras.optimizers.schedules.CosineDecay(initial_learning_rate=initial_lr, decay_steps=int(decay_length*n_epochs*N_ist_test*(1-validation_split)/batch_size), alpha=ratio_to_stop)))
    #Non serve compilare se non devo continuare il training

    NVP.build([(None, latent_dim), (None, n_class)])

    # Caricamento dei pesi salvati
    NVP.load_weights('./models/NVP_for_MCMC.weights.h5')

    # Caricamento della cronologia dell'addestramento
    hist = pd.read_pickle(path_save + 'hist_NVP_for_MCMC.pkl')


    # import time

    # # Selezione casuale di 3 indici da Z_test
    # random_indices = np.random.choice(Z_test.shape[0], size=2, replace=False)

    # # Inizio del timer per i 3 campioni
    # start_time = time.time()

    # # Predizione per i 3 campioni selezionati
    # log_likelihood_random = NVP.predict(Z_test[random_indices], labels_test[random_indices])

    # # Fine del timer e calcolo del tempo
    # end_time = time.time()
    # random_time = end_time - start_time
    # print(f"Tempo di esecuzione per 3 campioni casuali: {random_time} secondi")



    # # Selezione casuale di 3 indici da Z_test
    # random_indices = np.random.choice(Z_test.shape[0], size=2, replace=False)

    # # Inizio del timer per i 3 campioni
    # start_time = time.time()

    # # Predizione per i 3 campioni selezionati
    # log_likelihood_random = NVP.predict(Z_test[random_indices], labels_test[random_indices])

    # # Fine del timer e calcolo del tempo
    # end_time = time.time()
    # random_time = end_time - start_time
    # print(f"Tempo di esecuzione per 3 campioni casuali: {random_time} secondi")



    # # Inizio del timer per l'intero dataset
    # start_time = time.time()

    # # Esecuzione del modello NVP sul dataset intero
    # log_likelihood_test = NVP.predict(Z_test, labels_test)

    # # Fine del timer e calcolo del tempo
    # end_time = time.time()
    # total_time = end_time - start_time
    # print(f"Tempo di esecuzione per l'intero dataset: {total_time} secondi")




    # # Selezione casuale di 3 indici da Z_test
    # random_indices = np.random.choice(Z_test.shape[0], size=1, replace=False)

    # # Inizio del timer per i 3 campioni
    # start_time = time.time()

    # # Predizione per i 3 campioni selezionati
    # log_likelihood_random = NVP.predict_per_uno(Z_test[random_indices], labels_test[random_indices])

    # # Fine del timer e calcolo del tempo
    # end_time = time.time()
    # random_time = end_time - start_time
    # print(f"Tempo di esecuzione per 3 campioni casuali: {random_time} secondi")




    # # Selezione casuale di 3 indici da Z_test
    # random_indices = np.random.choice(Z_test.shape[0], size=1, replace=False)

    # # Inizio del timer per i 3 campioni
    # start_time = time.time()

    # # Predizione per i 3 campioni selezionati
    # log_likelihood_random = NVP.predict_per_uno(Z_test[random_indices], labels_test[random_indices])

    # # Fine del timer e calcolo del tempo
    # end_time = time.time()
    # random_time = end_time - start_time
    # print(f"Tempo di esecuzione per 3 campioni casuali: {random_time} secondi")









    log_likelihood_test = NVP.predict(Z_test, labels_test)
    log_likelihood_train = NVP.predict(Z_train, labels_train)

    print(f"Forma di log_likelihood_test: {log_likelihood_test.shape}")
    print(f"Forma di log_likelihood_train: {log_likelihood_train.shape}")


    # Converti i tensori in array NumPy
    log_likelihood_train_np = log_likelihood_train.numpy()
    log_likelihood_test_np = log_likelihood_test.numpy()


    # Crea la figura
    plt.figure(figsize=(8, 6))

    # Istogramma del vettore log_likelihood_train
    plt.hist(log_likelihood_train_np, bins=100, range=(min(log_likelihood_train_np), max(log_likelihood_train_np)),
            alpha=0.9, color='b', label='Train')

    # Istogramma del vettore log_likelihood_test
    plt.hist(log_likelihood_test_np, bins=100, range=(min(log_likelihood_test_np), max(log_likelihood_test_np)),
            alpha=0.8, color='r', label='Test')

    # Titolo e etichette
    plt.title('Istogramma sovrapposto delle log-likelihood')
    plt.xlabel('log-likelihood')
    plt.ylabel('Frequenza normalizzata')
    plt.grid(True)

    # Aggiungi una legenda
    plt.legend()

    # Salva il grafico in un file immagine
    plt.savefig(os.path.join(output_dir, "hist_confronto.png"))

    # Mostra il grafico
    plt.show()



    #CONFRONTO CLASSE GIUSTA MA LABEL SBAGLIATO
    # Ottieni le classi previste

    labels_test_wrong = labels_train[:4000,:]

    log_likelihood_test = NVP.predict(Z_test, labels_test)
    log_likelihood_wrong = NVP.predict(Z_test, labels_test_wrong)

    print(log_likelihood_test.shape)
    print(log_likelihood_wrong.shape)

    # Converti i tensori in array NumPy
    log_likelihood_wrong_np = log_likelihood_wrong.numpy()
    log_likelihood_test_np = log_likelihood_test.numpy()


    # Crea la figura
    plt.figure(figsize=(8, 6))

    # Istogramma del vettore log_likelihood_train
    plt.hist(log_likelihood_wrong_np, bins=100, range=(-150, max(log_likelihood_wrong_np)),
            alpha=0.9, color='r', label='Wrong')

    # Istogramma del vettore log_likelihood_test
    plt.hist(log_likelihood_test_np, bins=100, range=(2*min(log_likelihood_test_np), max(log_likelihood_test_np)),
            alpha=0.8, color='b', label='Right')

    # Titolo e etichette
    plt.xlabel('Log-likelihood', fontsize=20)
    plt.ylabel('Frequency', fontsize=20)
    plt.grid(True)

    # Aggiungi una legenda
    plt.legend(fontsize=18)

    # Salva il grafico in un file immagine
    plt.savefig(os.path.join(output_dir, "hist_confronto_2.png"))

    # Mostra il grafico
    plt.show()






    



    





    



"""
Title: MCMC NVP No Damage
"""

import os

os.environ["KERAS_BACKEND"] = "tensorflow"

# Imposta la variabile d'ambiente
os.environ['OMP_NUM_THREADS'] = '1'

import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras import regularizers
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_probability as tfp
import pandas as pd
import pymc as pm

from keras import ops
from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_absolute_error
import seaborn as sns

import scipy.stats as st

import arviz as az



EXPECTED_SHAPE_X = [None, 4]  # La dimensione batch può variare, ma x ha sempre 4 elementi
EXPECTED_SHAPE_Y = [None, 6]  # La dimensione batch può variare, ma y ha sempre 7 elementi

import multiprocessing











# Specify the example you are dealing with
esempio = 'TRAIN_FRAME_DT'
# Training and Testing data
ID              = '2_1'
save_ID         = '/Classifier_total_2_1/'
path            = "../" + esempio 
path_data_train   = path + '/istantrain_' + ID
path_data_test   = path + '/istantest_' + ID
# Saving model
path_save       = "models/"
# Prediction model
restore_ID = '/Classifier_total_2_1/'
path_restore = path + restore_ID

path_z = "output_VAE_for_MCMC"

# Which dof monitored
which_channels = [1,2,3,4,5,6,7,8,9,10]

n_channels = 10
seq_len = 600
# Specify if you are using accelerations =1 or displacements =0
accelerations = 0


latent_dim = 4

# Number of classes in the dataset
n_class = 6



# Read data and create dataset
def read_data(path_data, train):    
    label_path_class = path_data + '/Damage_class.csv'                                
    labels_class     = np.genfromtxt(label_path_class)
    labels_class     = labels_class.astype('int')
    label_path_level = path_data + '/Damage_level.csv'                                
    labels_level     = np.genfromtxt(label_path_level)
    
    # Filtra gli indici con damage_class diversa da 0
    valid_indices = np.arange(len(labels_class))
    N_ist = len(valid_indices)  # Usa solo gli indici validi

    # Importiamo i valori generati dal VAE
    if train:
        path_data_z = os.path.join(path_z, "z_train2.csv")
    else:
        path_data_z = os.path.join(path_z, "z_test2.csv")
    
    Z = pd.read_csv(path_data_z, header=0)


    # Creazione dei vettori di label di 7 elementi
    labels_vector = np.zeros((N_ist, n_class))  # Vettore di etichette a 7 classi
    
    for i, val in enumerate(labels_class):
        damage_class = val - 1  # Mappiamo la classe 1 su 0, 2 su 1, ecc.
        damage_level = labels_level[i]
        
        if damage_class == -1:  # Se damage_class è 0 (mappato a -1)
            labels_vector[i] = np.zeros(n_class)  # Imposta il vettore con tutti gli zeri
        else:
            labels_vector[i][damage_class] = damage_level #- 0.2  # Inserisci il damage_level nella posizione corretta

    return Z, labels_vector, N_ist


#---------------------------------------------------------------------------------------------------------------

"""
## Affine coupling layer
"""

# Creating a custom layer with keras API.
output_dim = 256  # Opzioni 256, 512, 1024
reg = 0.01


# Hyperparameters
validation_split = 0.20
batch_size = 32
n_epochs = 250
early_stop_epochs=15
initial_lr = 1e-3
decay_length = 0.8
ratio_to_stop = 0.05

filter_1      = 32;   filter_2      = 64;   filter_3      = 32
kernel_size_1 = 25;   kernel_size_2 =  13;   kernel_size_3 = 7
neurons_4 = 64
neurons_5 = 16
attivaz_conv = 'tanh'
attivaz_mlp = 'tanh'
k_reg = 1e-3
b_reg = 1e-3
rate_drop = 0.05




def Coupling(input_shape=latent_dim, num_classes=n_class, reg=0.005, dropout_rate=0.1):
    # Input per il vettore latente Z di dimensione 4
    input_z = layers.Input(shape=(input_shape,), name='Input_Z')
    # Input per il vettore di label condizionale di dimensione num_classes
    input_l = layers.Input(shape=(num_classes,), name='Input_Label')

    # Network per condizionare s e t in funzione del label y
    label_condition = layers.Dense(32, activation='relu')(input_l)
    label_condition = layers.Dense(64, activation='relu')(label_condition)
    label_embedding = layers.Dense(128, activation='relu')(label_condition) 

    # Livelli densi per la componente t (traslazione) con condizionamento MLP
    t = layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(reg))(input_z)
    t = layers.Dropout(dropout_rate)(t)  # Aggiungi Dropout
    t_embedding = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
    t = layers.Concatenate()([t, t_embedding])  # Condizionamento con il label embedding
    t = layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(reg))(t)
    t = layers.Dense(latent_dim, activation="linear", kernel_regularizer=regularizers.l2(reg))(t)

    # Livelli densi per la componente s (scalatura) con condizionamento MLP
    s = layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(reg))(input_z)
    s = layers.Dropout(dropout_rate)(s)  # Aggiungi Dropout
    s_embedding = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(reg))(label_embedding)
    s = layers.Concatenate()([s, s_embedding])  # Condizionamento con il label embedding
    s = layers.Dense(64, activation="relu", kernel_regularizer=regularizers.l2(reg))(s)
    s = layers.Dense(latent_dim, activation="tanh", kernel_regularizer=regularizers.l2(reg))(s)

    return keras.Model(inputs=[input_z, input_l], outputs=[s, t])



"""
## Real NVP
"""

class RealNVP(keras.Model):
    def __init__(self, num_coupling_layers, num_classes):
        super().__init__()

        self.num_coupling_layers = num_coupling_layers
        self.num_classes = num_classes

        # Distribuzione dello spazio latente di dim 4
        self.distribution = tfp.distributions.MultivariateNormalDiag(
            loc=[0.0] * 4, scale_diag=[1.0] * 4  # La distribuzione ha 4 dimensioni
        )

        # Maschere (alterniamo metà delle dimensioni: 2 su 4)
        self.masks = tf.convert_to_tensor(
            [[0] * 2 + [1] * 2, [1] * 2 + [0] * 2] * (self.num_coupling_layers // 2), dtype=tf.float32
        )


        self.loss_tracker = keras.metrics.Mean(name="loss")
        self.log_likelihood_tracker = keras.metrics.Mean(name="log_likelihood")
        self.log_det_inv_tracker = keras.metrics.Mean(name="log_det_inv")

        self.layers_list = [Coupling(input_shape=latent_dim, num_classes=n_class) for i in range(num_coupling_layers)]
    
    @property
    def metrics(self):
        return [self.loss_tracker,
                self.log_likelihood_tracker,
                self.log_det_inv_tracker]


    # call con struttura iniziale 
    def call(self, x, y, training=True):  

        log_det_inv = 0   

        for i in range(self.num_coupling_layers):

            mask = tf.reshape(self.masks[i], [1, 4])  # Aggiungi una dimensione batch
            x_masked = x * mask  # Maschera l'input
            reversed_mask = 1 - mask
            s, t = self.layers_list[i]([x_masked, y])
            s *= reversed_mask
            t *= reversed_mask

            x = (
                reversed_mask * (x * tf.exp(s) +  t )
                + x_masked
            )
            log_det_inv += tf.reduce_sum(s, axis=1)

        return x, log_det_inv
    

    def compute_loss(self, x, y):

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y)

        # Calcola la log-likelihood condizionata
        log_likelihood_z = self.distribution.log_prob(z_pred) 

        return tf.reduce_mean(log_likelihood_z), tf.reduce_mean(log_det_inv)


    

    def train_step(self, data):
        x, y = data  # Estraiamo i dati e le etichette
        with tf.GradientTape() as tape:
            log_likelihood_z, log_det_inv = self.compute_loss(x, y)

            # Total loss is the sum of log-likelihood and the log-det term
            loss = -(log_likelihood_z + log_det_inv)

        # Calcola i gradienti
        grads = tape.gradient(loss, self.trainable_variables)
        # Applica i gradienti
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        # Aggiorna lo stato del tracker della perdita
        self.loss_tracker.update_state(loss)
        # Aggiorna i tracker delle componenti
        self.log_likelihood_tracker.update_state(log_likelihood_z)
        self.log_det_inv_tracker.update_state(log_det_inv)

        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }



    def test_step(self, data):
        x, y = data
        log_likelihood_z, log_det_inv = self.compute_loss(x, y)
        loss = -(log_likelihood_z + log_det_inv)

        self.loss_tracker.update_state(loss)
        self.log_likelihood_tracker.update_state(log_likelihood_z)
        self.log_det_inv_tracker.update_state(log_det_inv)
        
        return {
            "loss": self.loss_tracker.result(),
            "log_likelihood": self.log_likelihood_tracker.result(),
            "log_det_inv": self.log_det_inv_tracker.result(),
        }


    def predict(self, x, y):
        # Assicurati che x e y siano TensorFlow tensors
        x = tf.convert_to_tensor(x, dtype=tf.float32)
        y = tf.convert_to_tensor(y, dtype=tf.float32)

        # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
        z_pred, log_det_inv = self(x, y, training=False)

        # Calcola la log-likelihood condizionata
        log_likelihood_z = self.distribution.log_prob(z_pred)

        log_likelihood = log_likelihood_z + log_det_inv

        return log_likelihood


    @tf.function(jit_compile=True)  # Abilita XLA per la compilazione JIT
    def predict_per_uno(self, x, y):
        # # Evita la conversione ridondante se x e y sono già tensori
        # if not isinstance(x, tf.Tensor):
        #     x = tf.convert_to_tensor(x, dtype=tf.float32)
        #     x.set_shape([EXPECTED_SHAPE_X])  # Specifica la forma statica di x
        # if not isinstance(y, tf.Tensor):
        #     y = tf.convert_to_tensor(y, dtype=tf.float32)
        #     y.set_shape([EXPECTED_SHAPE_Y])  # Specifica la forma statica di y

        x = tf.convert_to_tensor(x, dtype=tf.float64)
        y = tf.convert_to_tensor(y, dtype=tf.float64)

        # Disabilita la tracciatura del gradiente per migliorare l'efficienza
        with tf.GradientTape(persistent=False, watch_accessed_variables=False) as tape:
            tape.stop_recording()  # Disabilita il calcolo dei gradienti

            # Applica il modello per ottenere z_pred e il log-determinante del Jacobiano
            z_pred, log_det_inv = self(x, y, training=False)

            # Calcola la log-likelihood condizionata
            log_likelihood_z = self.distribution.log_prob(z_pred)
            log_likelihood = log_likelihood_z + log_det_inv

        # Restituisci il risultato scalato come richiesto
        return 8 * log_likelihood


#---------------------------------------------------------------------------------------------------------------------------------

# Creazione dell'istanza RealNVP
NVP = RealNVP(num_coupling_layers=16, num_classes=n_class)

NVP.build([(None, latent_dim), (None, n_class)])

# Caricamento dei pesi salvati
NVP.load_weights('./models/NVP_for_MCMC.weights.h5')





import pytensor.tensor as pt
from pytensor.graph.op import Op
from pytensor.graph.basic import Apply
import time

# Definisci l'Op personalizzata che esegue la chiamata al modello RealNVP con TensorFlow
class RealNVPTF(Op):
    def __init__(self, nvp_model):
        self.nvp_model = nvp_model  # Passa il modello NVP come attributo

    #make_node consente di definire come i dati entrano ed escono dal grafo computazionale e garantendo la compatibilità con i requisiti di PyMC.
    def make_node(self, data, label_vector):   
        # Forza il tipo di dati float64 per assicurarsi che PyMC riceva il tipo corretto
        data = pt.as_tensor_variable(data.astype(np.float64))
        label_vector = pt.as_tensor_variable(label_vector.astype(np.float64))
        return Apply(self, [data, label_vector], [pt.dscalar()])  # Restituisci uno scalare come output

    def perform(self, node, inputs, outputs):
        # Ricevi i numpy array e forza float64 per ogni operazione
        data, label_vector = inputs

        # # Imposta a 0 i valori tra 0 e 0.05 nel vettore delle label
        # label_vector[label_vector < 0.3] = 0.0

        # Assicurati che anche l'input al modello NVP sia in formato float64
        label_vector_eval = np.expand_dims(label_vector, axis=0).astype(np.float64)  # (batch_size, n_class)

        # Esegui il modello NVP e ottieni la log-likelihood in float64
        # log_prob = self.nvp_model.predict(data.astype(np.float64), label_vector_eval.astype(np.float64))
        log_prob = self.nvp_model.predict_per_uno(data.astype(np.float64), label_vector_eval.astype(np.float64))

        # Assegna l'output con la giusta precisione
        outputs[0][0] = np.array(log_prob, dtype=np.float64)  # Forza l'output come float64


# Crea un'istanza dell'Op personalizzata con il modello NVP
real_nvp_tf_op = RealNVPTF(NVP)

# Definisci la funzione di log-likelihood che userai in PyMC
def realnvp_logp(label_vector, z_mean, sigma):
    # Conversione esplicita in TensorVariable
    z_mean = pt.as_tensor_variable(z_mean, dtype='float64')
    sigma = pt.as_tensor_variable(sigma, dtype='float64')

    # Genera un array di numeri casuali da una distribuzione normale
    sampled_data = pt.random.normal(loc=z_mean, scale=sigma, size=(z_mean.shape[0],))

    # Converte i dati e label_vector in float64 prima di passare all'Op personalizzato
    return real_nvp_tf_op(sampled_data.astype(np.float64), label_vector.astype(np.float64))


# Funzione che esegue l'inferenza MCMC usando Metropolis-Hastings
def mcmc_classification_pymc(z_mean, z_log_var, num_iterations):
    with pm.Model() as model:
        # Definiamo una distribuzione a priori per il vettore di label
        # label_vector = pm.Normal("label_vector", mu=0, sigma=1, shape=(n_class,))
        label_vector = pm.Uniform('label_vector', lower=0, upper=0.8, shape=(n_class,))
        # label_vector = pm.Beta('label_vector', alpha=2, beta=5, shape=(n_class,))
        # label_vector = pm.Exponential('label_vector', lam=1/0.7, shape=(n_class,))

        # Calcola la deviazione standard dalla log-varianza
        sigma = pm.math.exp(0.5 * z_log_var)  # Deviance standard
        # Genera i nuovi dati campionando dalla distribuzione normale
        # sampled_data = pm.Normal('sampled_data', mu=z_mean, sigma=sigma, shape=(len(z_mean),))

        # Usa pm.Potential per integrare la log-likelihood calcolata esternamente
        pm.Potential("realnvp_loglikelihood", realnvp_logp(label_vector, z_mean, sigma))

        # Eseguiamo l'inferenza MCMC usandoset Metropolis-Hastings
        # step = pm.Metropolis()  # Usa Metropolis-Hastings come algoritmo di campionamento
        # Usare DE-Metropolis come algoritmo di campionamento
        # step = pm.DEMetropolis()

        step = pm.DEMetropolisZ(scaling=0.1, tune_interval=200)
        trace = pm.sample(num_iterations, cores=1, chains=2, step=step, tune=20000 , return_inferencedata=True)

        # Riepilogo del campionamento
        summary = az.summary(trace)

        # Mostra la sintesi
        print(summary)

    # Restituisci i risultati del vettore continuo campionato
    return trace




#---------------------------------------------------------------------------------------------------------------------------------






#Importo i dati 
Z_test, labels, N_ist = read_data(path_data_test, 0)

print(f"Forma di Z_test: {Z_test.shape}")

print(f"Forma di labels: {labels.shape}")

print("Prime 2 righe di labels:")
print(labels[:2])




# Funzione per stimare la moda utilizzando gli istogrammi
def estimate_mode_from_histogram(samples, bins=50):
    # Calcola la moda per ogni feature
    mode_per_feature = np.zeros(samples.shape[0])
    
    for i in range(samples.shape[0]):
        hist, bin_edges = np.histogram(samples[i, :], bins=bins)
        mode_bin_index = np.argmax(hist)  # Indice del bin con la frequenza massima
        mode_per_feature[i] = 0.5 * (bin_edges[mode_bin_index] + bin_edges[mode_bin_index + 1])  # Media del bin
    return mode_per_feature


# Funzione per calcolare la deviazione standard dei campioni
def calculate_std(samples):
    return np.std(samples, axis=-1)


import csv

# Funzione che salva i risultati in un file CSV
# def append_results_to_csv(file_name, idx, true_label, label_estimate_mean, label_estimate_mode):
#     # Se il file non esiste, crealo e aggiungi l'intestazione
#     file_exists = os.path.isfile(file_name)

#     with open(file_name, mode='a', newline='') as file:
#         writer = csv.writer(file)
        
#         # Aggiungi intestazione solo se il file non esiste
#         if not file_exists:
#             writer.writerow(['Index', 'True Label', 'Mean Label Estimate', 'Mode Label Estimate'])
        
#         # Converti i vettori in stringhe separate da virgole
#         true_label_str = ','.join(map(str, true_label))
#         mean_label_str = ','.join(map(str, label_estimate_mean))
#         mode_label_str = ','.join(map(str, label_estimate_mode))
        
#         # Scrivi la riga completa nel CSV
#         writer.writerow([idx, true_label_str, mean_label_str, mode_label_str])


def calculate_hpdi(samples, alpha=0.05):

    # Numero di campioni
    n_samples = samples.shape[0]
    
    # Percentuale da includere nell'HPDI (es. 95% -> include 95% dei campioni)
    credible_mass = 1.0 - alpha
    
    # HPDI per ciascuna dimensione
    hpdi = np.zeros((2, samples.shape[1]))
    
    # Calcola l'HPDI per ogni dimensione del vettore di label
    for dim in range(samples.shape[1]):
        # Ordina i campioni per quella dimensione
        sorted_samples = np.sort(samples[:, dim])
        
        # Numero di campioni da includere nell'HPDI
        interval_idx_inc = int(np.floor(credible_mass * n_samples))
        
        # Trova l'ampiezza minima dell'intervallo
        interval_width = sorted_samples[interval_idx_inc:] - sorted_samples[:n_samples - interval_idx_inc]
        
        # Trova l'indice dell'intervallo più stretto
        min_idx = np.argmin(interval_width)
        
        # Assegna i limiti inferiore e superiore dell'HPDI
        hpdi[0, dim] = sorted_samples[min_idx]
        hpdi[1, dim] = sorted_samples[min_idx + interval_idx_inc]
    
    return hpdi


# Funzione che salva i risultati in un file CSV
def append_results_to_csv(file_name, idx, true_label, samples_mcmc):
    # Se il file non esiste, crealo e aggiungi l'intestazione
    file_exists = os.path.isfile(file_name)

    with open(file_name, mode='a', newline='') as file:
        writer = csv.writer(file)
        
        # Aggiungi intestazione solo se il file non esiste
        if not file_exists:
            writer.writerow(['Index', 'True Label', 'Mean Estimate', 'Mode Estimate', 'Standard Deviation', 'HPDI Lower', 'HPDI Upper'])

        # Calcola le statistiche dai campioni
        label_estimate_mean = np.mean(samples_mcmc, axis=-1)
        label_estimate_mode = estimate_mode_from_histogram(samples_mcmc)
        label_std_dev = calculate_std(samples_mcmc)
        # Trasponi l'array per avere la forma (2800, 7)
        samples_mcmc = samples_mcmc.T
        hpdi = calculate_hpdi(samples_mcmc)

        # Imposta a 0 i valori inferiori a 0.05 per mean e mode
        label_estimate_mean[label_estimate_mean < 0.3] = 0.0
        label_estimate_mode[label_estimate_mode < 0.3] = 0.0

        # # Calcola le metriche di performance
        # metrics = calculate_performance_metrics(true_label, label_estimate_mean, label_estimate_mode)
        
        # Converti i vettori in stringhe separate da virgole
        true_label_str = ','.join(map(str, true_label))
        mean_label_str = ','.join(map(str, label_estimate_mean))
        mode_label_str = ','.join(map(str, label_estimate_mode))
        std_dev_str = ','.join(map(str, label_std_dev))
        hpdi_lower_str = ','.join(map(str, hpdi[0, :]))
        hpdi_upper_str = ','.join(map(str, hpdi[1, :]))

        # Scrivi la riga completa nel CSV
        writer.writerow([idx, true_label_str, mean_label_str, mode_label_str, std_dev_str,
                         hpdi_lower_str, hpdi_upper_str])



# def append_results_to_csv(file_name, idx, true_label, label_estimate_mean, label_estimate_mode, standard_deviation, metrics):
#     # Se il file non esiste, crealo e aggiungi l'intestazione
#     file_exists = os.path.isfile(file_name)

#     with open(file_name, mode='a', newline='') as file:
#         writer = csv.writer(file)

#         # Aggiungi intestazione solo se il file non esiste
#         if not file_exists:
#             writer.writerow(['Index', 'True Label', 'Mean Estimate', 'Mode Estimate', 'Standard Deviation'
#                              'MAE Mean', 'MSE Mean', 'RMSE Mean', 'R² Mean', 'SMAPE Mean',
#                              'NRMSE Mean', 'Cosine Sim. Mean', 'Explained Variance Mean',
#                              'MAE Mode', 'MSE Mode', 'RMSE Mode', 'R² Mode', 'SMAPE Mode',
#                              'NRMSE Mode', 'Cosine Sim. Mode', 'Explained Variance Mode'])

#         # Scrivi la riga completa nel CSV con i risultati e le metriche
#         writer.writerow([idx, 
#                          ','.join(map(str, true_label)), 
#                          ','.join(map(str, label_estimate_mean)), 
#                          ','.join(map(str, label_estimate_mode)),
#                          ','.join(map(str, standard_deviation)),
#                          metrics['mae_mean'], metrics['mse_mean'], metrics['rmse_mean'], 
#                          metrics['r_squared_mean'], metrics['smape_mean'], metrics['nrmse_mean'], 
#                          metrics['cosine_sim_mean'], metrics['explained_variance_mean'],
#                          metrics['mae_mode'], metrics['mse_mode'], metrics['rmse_mode'], 
#                          metrics['r_squared_mode'], metrics['smape_mode'], metrics['nrmse_mode'], 
#                          metrics['cosine_sim_mode'], metrics['explained_variance_mode']])

# def update_performance_metrics_from_file(input_csv, output_csv):
#     # Leggi i dati dal file di input
#     df = pd.read_csv(input_csv)

#     # Per ogni riga del CSV
#     for idx, row in df.iterrows():
#         # Estrai i vettori di True Label, Mean Estimate e Mode Estimate
#         true_label = np.array([float(x) for x in row['True Label'].split(',')])
#         label_estimate_mean = np.array([float(x) for x in row['Mean Estimate'].split(',')])
#         label_estimate_mode = np.array([float(x) for x in row['Mode Estimate'].split(',')])
#         standard_deviation = np.array([float(x) for x in row['Standard Deviation'].split(',')])

#         # Calcola le metriche di performance
#         metrics = calculate_performance_metrics(true_label, label_estimate_mean, label_estimate_mode)

#         # Appendi i risultati aggiornati nel file di output
#         append_results_to_csv(output_csv, idx, true_label, label_estimate_mean, label_estimate_mode, standard_deviation, metrics)










# Funzione che calcola la classe predetta in base al valore massimo del vettore stimato
def get_predicted_class(estimated_label):
    # Trova l'indice con il valore massimo (classe predetta)
    return np.argmax(estimated_label)

# Funzione per creare confusion matrix e metriche
def create_confusion_matrix(csv_file):
    # Leggi il file CSV
    df = pd.read_csv(csv_file)
    
    # Converti le colonne 'True Label', 'Mean Label Estimate' e 'Mode Label Estimate' da stringhe a array di float
    df['True Label'] = df['True Label'].apply(lambda x: np.array(list(map(float, x.split(',')))))
    df['Mean Estimate'] = df['Mean Estimate'].apply(lambda x: np.array(list(map(float, x.split(',')))))
    df['Mode Estimate'] = df['Mode Estimate'].apply(lambda x: np.array(list(map(float, x.split(',')))))

    # Ottieni le classi vere e quelle stimate (media e moda)
    true_classes = df['True Label'].apply(get_predicted_class).values +1 
    mean_predicted_classes = df['Mean Estimate'].apply(get_predicted_class).values +1
    mode_predicted_classes = df['Mode Estimate'].apply(get_predicted_class).values +1

    # Calcola metriche per predizioni con media
    accuracy_mean = accuracy_score(true_classes, mean_predicted_classes)
    precision_mean = precision_score(true_classes, mean_predicted_classes, average='weighted')
    recall_mean = recall_score(true_classes, mean_predicted_classes, average='weighted')
    f1_mean = f1_score(true_classes, mean_predicted_classes, average='weighted')
    conf_matrix_mean = confusion_matrix(true_classes, mean_predicted_classes)

    # Calcola metriche per predizioni con moda
    accuracy_mode = accuracy_score(true_classes, mode_predicted_classes)
    precision_mode = precision_score(true_classes, mode_predicted_classes, average='weighted')
    recall_mode = recall_score(true_classes, mode_predicted_classes, average='weighted')
    f1_mode = f1_score(true_classes, mode_predicted_classes, average='weighted')
    conf_matrix_mode = confusion_matrix(true_classes, mode_predicted_classes)

    # Crea una figura con due sotto-figure
    fig, axs = plt.subplots(2, 2, figsize=(16, 12))

    # Plot della matrice di confusione per la media
    sns.heatmap(conf_matrix_mean, annot=True, fmt='d', cmap='Blues', ax=axs[0, 0])
    axs[0, 0].set_title('Confusion Matrix (Mean)')
    axs[0, 0].set_xlabel('Predicted Label')
    axs[0, 0].set_ylabel('True Label')

    # Aggiungi le metriche di classificazione per la media
    metrics_mean = (
        f"Accuracy: {accuracy_mean:.4f}\n"
        f"Precision: {precision_mean:.4f}\n"
        f"Recall: {recall_mean:.4f}\n"
        f"F1-Score: {f1_mean:.4f}"
    )
    axs[1, 0].text(0.5, 0.5, metrics_mean, fontsize=12, ha='center', va='center')
    axs[1, 0].set_axis_off()

    # Plot della matrice di confusione per la moda
    sns.heatmap(conf_matrix_mode, annot=True, fmt='d', cmap='Blues', ax=axs[0, 1])
    axs[0, 1].set_title('Confusion Matrix (Mode)')
    axs[0, 1].set_xlabel('Predicted Label')
    axs[0, 1].set_ylabel('True Label')

    # Aggiungi le metriche di classificazione per la moda
    metrics_mode = (
        f"Accuracy: {accuracy_mode:.4f}\n"
        f"Precision: {precision_mode:.4f}\n"
        f"Recall: {recall_mode:.4f}\n"
        f"F1-Score: {f1_mode:.4f}"
    )
    axs[1, 1].text(0.5, 0.5, metrics_mode, fontsize=12, ha='center', va='center')
    axs[1, 1].set_axis_off()

    # Aggiungi spazi tra le figure
    plt.tight_layout()

    # Salva il grafico in un file immagine
    plt.savefig("output_MCMC_NVP_2/Conf_Matrix_MCMC")

    plt.show()



def cosine_similarity_media(y_true_array, y_pred_array):
    cosine_similarities = []
    
    # Itera su ogni coppia di true label e predicted label
    for y_true, y_pred in zip(y_true_array, y_pred_array):
        dot_product = np.sum(y_true * y_pred)
        norm_true = np.linalg.norm(y_true)
        norm_pred = np.linalg.norm(y_pred)
        
        # Calcola la cosine similarity per la coppia corrente
        cosine_similarity = dot_product / (norm_true * norm_pred)
        cosine_similarities.append(cosine_similarity)
    
    # Restituisci la media delle cosine similarity
    return np.mean(cosine_similarities)


def calculate_metrics_mean(true_label_array, label_estimate_mean_array, label_estimate_mode_array):
    # Array per le metriche RMSE
    rmse_mean_array = []
    rmse_mode_array = []

    # Array per le metriche SMAPE
    smape_mean_array = []
    smape_mode_array = []

    # Array per le metriche NRMSE
    nrmse_mean_array = []
    nrmse_mode_array = []

    # Itera su ogni istanza del dataset
    for true_label, label_estimate_mean, label_estimate_mode in zip(true_label_array, label_estimate_mean_array, label_estimate_mode_array):
        
        # RMSE per ogni istanza
        mse_mean = np.mean((label_estimate_mean - true_label) ** 2)
        mse_mode = np.mean((label_estimate_mode - true_label) ** 2)
        rmse_mean = np.sqrt(mse_mean)
        rmse_mode = np.sqrt(mse_mode)
        rmse_mean_array.append(rmse_mean)
        rmse_mode_array.append(rmse_mode)

        # SMAPE per ogni istanza
        smape_mean = 100 * np.mean(2 * np.abs(label_estimate_mean - true_label) / (np.abs(label_estimate_mean) + np.abs(true_label)))
        smape_mode = 100 * np.mean(2 * np.abs(label_estimate_mode - true_label) / (np.abs(label_estimate_mode) + np.abs(true_label)))
        smape_mean_array.append(smape_mean)
        smape_mode_array.append(smape_mode)

        # NRMSE per ogni istanza
        nrmse_mean = rmse_mean / (np.max(true_label) - np.min(true_label))
        nrmse_mode = rmse_mode / (np.max(true_label) - np.min(true_label))
        nrmse_mean_array.append(nrmse_mean)
        nrmse_mode_array.append(nrmse_mode)

    # Calcolo della media delle metriche
    rmse_mean_avg = np.mean(rmse_mean_array)
    rmse_mode_avg = np.mean(rmse_mode_array)

    smape_mean_avg = np.mean(smape_mean_array)
    smape_mode_avg = np.mean(smape_mode_array)

    nrmse_mean_avg = np.mean(nrmse_mean_array)
    nrmse_mode_avg = np.mean(nrmse_mode_array)

    return rmse_mean_avg, rmse_mode_avg, smape_mean_avg, smape_mode_avg, nrmse_mean_avg, nrmse_mode_avg


def calcola_medie_normalizzate_dataset(vettori):
    media_vicini_normalizzata_totale = []
    media_altri_normalizzata_totale = []
    
    # Cicla su ogni vettore nel dataset
    for vettore in vettori:
        # Trova l'indice del valore massimo (classe predetta)
        indice_max = np.argmax(vettore)
        valore_max = vettore[indice_max]

        # Calcola la media dei due valori vicini
        if indice_max == 0:
            # Se il massimo è il primo elemento, prendi solo il secondo
            media_vicini = vettore[indice_max + 1]
        elif indice_max == len(vettore) - 1:
            # Se il massimo è l'ultimo elemento, prendi solo il penultimo
            media_vicini = vettore[indice_max - 1]
        else:
            # Altrimenti, prendi il valore precedente e successivo
            media_vicini = (vettore[indice_max - 1] + vettore[indice_max + 1]) / 2

        # Calcola la media degli altri quattro elementi
        altri_elementi = [vettore[i] for i in range(len(vettore)) if i not in [indice_max, indice_max - 1, indice_max + 1]]
        media_altri = np.mean(altri_elementi)

        # Normalizza le due medie rispetto al valore massimo
        media_vicini_normalizzata = media_vicini #/ valore_max
        media_altri_normalizzata = media_altri #/ valore_max

        # Aggiungi le medie normalizzate alle liste totali
        media_vicini_normalizzata_totale.append(media_vicini_normalizzata)
        media_altri_normalizzata_totale.append(media_altri_normalizzata)
    
    # Calcola la media tra tutti i vettori
    media_vicini_finale = np.mean(media_vicini_normalizzata_totale)
    media_altri_finale = np.mean(media_altri_normalizzata_totale)
    
    return media_vicini_finale, media_altri_finale




def analyze_damage_levels(csv_file_name):
    # Carica i dati dal file CSV
    df = pd.read_csv(csv_file_name)
    
    # Estrai le colonne necessarie
    true_labels = df['True Label'].apply(lambda x: np.fromstring(x, sep=',')).to_list()
    mean_labels = df['Mean Estimate'].apply(lambda x: np.fromstring(x, sep=',')).to_list()
    mode_labels = df['Mode Estimate'].apply(lambda x: np.fromstring(x, sep=',')).to_list()
    std_devs = df['Standard Deviation'].apply(lambda x: np.fromstring(x, sep=',')).to_list()

    # Converti in array NumPy per un'elaborazione più semplice
    true_labels = np.array(true_labels)
    mean_labels = np.array(mean_labels)
    mode_labels = np.array(mode_labels)
    std_devs = np.array(std_devs)

    # Funzione per estrarre il livello di danno dalla classe giusta
    def get_damage_level(label):
        # Controlla se tutte le classi sono sotto 0.3
        if np.all(label < 0.3):
            return 0, -1  # Nessun danno rilevante
        damage_index = np.argmax(label)  # Indice della classe con danno massimo
        return label[damage_index], damage_index

    # Analizza i livelli di danno
    true_damage_levels = np.array([get_damage_level(label)[0] for label in true_labels])  # Livelli di danno reali
    true_classes = np.array([get_damage_level(label)[1] for label in true_labels])  # Classi reali

    # Calcola i livelli di danno predetti
    predicted_damage_levels_mean = np.array([
        np.max(mean_labels[i]) if true_classes[i] == -1 else mean_labels[i][true_classes[i]]
        for i in range(len(mean_labels))
    ])

    predicted_damage_levels_mode = np.array([
        np.max(mode_labels[i]) if true_classes[i] == -1 else mode_labels[i][true_classes[i]]
        for i in range(len(mode_labels))
    ])

    # Calcola MAE
    mae_mean = np.mean(np.abs(predicted_damage_levels_mean - true_damage_levels))
    mae_mode = np.mean(np.abs(predicted_damage_levels_mode - true_damage_levels))

    media_vicini_mean, media_altri_mean = calcola_medie_normalizzate_dataset(mean_labels)
    media_vicini_mode, media_altri_mode = calcola_medie_normalizzate_dataset(mode_labels)

    # Errore relativo
    # relative_error_mean = np.mean(np.abs(predicted_damage_levels_mean - true_damage_levels) / (true_damage_levels + 0.25))
    # relative_error_mode = np.mean(np.abs(predicted_damage_levels_mode - true_damage_levels) / (true_damage_levels + 0.25))

    # Calcola metriche per predizioni con media

    mean_predicted_classes = np.array([
        0 if np.all(mean_label < 0.3) else np.argmax(mean_label) + 1
        for mean_label in mean_labels
    ])

    mode_predicted_classes = np.array([
        0 if np.all(mode_label < 0.3) else np.argmax(mode_label) + 1
        for mode_label in mode_labels
    ])

    true_classes_adjusted = true_classes + 1  # Per correggere la numerazione se necessario

    print(f"Forma di true_classes_adjusted: {true_classes_adjusted.shape}")
    print(f"Forma di mean_predicted_classes: {mean_predicted_classes.shape}")

    # Calcola la confusion matrix
    cm = confusion_matrix(true_classes_adjusted, mean_predicted_classes)

    # Visualizza la confusion matrix come heatmap
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(true_classes), yticklabels=np.unique(true_classes))
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    accuracy_mean = accuracy_score(true_classes_adjusted, mean_predicted_classes)
    precision_mean = precision_score(true_classes_adjusted, mean_predicted_classes, average='weighted')
    recall_mean = recall_score(true_classes_adjusted, mean_predicted_classes, average='weighted')
    f1_mean = f1_score(true_classes_adjusted, mean_predicted_classes, average='weighted')

    accuracy_mode = accuracy_score(true_classes_adjusted, mode_predicted_classes)
    precision_mode = precision_score(true_classes_adjusted, mode_predicted_classes, average='weighted')
    recall_mode = recall_score(true_classes_adjusted, mode_predicted_classes, average='weighted')
    f1_mode = f1_score(true_classes_adjusted, mode_predicted_classes, average='weighted')

    # Calcolo vettore deviazione standard media per ogni componente
    mean_std_devs_per_component = np.mean(std_devs, axis=0)

    # Calcolo deviazione standard media per ogni "True Label" distinto
    unique_true_classes = np.unique(true_classes)
    mean_std_dev_per_class = {
        label: np.mean(std_devs[true_classes == label]) for label in unique_true_classes
    }

    # Stampa dei risultati
    print(f"Errore Assoluto Medio (Media): {mae_mean:.4f}")
    print(f"Errore Assoluto Medio (Moda): {mae_mode:.4f}")
    # print(f"Errore Relativo Medio (Media): {relative_error_mean:.4f}")
    # print(f"Errore Relativo Medio (Moda): {relative_error_mode:.4f}")

    print(f"Accuracy Medio (Media): {accuracy_mean:.4f}")
    print(f"Precision Medio (Media): {precision_mean:.4f}")
    print(f"Recall Medio (Media): {recall_mean:.4f}")
    print(f"F1 Medio (Media): {f1_mean:.4f}")

    print(f"Accuracy Medio (Moda): {accuracy_mode:.4f}")
    print(f"Precision Medio (Moda): {precision_mode:.4f}")
    print(f"Recall Medio (Moda): {recall_mode:.4f}")
    print(f"F1 Medio (Moda): {f1_mode:.4f}")

    print(f"Media vicini normalizzata (Mean): {media_vicini_mean:.4f} | Media altri normalizzata(Mean):  {media_altri_mean:.4f}")
    print(f"Media vicini normalizzata (Mode): {media_vicini_mode:.4f} | Media altri normalizzata(Mode):  {media_altri_mode:.4f}")

    print("\nDeviazione standard media per ogni componente:")
    print(mean_std_devs_per_component)

    print("\nDeviazione standard media per ciascun 'True Label' distinto:")
    for label, mean_std in mean_std_dev_per_class.items():
        print(f"True Label {label}: Media deviazione standard = {mean_std:.4f}")
















"""
## Main
"""

# Indice casuale per selezionare un campione dal dataset Z
idx = np.random.randint(0, Z_test.shape[0]) 

# Estrai le colonne di media e log-varianza per il campione selezionato
z_mean = Z_test[['mean_0', 'mean_1', 'mean_2', 'mean_3']].to_numpy()  # Estrai le colonne delle medie
z_log_var = Z_test[['var_0', 'var_1', 'var_2', 'var_3']].to_numpy()  # Estrai le colonne delle log-varianze

# Definisci il numero di iterazioni per MCMC
n_iter = 20000  

# Esegui la classificazione con PyMC usando il vettore continuo
# Assicurati di passare la riga completa di medie e varianze per il campione selezionato
trace = mcmc_classification_pymc(z_mean[idx], z_log_var[idx], n_iter)


# Questo ti restituirà un array con dimensioni (n_samples_totali, 7), dove n_samples_totali = n_chains * n_draws (n_iter, i tunes sono scartati)
accepted_labels = trace.posterior["label_vector"].stack(samples=("chain", "draw")).values



# Analizza i risultati: Calcola la media dei campioni per ottenere il label continuo stimato
label_estimate_mean = np.mean(accepted_labels, axis=-1)
print(f"Label stimato con media: {label_estimate_mean}")
print(f"Forma di label_estimate_mean: {label_estimate_mean.shape}")

# Calcola la moda basata sull'istogramma per ciascun componente del label continuo
label_estimate_mode = estimate_mode_from_histogram(accepted_labels, bins=50)
print(f"Label stimato con moda (basata su istogramma): {label_estimate_mode}")
print(f"Forma di label_estimate_mode: {label_estimate_mode.shape}")

# Ottieni il vero label per l'indice scelto
true_label = labels[idx]
print(f"Vero label: {true_label}")



# Visualizzazione dei risultati con istogrammi
num_labels = true_label.shape[0]

plt.figure(figsize=(15, 10))

for i in range(num_labels):
    plt.subplot(2, 4, i + 1)  # Crea una griglia di 2 righe e 4 colonne
    plt.hist(accepted_labels[i, :], bins=50, alpha=0.7, color='blue', edgecolor='black', range=(0,0.8))
    plt.title(f'Class {i + 1}\nTrue label: {true_label[i]:.2f}',fontsize=16)
    plt.xlabel('Valore',fontsize=14)
    plt.ylabel('Frequenza',fontsize=14)
    plt.tick_params(axis='both', which='major', labelsize=10)


output_dir = "output_MCMC_NVP_2"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Aggiungi un layout e salvataggio
plt.tight_layout()
plt.savefig(os.path.join(output_dir, f'accepted_labels_histogram_idx_{idx}.png'))
plt.show()




# Seleziona la componente da tracciare
components_to_plot = [2]

plt.figure(figsize=(10, 6))

for i in components_to_plot:
    plt.plot(accepted_labels[i, 0:20000], label=f'Component {i}')

plt.xlabel('Iteration')
plt.ylabel('Parameter Value')
plt.title('MCMC Trace for Selected Label Vector Components')
plt.legend()
plt.show()




"""
## Creazione .csv
"""

n_iter = 20000 

# Nome del file CSV dove salvare i risultati
csv_file_name = 'mcmc_results_sampled_2.csv'

# Numero di iterazioni
num_iterations = 0

for i in range(num_iterations):

    # Stampa l'iterazione corrente
    print(f"Inizio iterazione {i + 1} di {num_iterations}")

    # Seleziona un indice casuale dal dataset Z_test
    idx = np.random.randint(0, Z_test.shape[0])
    
    # Estrai le colonne di media e log-varianza per il campione selezionato
    z_mean = Z_test[['mean_0', 'mean_1', 'mean_2', 'mean_3']].to_numpy()  # Estrai le colonne delle medie
    z_log_var = Z_test[['var_0', 'var_1', 'var_2', 'var_3']].to_numpy()  # Estrai le colonne delle log-varianze

    # Esegui la classificazione con PyMC usando il vettore continuo
    # Assicurati di passare la riga completa di medie e varianze per il campione selezionato
    trace = mcmc_classification_pymc(z_mean[idx], z_log_var[idx], n_iter)

    # Accedi ai campioni accettati
    accepted_labels = trace.posterior["label_vector"].stack(samples=("chain", "draw")).values

    # Ottieni il vero label per l'indice scelto
    true_label = labels[idx]

    # Salva i risultati nel CSV
    append_results_to_csv(csv_file_name, idx, true_label, accepted_labels)

print(f"Risultati salvati nel file {csv_file_name} dopo {num_iterations} iterazioni.")



"""
## Analisi performance
"""

# metrics = calculate_performance_metrics(true_label, label_estimate_mean, label_estimate_mode)

# print("Metriche di performance:")

# # Per la media
# print(f"MAE (Media): {metrics['mae_mean']}")
# print(f"MSE (Media): {metrics['mse_mean']}")
# print(f"RMSE (Media): {metrics['rmse_mean']}")
# print(f"R² (Media): {metrics['r_squared_mean']}")
# print(f"SMAPE (Media): {metrics['smape_mean']}")
# print(f"NRMSE (Media): {metrics['nrmse_mean']}")
# print(f"Cosine Similarity (Media): {metrics['cosine_sim_mean']}")
# print(f"Explained Variance (Media): {metrics['explained_variance_mean']}")

# # Per la moda
# print(f"MAE (Moda): {metrics['mae_mode']}")
# print(f"MSE (Moda): {metrics['mse_mode']}")
# print(f"RMSE (Moda): {metrics['rmse_mode']}")
# print(f"R² (Moda): {metrics['r_squared_mode']}")
# print(f"SMAPE (Moda): {metrics['smape_mode']}")
# print(f"NRMSE (Moda): {metrics['nrmse_mode']}")
# print(f"Cosine Similarity (Moda): {metrics['cosine_sim_mode']}")
# print(f"Explained Variance (Moda): {metrics['explained_variance_mode']}")


# medie_parametri = calcola_media_parametri('mcmc_results_3.csv')


# Esegui la funzione con il file CSV contenente i risultati
csv_file_name = 'mcmc_results_sampled_2.csv'
# create_confusion_matrix(csv_file_name)
analyze_damage_levels(csv_file_name)

# update_performance_metrics_from_file('mcmc_results.csv', 'mcmc_results_1.csv')

# medie_parametri = calcola_media_parametri('mcmc_results_2.csv')

